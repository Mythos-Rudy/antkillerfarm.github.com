---
layout: post
title:  深度强化学习（二）——Deep Q-learning Network, DQN进化史
category: RL 
---

# 概述（续）

## 参考

https://www.nervanasys.com/demystifying-deep-reinforcement-learning/

深度强化学习揭秘

https://zhuanlan.zhihu.com/p/24446336

深度强化学习Deep Reinforcement Learning学习整理

https://mp.weixin.qq.com/s/K82PlSZ5TDWHJzlEJrjGlg

深度学习与强化学习

http://mp.weixin.qq.com/s/lLPRwInF5qaw7ewYHOpPyw

深度强化学习资料

http://lamda.nju.edu.cn/yangjw/project/drlintro.html

深度强化学习初探

https://zhuanlan.zhihu.com/p/21498750

深度强化学习导引

https://mp.weixin.qq.com/s/RnUWHa6QzgJbE_XqLeAQmg

深度强化学习，决策与控制

https://mp.weixin.qq.com/s/SckTPgEw7KWmfKXWriNIRg

浅谈强化学习的方法及学习路线

https://mp.weixin.qq.com/s/-JHHOQPB6pKVuge64NkMuQ

DeepMind主攻的深度强化学习3大核心算法及7大挑战

https://mp.weixin.qq.com/s/2SOHQElaYbplse3QqG9tYw

强化学习介绍

https://mp.weixin.qq.com/s/R30quVGK0TgjerLpiIK9eg

从算法到训练，综述强化学习实现技巧与调试经验

https://www.zhihu.com/question/49230922

强化学习（reinforcement learning)有什么好的开源项目、网站、文章推荐一下？

https://zhuanlan.zhihu.com/p/75913329

深度强化学习简介

# DRL和Robotics

虽然，DRL似乎生来就是为了Robotics，然而现实中的无人系统，目前基本还是使用传统的控制方法。

例如：

https://www.zhihu.com/question/50050401

如何看待百度无人车， 三千多个场景，一万多个if？

不止国内，就连业界标杆Boston Dynamics也是这样：

https://www.zhihu.com/question/29871410

波士顿动力Boston Dynamics的大狗Big Dog用的了哪些控制、估计等算法？

直到最近才终于有了改观：

https://mp.weixin.qq.com/s/xSODAGf3QcJ3A9oq6xP11A

真的超越了波士顿动力！深度强化学习打造的ANYmal登上Science子刊

此外，还有一些论文：

《Learning to Drive in a Day》

参考：

https://zhuanlan.zhihu.com/p/59197798

那些个端到端的自动驾驶研发系统

https://mp.weixin.qq.com/s/yU-6r-7l50y5msw8gUWnUQ

美团技术部解析：无人车端到端驾驶模型概述

# Deep Q-learning Network

## 基本思想

Deep Q-learning Network是DL在RL领域的开山之作。它的思想主要来自于Deepmind的两篇论文：

《Playing Atari with Deep Reinforcement Learning》

《Human-level control through deep reinforcement learning》

Deepmind是当今DL领域最前沿的科研机构，尤其在RL领域更是领先同行一大截，是当之无愧的RL王者。

![](/images/img2/DQN.png)

上图是DQN的网络结构图。由于这里的任务是训练Atari游戏的AI，因此网络的输入实际上就是游戏的画面。而理解游戏画面，就需要一定的CNN结构。所以DQN的结构实际上和一般的CNN是一致的，其关键要害在于loss函数的设定。

由《强化学习（六）》中的“价值函数的近似表示”可知：$$Q(s,a)\approx f(s,a,w)$$

老套路，我们可以用一个神经网络来拟合f函数，这也就是所谓的Q网络（Q-Network）。

对于Atari游戏而言，这是一个高维状态输入（原始图像），低维动作输出（只有几个离散的动作，比如上下左右）。那么怎么来表示这个函数f呢？

Deepmind采取的方法是：只把状态s作为输入，但是输出的时候，会输出每一个动作的Q值，也就是输出一个向量$$[Q(s,a_1),Q(s,a_2),\dots,Q(s,a_n)]$$。这样我们只要输入状态s，就可以得到所有的动作Q值。

在《强化学习（二）》中，我们指出Q-Learning算法的**transition rule**为：

$$Q(s,a)=R(s,a)+\gamma \max(Q(\tilde s,\tilde a))$$

这时就存在两个Q值了：

- 根据当前的<s,a>，由Q网络计算得到的Q值，被称为**current Q-value**。

- 使用transition rule，也由Q网络计算得到的Q值，被称为**target Q-value**。

显然从Q函数的定义来看，这两者是等价的。因此，我们的目标就是使这两者尽可能的一致，即：

$$L=E[(\color{blue}{r+\gamma \max_{a'}Q(s',a',w)}-Q(s,a,w))^2]$$

上式中蓝色的部分即为target Q-value。

DQN虽然是model-free的，但是也有难点，那就是Reward的定义。好在在Atari游戏的例子里，这个问题比较简单，直接采用游戏得分即可。但得分奖励过于肤浅，在动作类游戏中或许没啥问题，但对于《Montezuma's Revenge》这样的解谜游戏，就不太好使了。

## Nature DQN

DQN最早发表于NIPS 2013，该版本的DQN，也被称为NIPS DQN。NIPS DQN除了提出DQN的基本概念之外，还使用了《强化学习（六）》中提到的Experience Replay技术。

2015年初，Deepmind在Nature上提出了改进版本，是为Nature DQN。它改进了Loss函数：

$$L=(\color{blue}{r+\gamma \max_{a'}Q(s',a',w^-)}-Q(s,a,w))^2$$

上式中，计算目标Q值的网络使用的参数是$$w^-$$，而不是w。也就是说，原来NIPS DQN的target Q网络是动态变化的，跟着Q网络的更新而变化，这样不利于计算目标Q值，导致目标Q值和当前的Q值相关性较大。

因此，Deepmind提出可以单独使用一个目标Q网络。那么目标Q网络的参数如何来呢？还是从Q网络中来，只不过是延迟更新。也就是每次等训练了一段时间再将当前Q网络的参数值复制给目标Q网络。

个人理解这里的机制类似于进化算法。**变异既可以是进化，也可以是退化，只有进化的Q网络才值得保留下来。**

这在下文介绍的Leela Zero项目的训练过程中体现的很明显：一开始我们只训练一个简单的AI，然后逐渐加深加大网络，淘汰废物AI，得到更优秀的AI。

为了使进化生效，在一段时间内保持目标Q网络不变就是很显然的了——你过不了我这关，就没资格继续下去了。

## 参考

https://zhuanlan.zhihu.com/p/21262246

DQN从入门到放弃1 DQN与增强学习

https://zhuanlan.zhihu.com/p/21292697

DQN从入门到放弃2 增强学习与MDP

https://zhuanlan.zhihu.com/p/21340755

DQN从入门到放弃3 价值函数与Bellman方程

https://zhuanlan.zhihu.com/p/21378532

DQN从入门到放弃4 动态规划与Q-Learning

https://zhuanlan.zhihu.com/p/21421729

DQN从入门到放弃5 深度解读DQN算法

https://zhuanlan.zhihu.com/p/21547911

DQN从入门到放弃6 DQN的各种改进

https://zhuanlan.zhihu.com/p/21609472

DQN从入门到放弃7 连续控制DQN算法-NAF

https://zhuanlan.zhihu.com/p/56023723

通过Q-learning深入理解强化学习（上）

https://mp.weixin.qq.com/s/0HukwNmg3k-rBrIBByLhnQ

深度Q学习：一步步实现能玩《毁灭战士》的智能体

https://mp.weixin.qq.com/s/x-qCA0TzoVUtZ8VAf8ey0A

深度Q学习介绍

https://mp.weixin.qq.com/s/snNvN4FFP0VEZwDA0TAp1w

强化学习训练Chrome小恐龙Dino Run：最高超过4000分

https://mp.weixin.qq.com/s/SqU74jYBrjtp9L-bnBuboA

教你完美实现深度强化学习算法DQN

https://mp.weixin.qq.com/s/Vdt5h8APAFoeVxFYKlywpg

用DeepMind的DQN解数学题，准确率提升15%

https://mp.weixin.qq.com/s/RH4ifA46njdC7fyRI9kVMg

深度Q网络与视觉格斗类游戏

https://mp.weixin.qq.com/s/uymKtR_7IgMpfXcekfkCDg

从强化学习基本概念到Q学习的实现，打造自己的迷宫智能体

https://mp.weixin.qq.com/s/9ZvaZQ1yumhr75okBxNyeA

优化强化学习Q-learning算法进行股市交易

https://mp.weixin.qq.com/s/QkRdv0xMoqwaXsNAMAHR0A

一图尽展视频游戏AI技术，DQN无愧众算法之鼻祖

https://mp.weixin.qq.com/s/SJd-3qH4W4GMMLZSmvFk1w

利用DQN玩吃豆人（Pacman）小游戏

https://zhuanlan.zhihu.com/p/35882937

强化学习——从Q-Learning到DQN到底发生了什么？

# DQN进化史

![](/images/img2/DQN.jpg)

上图引用自论文：

《Deep Learning for Video Game Playing》

该论文最早发表于2017年，但是作者每年都会更新论文的内容。

----

在Nature DQN出来之后，肯定很多人在思考如何改进它。那么DQN有什么问题呢？

- 目标Q值的计算准确吗？全部通过max Q来计算有没有问题？

- 随机采样的方法好吗？按道理不同样本的重要性是不一样的Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？

- DQN中使用$$\epsilon$$-greedy的方法来探索状态空间，有没有更好的做法？

- 使用卷积神经网络的结构是否有局限？加入RNN呢？

- DQN无法解决一些高难度的Atari游戏比如《Montezuma's Revenge》，如何处理这些游戏？

- DQN训练时间太慢了，跑一个游戏要好几天，有没有办法更快？

- DQN训练是单独的，也就是一个游戏弄一个网络进行训练，有没有办法弄一个网络同时掌握多个游戏，或者训练某一个游戏后将知识迁移到新的游戏？

- DQN能否用在连续动作输出问题？

## Double DQN

$$L=\left(\color{blue}{r+\gamma Q(s',\color{red}{\mathop{\arg\max}_{a'}Q(s',a',w)},w^-)}-Q(s,a,w)\right)^2$$

Nature DQN的两个Q网络分离的还不够彻底，Double DQN用当前的Q网络来选择动作（公式中红色部分所示），而用目标Q网络来计算目标Q。

## Prioritised replay

采样优先级采用目标Q值与当前Q值的差值来表示：

$$| \color{blue}{r+\gamma \max_{a'}Q(s',a',w^-)}-Q(s,a,w) |$$

优先级越高，那么采样的概率就越高。

## Dueling Network

Dueling Network将Q网络分成了两个通道：

- Action无关的值函数：$$V(s,v)$$

- Action相关的值函数：$$A(s,a,w)$$

$$Q(s,a)=V(s,v)+A(s,a,w)$$

![](/images/img3/DuelDQN.png)

## NAF

传统RL任务：低维输入，低维离散输出。

Atari游戏：高维输入，低维离散输出。

通常的控制系统，除了离散控制之外，还有连续控制，然而DQN并不能直接用于连续域：因为它根本就没办法穷举每一个动作，也就无法计算最大的Q值对应的动作。

论文：

《Continuous Deep Q-Learning with Model-based Acceleration》

对于连续控制，由于我们已经无法选择动作，因此只能设计一种方法，使得我们输入状态，然后能够输出动作，并且保证输出动作对应的Q值是最大值。

这个时候，我们有两种选择：

- 弄两个神经网络，一个是Policy网络，输入状态，输出动作，另一个是Q网络，输入状态，输出Q值。这种做法其实就是Actor-Critic算法。

- 只弄一个神经网络，既能输出动作也能输出Q值。这就是本节所要使用的方法。
