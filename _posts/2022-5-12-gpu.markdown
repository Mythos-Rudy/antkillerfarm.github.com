---
layout: post
title: GPU体系结构
category: Chip 
---

* toc
{:toc}

# GPU体系结构

![](/images/img4/SGI.png)

上图是SGI公司1993年的一款显卡的模块图。可以看出，早期的显卡是一个具有固定流水线的专用芯片，只能处理特定任务，没有多少可编程的能力。

![](/images/img4/Tesla.png)

到了2005年左右，这些渲染流水线的功能被统一到一个叫做Shader Processor的可编程单元中。此后，显卡也被称为GPU（Graphics Processing Unit）。

![](/images/img4/Shader.png)

无论是CPU，还是GPU，或者是现在比较火的NPU，一个Processing Unit，一般都是由上图所示的三部分组成：

- Fetch/Decode。数据的输入/输出单元。

- ALU（arithmetic and logic unit）。运算单元。

- Execution Context。寄存器单元。

CPU强调单core的极致性能，因此广泛使用了如下技术：

- Out-of-order control logic

- Fancy branch predictor 

- Memory pre-fetcher

![](/images/img4/GPU.png)

GPU不追求单core的性能，因此也没有那些复杂的控制逻辑。这样省下来的电路，可以变成更多的ALU。由于数据处理的局部性，通常可以若干ALU共享一套Fetch/Decode和Ctx。

当然，如果ALU非常多的话，就不能都共享一套了，这时可以采用下图所示的分组共享。

![](/images/img4/GPU_2.png)

其中的每个组，被称为一个**core**。

有利就有弊，下面看看GPU是如何执行条件语句的。

![](/images/img4/GPU_3.png)

上图展示了8个ALU并行执行条件语句的过程：

- 计算每个数据的条件，生成对应的标志。

- 执行True分支。

- 执行False分支。

可以看出，GPU对于条件语句的执行时间=True分支执行时间+False分支执行时间。而CPU只需要根据条件，执行一个分支就可以了。显然，GPU对于程序的控制能力，相对CPU简直弱爆了。GPU并不擅长做这些东西。

>CPU的SSE、AVX之类的指令集，也借鉴了GPU的设计思想，但是CPU的core数，相对于GPU而言，差了10～1000倍。

---

VGPR：Vector general purpose register

SGPR：Scalar general purpose register

---

参考：

http://www.cnblogs.com/geniusalex/archive/2008/12/26/1941766.html

CPU GPU设计工作原理

https://mp.weixin.qq.com/s/-Wg1GtVGUxfshJ5d5NDd-Q

聊聊GPU的计算能力上限

https://mp.weixin.qq.com/s/zdr7BfJxVepQL1TCDXQoJA

一文带你了解GPU的前生今世

## Programming Model vs Hardware Execution Model

- **Programming Model**: how the programmer expresses the code.

Sequential (von Neumann), Data Parallel (SIMD), Dataflow, Multi-threaded (MIMD, SPMD), …

- **Execution Model**: how the hardware executes the code underneath.

Out-of-order execution, Vector processor, Array processor, Dataflow processor, Multiprocessor, Multithreaded processor, …

- Execution Model can be very different from the Programming Model.

von Neumann model implemented by an OoO processor

SPMD model implemented by a SIMD processor (a GPU)

GPU虽然是一个SIMD机器，但是并没有采用SSE、AVX之类的SIMD编程模型，而是SPMD模型。

## 移动GPU

移动GPU和桌面GPU最核心的差别在于渲染流程不同。目前主流的移动GPU，无论ARM、高通还是Imagination，其GPU都是TBR（Tile Based Rendering）。而桌面GPU，无论NVIDIA、AMD还是Intel，都是Immediate Rendering。

所谓的Immediate Rendering，就是将一个图元（最常见的是三角形），从头到尾走完整个Pipeline，中间没有停止。这种结构，控制简单，容易实现。问题是在做blending的时候需要从存储单元中不断读回之前render的结果，因此在绘制一个大的场景时这个带宽消耗是比较大的。而桌面GPU都是有自身显存的，它不需要通过系统总线从系统的DDR memory中读回数据。所以开销比较小，这是完全可以接受的。

TBR的精髓就是一个Tile一个Tile的渲染。这样只需要给GPU配上一块很小的片上cache（足够装下一两个Tile的内容就行），就能实现高效的blending。移动GPU中有Tiler模块，将整个场景的图元信息（最主要的是位置）都保存下来，并且能在做光栅化时快速的检索出属于这个Tile的图元。

https://www.zhihu.com/question/20720436

移动GPU和桌面GPU的差距有哪些？

## GPGPU

通用图形处理器（General-purpose computing on graphics processing units，简称GPGPU），是一种利用处理图形任务的图形处理器来计算原本由中央处理器处理的通用计算任务。**这些通用计算常常与图形处理没有任何关系。**

从市场宣传的角度，如果某家公司的产品定位为GPGPU，那么也就意味着该GPU没有图形能力。

![](/images/img4/GPU_4.png)

上图展现了GPU和GPGPU之间的差异。

![](/images/img4/GPU_5.png)

然而通用处理对于图形处理的侵蚀，或者说充分运用AI算力，简化图形专用硬件也是一大趋势。

如上图所示，先进图形标准中，崭新的、更加利用计算功能的mesh shader可以取代从vertex shader到geometry shader的着色器，从而在不减功能的前提下，将图形管线节点数大量减少，并移除一些连接节点的专用硬件。性能还可能因为mesh shader拥有的弹性，而有所提升。

# AI Chip+

https://zhuanlan.zhihu.com/p/57808378

AI芯片0.5与2.0

https://mp.weixin.qq.com/s/XDwTI-gnnFMLjVBbOGKL9w

清华大学团队研制高能效通用神经网络处理器芯片STICKER-T

https://mp.weixin.qq.com/s/xbHP1RFn7F7BbimxgWaKqg

Facebook把服务27亿人的AI硬件系统开源了

https://mp.weixin.qq.com/s/BD-HAILp3TPvBFlIy6QC4w

一文看懂机器视觉芯片

https://mp.weixin.qq.com/s/PMnNay4CRgVghA4fU9oLqg

牛津大学研发类脑光子芯片，运算速度超人脑1000倍

https://mp.weixin.qq.com/s/e333KjLavEvvpNIL3u1Y4Q

NovuMind异构智能核心技术引领智联网

https://mp.weixin.qq.com/s/fSSyOs4-NXbPTbDjpfJBNQ

Google IPU：互联网巨头纷纷进军芯片行业是为何？

https://mp.weixin.qq.com/s/S1y4NEx4_Mgwf68S2pexqA

拿着锤子找钉子，数字芯片领导者比特大陆进军人工智能

https://mp.weixin.qq.com/s/gtgPYf939uYRzxAab_LZLQ

谢源：计算存储一体化，在存储里做深度学习，架构创新实现下一代AI芯片

https://mp.weixin.qq.com/s/s-fYxv4z5kkJUFueU2IR7w

BP表达式与硬件架构：相似性构建更高效的计算单元

https://mp.weixin.qq.com/s/1r7G84les7FihqPbSiS0Ng

华为首款手机端AI芯片麒麟970

https://mp.weixin.qq.com/s/y9dVg9YtfWxu6NcW-fxi6Q

内存带宽与计算能力，谁才是决定深度学习执行性能的关键？

https://mp.weixin.qq.com/s/K_dohaZbCISZlxe1Utu50w

如何用FPGA加速卷积神经网络(CNN)？

https://mp.weixin.qq.com/s/z68hk1yqg60QCjgTyzgG2w

GPU深度学习的“加速神器”

https://mp.weixin.qq.com/s/O-NDsFs6AOwl43LyevXtzg

OpenAI发布“块稀疏”GPU内核：实现文本情感分析与图像生成建模当前最优水平

https://mp.weixin.qq.com/s/Qfbc2iQnXacOqOGIrpRQRw

Tensor Core究竟有多快？全面对比英伟达Tesla V100/P100的RNN加速能力

https://mp.weixin.qq.com/s/XXef4F9HEZizoWRYXwHitw

如何配置一台深度学习工作站?

https://mp.weixin.qq.com/s/_xFRRkVyN9qevLeek7bFxQ

深度学习中GPU和显存分析

https://mp.weixin.qq.com/s/k5Xx-nnaf-yfWqGLIY3LEg

特斯拉的芯片究竟多强

https://zhuanlan.zhihu.com/p/88927564

窥探神经网络加速器的数据复用（一）

https://mp.weixin.qq.com/s/YARcCzQXqnJmWRtV2zd_FQ

国内外AI芯片发展现状

https://mp.weixin.qq.com/s/b_Hy0JSZ5ZGT9AsczCkp9Q

ISSCC 2020：AI芯片架构的转变

https://zhuanlan.zhihu.com/p/115219461

张量在神经网络加速器中的应用

https://pan.baidu.com/s/1zcXdXTaN3dbJ9VGplViHZw 提取码:ews4

AI芯片体系架构和软件专题报告会2020论文下载

https://mp.weixin.qq.com/s/cBVio3W64np8fXwUQs2wIw

机器学习如何用于芯片系统设计？Jeff Dean推荐Google最新《机器学习系统芯片设计》70页ppt为你讲解

https://zhuanlan.zhihu.com/p/150656419

AI芯片技术发展

https://mp.weixin.qq.com/s/9fBe6MsUOCDhtp4MSfP2jg

AI处理器热潮正在消退

https://mp.weixin.qq.com/s/JYTqJDlGw6Q2gNLaYIGLcQ

特斯拉芯片究竟怎么样？

https://mp.weixin.qq.com/s/cA1AXfpV3ZcMNX9k5XKlww

面向深度神经网络的芯片布图规划问题简介

https://zhuanlan.zhihu.com/p/231302709

聊聊GPU峰值计算能力

https://zhuanlan.zhihu.com/p/340775090

从Thinker到Evolver：对可演化AI芯片的探索

https://mp.weixin.qq.com/s/kuIsyX0QEIeFHn8tvFE8vw

AI训练的最大障碍不是算力，而是“内存墙”

https://mp.weixin.qq.com/s/MwZ9j1MIwRBrJK4iWKzRqQ

芯故事：和AMD有渊源的那些AI创业公司

https://zhuanlan.zhihu.com/p/446830540

GPU架构（上）—AI芯片设计入门

https://zhuanlan.zhihu.com/p/446837354

GPU架构（下）—AI芯片设计入门

https://mp.weixin.qq.com/s/rtO8PxRj08GVimT3bfbplA

我看英伟达H100 GPU

https://www.zhihu.com/question/523521515

如何评价英伟达3月22日发布的全新GPU H100？

https://mp.weixin.qq.com/s/S1fVrSfAM_UJh06Q43s8vA

网络芯片架构的新改变

https://zhuanlan.zhihu.com/p/47904879

AI芯片在5G中的机会

https://mp.weixin.qq.com/s/Z_QVN7OCLqeyMrwK3Sc7qA

AI芯片和传统芯片的区别

https://mp.weixin.qq.com/s/mMiAGH2Yz42xes7jicyygA

“超级芯片”或在十年内诞生，摩尔定律再续一命！（自旋电子逻辑器件“MESO器件”）
