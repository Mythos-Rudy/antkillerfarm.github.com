---
layout: post
title:  强化学习（十）——Integrating Learning and Planning（1）
category: RL 
---

# 博弈论（续）

## 其他博弈问题

当年英国政府将流放澳洲的犯人交给往来于澳洲之间的商船来完成，由此经常会发生因商船主或水手虐待犯人，致使大批流放人员因此死在途中(葬身大海)的事件发生。

后来大英帝国对运送犯人的办法(制度)稍加改变，流放人员仍然由往来于澳洲的商船来运送，只是运送犯人的费用要等到犯人送到澳洲后才由政府按实到犯人人数支付给商船。

仅就这样一点小小的“改变”，几乎再也没有犯人于中途死掉的事情发生。

----

枪手博弈是指，在三个枪手A、B、C之间一场对决即将展开，枪手A的枪法最好，命中率达到80%；枪手B的枪法次之命中率60%；而枪手C的命中率最差只有50%。

而此时由于是三方对决，先瞄准谁成了关键，对于A枪手来说，当然先瞄准仅次于他的B枪手，但对于B和C枪手而言，威胁最大的当然是A，在A倒下后B面对C的胜率会大很多，同时C的存活率也会提高，所以枪口都会对上A。

结论：

- 第一轮枪战，枪法最差的C竟然存活概率最大——肯定存活，而枪法好的A和B存活概率远低于C。启示：韬晦很重要。

- 如果在第一轮枪战中A、B均被击中，则C成为最终幸存者；只要A、B在第一轮枪战中有一人存活，那最终胜出的很可能是A和B中的幸存者。启示：实力很关键。能力较差的C靠着策略虽然能在第一轮枪战中暂时获胜，但只要A、B在第一轮枪战中有一人存活，那么第二轮枪战里C的存活的概率就会比对手低了。

## 参考

https://www.cnblogs.com/steven-yang/tag/博弈论/

一个博弈论的专栏

https://mp.weixin.qq.com/s/5o3m8RLwYkZJEhqNxOLq_A

不对称多代理博弈中的博弈理论解读

https://mp.weixin.qq.com/s/D9bRjYVJOMT0Jkh59XZ-Rg

DeepMind于Nature子刊发文提出非对称博弈的降维方法

https://mp.weixin.qq.com/s/1t6WuTQpltMtP-SRF1rT4g

当博弈论遇上机器学习：一文读懂相关理论

https://news.mbalib.com/story/242878

智猪博弈、龟兔悖论、谷堆悖论...这些有趣的博弈论值得一看！

https://blog.csdn.net/qq_27351341/article/details/81119533

偏好函数、无差异曲线、帕累托标准、卡尔多-希克斯标准等基础概念

https://blog.csdn.net/qq_27351341/article/details/81138774

囚徒困境、智猪博弈、纳什均衡与一致预期

https://blog.csdn.net/qq_27351341/article/details/81268801

多重均衡与制度和文化

https://blog.csdn.net/qq_27351341/article/details/81276298

动态博弈、威胁与承诺

https://blog.csdn.net/sobermineded/article/details/79541511

几个经典博弈模型（囚徒的困境、赌胜博弈、产量决策的古诺模型）

https://mp.weixin.qq.com/s/5k6A4NExdb9Ysbd753pPHw

博弈论速成指南：那些融入深度学习的经典想法和新思路

# Integrating Learning and Planning

前面的章节主要介绍了Model-Free RL，下面将讲一下Model-Based RL，主要包括如下内容：

- 如何从经历中直接学习Model。

- 如何基于模型来进行Planning（规划）。

- 如何将“学习”和“规划”整合起来。

![](/images/img3/Model_Free_RL.png) | ![](/images/img3/Model_RL.png)

上面两图形象的说明了Model-Free RL（左图）和Model-Based RL（右图）的差别。

## Model-Based RL

![](/images/img3/Model_RL_2.png)

上图比较好的说明了模型学习在整个RL学习中的位置和作用。

我们首先看一下Model的定义：Model是一个参数化（参数为$$\eta$$）的MDP<S, A, P, R>，其中假定：状态空间Ｓ和行为空间Ａ是已知的，则$$M=<P_\eta, R_\eta>$$，其中$$P_\eta \approx P, R_\eta \approx R$$。则：

$$S_{t+1} \sim P_\eta (S_{t+1} | S_t, A_t)$$

$$R_{t+1} = R_\eta (R_{t+1} | S_t, A_t)$$

通常我们需要如下的假设，即状态转移函数和奖励函数是条件独立的：

$$P[S_{t+1}, R_{t+1} | S_t, A_t] = P[S_{t+1} | S_t, A_t] P[R_{t+1} | S_t, A_t]$$

## Model Learning

所谓Model Learning是指：从experience$$\{S_1, A_1, R_2, \dots, S_T\}$$，学习

$$S_1, A_1\to R_2,S_2$$

$$S_2, A_2\to R_3,S_3$$

$$\dots$$

$$S_{T-1}, A_{T-1}\to R_T,S_T$$

这实际上是一个监督学习的问题。其中，$$s,a\to r$$是一个回归问题(regression problem)，而$$s,a\to s'$$是一个密度估计问题(density estimation problem)。

选择一个损失函数，比如均方差，KL散度等，优化参数$$\eta$$来最小化经验损失(empirical loss)。所有监督学习相关的算法都可以用来解决上述两个问题。

根据使用的算法不同，可以有如下多种模型：查表式(Table lookup Model)、线性期望模型(Linear Expectation Model)、线性高斯模型(Linear Gaussian Model)、高斯决策模型(Gaussian Process Model)、和深信度神经网络模型(Deep Belief Network Model)等。

这里主要以查表模型来解释模型的构建。

## Table Lookup Model

查表模型适用于MDP的P，R都为已知的情况。我们通过visit得到各状态行为的转移概率和奖励，把这些数据存入表中，使用时直接检索。状态转移概率和奖励计算方法如下：

$$\hat{P}^a_{s,s'}=\frac{1}{N(s,a)}\sum_{t=1}^T 1(S_t,A_t,S_{t+1}=s,a,s')$$

$$\hat{R}^a_{s}=\frac{1}{N(s,a)}\sum_{t=1}^T 1(S_t,A_t=s,a)R_t$$

其中的$$N(s,a)$$表示state action pair$$<s,a>$$在visit中出现的次数。

这里也可以采用如下变种：

- 在每个time-step：t，记录experience tuple：$$<S_t, A_t, R_{t+1}, S_{t+1}>$$。

- 从模型采样构建虚拟经历时，从tuples中随机选择符合$$<s,a,\cdot,\cdot>$$的一个tuple。

这种方法，不再以Episode为最小学习单位，而是以时间步（time-step）为单位，一次学习一个状态转换。

## Planning with a Model

规划的过程相当于求解一个MDP的过程，即给定一个模型$$M_\eta = <P_\eta, R_\eta>$$，求解MDP$$<S, A, P_\eta, R_\eta>$$，找到基于该模型的最优价值函数或最优策略，也就是在给定的状态s下确定最优的行为a。

我们可以使用之前介绍过的价值迭代，策略迭代方法；也可以使用树搜索方法（Tree Search）。

## Sample-Based Planning

Sample-Based Planning使用Model生成采样数据，即一个time-step的虚拟经历。

有了这些虚拟采样，随后使用model-free RL方法来学习得到价值或策略函数。

## Planning with an Inaccurate Model

由于实际经历的不足或者一些无法避免的缺陷，我们通过实际经历学习得到的模型不可能是完美的模型，即：$$<P_\eta, R_\eta> = <P,R>$$

而model-based RL的性能取决于Model的准确度，如果Model不准确的话，往往只能得到一些次优解。

使用近似的模型解决MDP问题与使用价值函数或策略函数的近似表达来解决MDP问题并不冲突，他们是从不同的角度来近似求解MDP问题，有时候构建一个模型来近似求解MDP比构建一个近似的价值函数或策略函数要更加方便，这是model-based RL的可取之处。

同时由于个体经历的更新，模型处在一个动态变化的过程，当利用早期经历构建的模型后来被认为存在很大错误，不适用于当下经历的时候，需要放弃模型，转而使用model-free RL。

此外，可以在模型中，增加一些不确定性，以跳出次优解。

model-based RL适用于连续变量的状态和行为空间。

## Dyna-Q Algorithm

当构建了一个环境的模型后，个体可以有两种经历来源：

- 实际经历(real experience)。

$$S'\sim P_{s,s'}^a, R=R_s^a$$

- 模拟经历(simulated experience)。

$$S'\sim P_\eta(S' | S,A), R=R_\eta(R | S,A)$$

对于Model-Free RL来说，所有的知识都来源于real experience。

对于Model-Based RL来说，Model来源于real experience，而planning相关的知识来源于simulated experience。

于是我们很自然的想到，是否可以把real experience和simulated experience结合起来呢？

![](/images/img3/Dyna.png)

上图是Dyna算法的流程图。它从实际经历中学习得到模型，然后联合使用实际经历和模拟经历，一边学习，一边规划更新价值和（或）策略函数。

Dyna-Q算法流程如下：

>Initialize $$Q(s, a)$$ and $$Model(s, a)$$ for all $$s \in S$$ and $$a \in A(s)$$   
>Do forever:   
>>(a) $$S \leftarrow$$ current (nonterminal) state   
>>(b) $$A \leftarrow \epsilon-greedy(S, Q)$$   
>>(c) Execute action A; observe resultant reward, R, and state S'   
>>(d) $$Q(S, A) \leftarrow Q(S, A)+\alpha[R+\gamma \max_a Q(S', a)-Q(S, A)]$$   
>>(e) $$Model(S, A) \leftarrow R,S'$$(assuming deterministic environment)   
>>(f) Repeat n times:   
>>>$$S \leftarrow$$ random previously observed state   
>>>$$A \leftarrow$$ random action previously taken in S   
>>>$$R, S' \leftarrow Model(S, A)$$   
>>>$$Q(S, A) \leftarrow Q(S, A)+\alpha[R+\gamma \max_a Q(S', a)-Q(S, A)]$$

这个算法赋予了个体在与实际环境进行交互式时有一段时间用来思考的能力。其中的步骤：a,b,c,d和e都是从实际经历中学习，d过程是学习价值函数，e过程是学习模型。

在f步，给以个体一定时间（或次数）的思考。在思考环节，个体将使用模型，在之前观测过的状态空间中随机采样一个状态，同时从这个状态下曾经使用过的行为中随机选择一个行为，将两者带入模型得到新的状态和奖励，依据这个来再次更新行为价值函数。

![](/images/img3/Dyna_2.png)

上图是细化后的Dyna框架图。从上面的分析可以看出，Dyna与其说是一个算法，倒不如说是一个算法框架，两者的区别在于：框架的每个组成部分，可以采用不同的算法，只要各部分之间的关系不变，就还算是同一个框架。

![](/images/img3/Dyna_3.png)

上图比较了在Dyna算法中一次实际经历期间采取不同步数的规划时的个体表现。横坐标是Episode序号，纵坐标是特定Episode所花费的步数。可以看出，当直接使用实际经历时，个体在早期完成一个Episode需要花费较多步数，而使用5步或50步思考步数时，个体完成一个Episode与环境发生的实际交互的步数要少很多。不过到了后期，基本上就没什么差别了。

可以看出**Dyna算法可以有效提升算法的收敛速度，但不会提高最终的性能**。
