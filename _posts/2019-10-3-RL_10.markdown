---
layout: post
title:  强化学习（十）——Exploration & Exploitation, Inverse Reinforcement Learning, 博弈论
category: RL 
---

# Integrating Learning and Planning

## Monte-Carlo Search（续）

下面我们结合实例（下围棋）和示意图，来实际了解MCTS的运作过程。

![](/images/img3/MCTS.png)

**第一次迭代**：五角形表示的状态是个体第一次访问的状态，也是第一次被录入搜索树的状态。我们构建搜索树：将当前状态录入搜索树中。使用基于蒙特卡罗树搜索的策略（两个阶段），由于当前搜索树中只有当前状态，全程使用的应该是一个搜索第二阶段的默认随机策略，基于该策略产生一个直到终止状态的完整Episode。图中那些菱形表示中间状态和方格表示的终止状态，在此次迭代过程中并不录入搜索树。终止状态方框内的数字1表示（黑方）在博弈中取得了胜利。此时我们就可以更新搜索树种五角形的状态价值，以分数1/1表示从当前五角形状态开始模拟了一个Episode，其中获胜了1个Episode。

![](/images/img3/MCTS_2.png)

**第二次迭代**：当前状态仍然是树内的圆形图标指示的状态，从该状态开始决定下一步动作。根据目前已经访问过的状态构建搜索树，依据模拟策略产生一个行为模拟进入白色五角形表示的状态，并将该状态录入搜索树，随后继续该次模拟的对弈直到Episode结束，结果显示黑方失败，因此我们可以更新新加入搜索树的五角形节点的价值为0/1，而搜索树种的圆形节点代表的当前状态其价值估计为1/2，表示进行了2次模拟对弈，赢得了1次，输了1次。

经过前两次的迭代，当位于当前状态（黑色圆形节点）时，当前策略会认为选择某行为进入上图中白色五角形节点状态对黑方不利，策略将得到更新：当前状态时会个体会尝试选择其它行为。

![](/images/img3/MCTS_3.png)

**第三次迭代**：假设选择了一个行为进入白色五角形节点状态，将该节点录入搜索树，模拟一次完整的Episode，结果显示黑方获胜，此时更新新录入节点的状态价值为1/1，同时更新其上级节点的状态价值，这里需要更新当前状态的节点价值为2/3，表明在当前状态下已经模拟了3次对弈，黑方获胜2次。

随着迭代次数的增加，在搜索树里录入的节点开始增多，树内每一个节点代表的状态其价值数据也越来越丰富。在搜索树内依据$$\epsilon$$-greedy策略会使得当个体出于当前状态（圆形节点）时更容易做出到达图中五角形节点代表的状态的行为。

![](/images/img3/MCTS_4.png)

**第四次迭代**：当个体位于当前（圆形节点）状态时，树内策略使其更容易进入左侧的蓝色圆形节点代表的状态，此时录入一个新的节点（五角形节点），模拟完Episode提示黑方失败，更新该节点以及其父节点的状态价值。

![](/images/img3/MCTS_5.png)

**第五次迭代**：更新后的策略使得个体在当前状态时仍然有较大几率进入其左侧圆形节点表示的状态，在该节点，个体避免了进入刚才失败的那次节点，录入了一个新节点，基于模拟策略完成一个完整Episode，黑方获得了胜利，同样的更新搜索树内相关节点代表的状态价值。

如此反复，随着迭代次数的增加，当个体处于当前状态时，其搜索树将越来越深，那些能够引导个体获胜的搜索树内的节点将会被充分的探索，其节点代表的状态价值也越来越有说服力；同时个体也忽略了那些结果不好的一些节点（上图中当前状态右下方价值估计为0/1的节点）。需要注意的是，仍然要对这部分节点进行一定程度的探索，以确保这些节点不会被完全忽视。

MCTS的优点：

- 蒙特卡罗树搜索是具有高度选择性的（Highly selective)、导致越好结果的行为越被优先选择（best-first）的一种搜索方法。

- 它可以动态的评估各状态的价值，这种动态更新价值的方法与动态规划不同，后者聚焦于整个状态空间，而蒙特卡罗树搜索是立足于当前状态，动态更新与该状态相关的状态价值。

- 使用采样避免了维度灾难；同样由于它仅依靠采样，因此适用于那些“黑盒”模型（black-box models）。

- MCTS是可以高效计算的、不受时间限制以及可以并行处理的。

参考：

https://mp.weixin.qq.com/s/4qeHfU9GS4aDWOHsu4Dw2g

记忆增强蒙特卡洛树搜索细节解读

## Dyna-2

如果我们把基于模拟的前向搜索应用到Dyna算法中来，就变成了Dyna-2算法。

使用该算法的个体维护了两套特征权重：

- 一套反映了个体的长期记忆，该记忆是从真实经历中使用TD学习得到，它反映了个体对于某一特定强化学习问题的普遍性的知识、经验；

- 另一套反映了个体的短期记忆，该记忆从基于模拟经历中使用TD搜索得到，它反映了个体对于某一特定强化学习在特定条件（比如某一Episode、某一状态下）下的特定的、局部适用的知识、经验。

# Exploration & Exploitation

几个基本的探索方法：

- **朴素探索(Naive Exploration)**: 在贪婪搜索的基础上增加一个$$\epsilon$$以实现朴素探索；

- **乐观初始估计(Optimistic Initialization)**: 优先选择当前被认为是最高价值的行为，除非新信息的获取推翻了该行为具有最高价值这一认知；

- **不确定优先(Optimism in the Face of Uncertainty)**: 优先尝试不确定价值的行为；

- **概率匹配（Probability Matching)**: 根据当前估计的概率分布采样行为；

- **信息状态搜索(Information State Search)**: 将已探索的信息作为状态的一部分联合个体的状态组成新的状态，以新状态为基础进行前向探索。

根据搜索过程中使用的数据结构，可以将搜索分为：

- **依据状态行为空间的探索(State-Action Exploration)**。针对每一个当前的状态，以一定的算法尝试之前该状态下没有尝试过的行为。

- **参数化搜索（Parameter Exploration)**。直接针对策略的函数近似，此时策略用各种形式的参数表达，探索即表现为尝试不同的参数设置。

Parameter Exploration的优点：得到基于某一策略的一段持续性的行为。

缺点：对个体曾经到过的状态空间毫无记忆，也就是个体也许会进入一个之前曾经进入过的状态而并不知道其曾到过该状态，不能利用已经到过这个状态这个信息。

## UCB

$$\epsilon$$-greedy和softmax算法的缺陷：

只关心回报是多少，并不关心每个臂被拉下了多少次，这就意味着，这些算法不再会选中初始回报特别低的臂，即使这个臂的回报只测试了一次。

UCB（The Upper Confidence Bound Algorithm）算法将会不仅仅关注于回报，同样会关注每个臂被探索的次数。

为了进一步刻画每个臂被探索的次数和回报之间的关系，UCB引入了**置信区间**的概念：

- 每个item的回报均值都有个置信区间，随着试验次数增加，置信区间会变窄（逐渐确定了到底回报丰厚还是可怜）。
- 每次选择前，都根据已经试验的结果重新估计每个item的均值及置信区间。
- 选择置信区间上限最大的那个item。

显然：

- 如果item置信区间很宽（被选次数很少，还不确定），那么它会倾向于被多次选择，这个是算法冒风险的部分；
- 如果item置信区间很窄（备选次数很多，比较确定其好坏了），那么均值大的倾向于被多次选择，这个是算法保守稳妥的部分；

UCB是一种乐观的算法，选择置信区间上界排序，如果是悲观保守的做法，是选择置信区间下界排序。

置信区间的度量有很多方式，由此产生了很多UCB的变种算法。最常用的UCB1算法，采用了如下度量方式：

$$I_i=\bar{x}_i+\sqrt{2\frac{\log t}{n_i}}$$

其中$$x_i$$是第i个臂的奖励均值，$$n_i$$是臂i当前累积被选择的次数。

参考：

https://www.cnblogs.com/Ryan0v0/p/11366578.html

多臂赌博机问题(MAB)的UCB算法介绍

https://blog.csdn.net/yw8355507/article/details/48579635

UCB算法

https://blog.csdn.net/LegenDavid/article/details/71082124

LinUCB算法

https://zhuanlan.zhihu.com/p/32356077

Multi-Armed Bandit: UCB (Upper Bound Confidence)

# Inverse Reinforcement Learning

逆强化学习问题定义：

- 已知：

智能体在变化的环境中的行为

（optional）智能体传感器的数据

（optional）环境的模型

- 求：

最优化的Reward function

逆强化学习研究意义：

- 对动物人类行为用强化学习建模（生物学、计量经济学...）模仿学习。

- reward function在强化学习里面非常非常重要，是对行为的抽象精简的描述，因此IRL (Inverse Reinforcement Learning)可能是一种很高效的模仿学习范式。

通俗的说法就是：**我们用优化控制或强化学习得到的策略能用来解释人类的行为吗？**

比如让一个人去拿桌子上的一个橘子，那手的轨迹一定不是一条从起点到目标的直线，而是有一些弯曲的轨迹，也就是带有偏差的较优行为，但是这种偏差其实并不重要，只要最后拿到橘子就行了，也就是说：

1.有过程中一些偏差是不重要的，但另一些偏差就比较重要了（如最后没拿到）。

2.每次拿橘子的动作也是不一样的，因此人的动作带有一定的随机性。

3.我们可以认为人类行为轨迹分布是以最优策略为峰的随机分布。

![](/images/img3/GAN_vs_IRL.png)

参考：

https://zhuanlan.zhihu.com/p/61674994

Algorithms for Inverse Reinforcement Learning

https://zhuanlan.zhihu.com/p/32502503

CS 294：IRL

# 博弈论

博弈论（game theory）是一门单独的学科，和RL并无统属关系。然而由于RL，特别是MARL大量应用到了相关的知识，所以这里也把它写在RL系列里了。

## 历史

博弈论最早可追溯到“齐威王田忌赛马”，但它真正的发展是在20世纪下半叶。

RL的历史相对比较晚，因此从渊源来看，RL=博弈论+控制论+ML。

参考：

https://blog.csdn.net/sobermineded/article/details/79601986

博弈论历史、发展与应用
