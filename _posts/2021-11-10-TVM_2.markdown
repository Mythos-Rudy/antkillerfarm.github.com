---
layout: post
title:  TVM实战
category: AI 
---

* toc
{:toc}

# TVM实战

## 问题的由来（续）

这是TFLite模型的IR片段：

```text
  %0 = qnn.quantize(%input, 0.0186579f /* ty=float32 */, 114 /* ty=int32 */, out_dtype="uint8", axis=1) /* ty=Tensor[(1, 3, 224, 224), uint8] */;
  %1 = nn.pad(%0, 114f /* ty=float32 */, pad_width=[[0, 0], [0, 0], [1, 1], [1, 1]]) /* ty=Tensor[(1, 3, 226, 226), uint8] */;
  %2 = qnn.conv2d(%1, meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), int8] */, 114 /* ty=int32 */, 0 /* ty=int32 */, 0.0186579f /* ty=float32 */, 0.00288958f /* ty=float32 */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(1, 32, 112, 112), int32] */;
  %3 = nn.bias_add(%2, meta[relay.Constant][1] /* ty=Tensor[(32), int32] */) /* ty=Tensor[(1, 32, 112, 112), int32] */;
  %4 = qnn.requantize(%3, 5.39136e-05f /* ty=float32 */, 0 /* ty=int32 */, 0.0150183f /* ty=float32 */, 0 /* ty=int32 */, axis=1, out_dtype="uint8") /* ty=Tensor[(1, 32, 112, 112), uint8] */;
```

可以看出同一个QnnConv2D两者的展开形式存在一定的差异，但是语义上却基本是一致的。

## Relay IR

在继续主题之前，我们首先来看看这个展开形式的差异是如何造成的。

TFLite/Pytorch模型导入之后，有一个转换成Relay IR的过程，也就是所谓的frontend。

相关代码在：

TFLite: python/tvm/relay/frontend/tflite.py

Pytorch: python/tvm/relay/frontend/pytorch.py和python/tvm/relay/frontend/qnn_torch.py

可以看出Pytorch的量化模型的weight是浮点数，而TFLite的量化模型的weight是整数。

有frontend自然就有backend：

python/tvm/relay/op/contrib/ethosn.py

和切割计算图有关的代码主要是：

```python
seq = tvm.transform.Sequential(
    [
        ......
        transform.MergeComposite(pattern_table()),
        transform.AnnotateTarget("ethos-n"),
        transform.MergeCompilerRegions(),
        transform.PartitionGraph(),
    ]
)
seq(mod)
```

这里的Pass基本看名字就知道功能了，唯一需要关注的是MergeComposite。

这是pattern_table的其中一个表项：

```python
def qnn_conv_pattern():
    pattern = is_op("nn.pad")(wildcard(), wildcard()) | wildcard()
    pattern = is_op("qnn.conv2d")(
        pattern, is_constant(), is_constant(), is_constant(), is_constant(), is_constant()
    )
    pattern = is_op("nn.bias_add")(pattern, is_constant())
    pattern = is_op("qnn.requantize")(
        pattern, is_constant(), is_constant(), is_constant(), is_constant()
    )
    return pattern
```

一般来说IR会定的比较细粒度，而AI硬件更喜欢粗粒度的op。所以往往若干个IR op组合起来，才能得到一个AI硬件op。这时就需要进行模板匹配。

一般来说，数据可以分为变量和常量（`is_constant()`），如果既可能是变量，也可能是常量的话，就用`wildcard()`匹配。

MergeComposite会将这个模板打包成一个函数，变量会写为函数的参数，而常量放到全局的meta data里。

## 添加Pass

下面我们来看看如何添加Pass进行这样的转换。

```cpp
class QnnQuantizeConstFold : public DFPatternRewrite {
 public:
  QnnQuantizeConstFold() {
    data_pat_ = IsConstant();
    pattern_ = IsOp("qnn.quantize")({data_pat_, IsConstant(), IsConstant()});
  }

  Expr Callback(const Expr& pre, const Expr& post,
                const Map<DFPattern, Array<Expr>>& node_map) const override {
    ......

    if (output->dtype == DataType::Int(8)) {
      return QuantizeData<int8_t>(......);
    } else if (output->dtype == DataType::Int(32)) {
      return QuantizeData<int32_t>(......);
    }
    return post;
  }

 protected:
  DFPattern data_pat_;
};
```

`Rewrite_`是Pass最重要的函数。其中最简单的当属`DFPatternRewrite`。

`Callback`的参数中，`pre`表示原始`Expr`，`post`表示替换后的`Expr`，返回值也是替换后的`Expr`。（方便使用链式调用？）

如果Pass什么都不做的话，直接返回`post`就可以了。

PS：虽然`pre`和`post`在运行之初是相同的，但是一旦替换开始，两者就有差异了，所以如果是写入的内容，一定要引用`post`里的那份。

## Python版的Pass

Pass不仅能用C++写，也可用python写。

```python
class QnnQuantizeConstFold(tvm.relay.dataflow_pattern.DFPatternCallback):
    def __init__(self, require_type=False):
        super().__init__(require_type)
        self.pattern = is_op("qnn.quantize")(
            is_constant(), is_constant(), is_constant()
        )

    def callback(self, pre, post, node_map):
        ......
        if (dtype == "int8"):
             return tvm.relay.Constant(tvm.nd.array(data.astype(np.int8)))
        if (dtype == "int32"):
            return tvm.relay.Constant(tvm.nd.array(data.astype(np.int32)))
        return post
```

可以看出，写法也是大同小异，只是更便于集成，也不用写FFI接口了。

使用方法：

```python
func = mod["main"]
func = tvm.relay.Function(func.params, func.body, None, func.type_params, func.attrs)
func = tvm.relay.dataflow_pattern.rewrite(RemoveClipAfterRequantize(), func)
func = tvm.relay.dataflow_pattern.rewrite(QnnQuantizeConstFold(), func)
mod = tvm.IRModule.from_expr(func)
```

这里展示了`DFPatternCallback`的使用方法，还有`IRModule`和`Expr`的相互转换方法。

参考：

https://tvm.apache.org/docs/reference/langref/relay_pattern.html

Pattern Matching in Relay

## tvmc

tvmc是tvm提供的一个命令行接口。

```bash
export TARGET="bnns, llvm -device=arm_cpu -mtriple=aarch64-linux-gnu"
python3 -m tvm.driver.tvmc compile ./mobilenet_v1_0.25_224_quant.tflite --target "$TARGET" -o tvmc.tar --cross-compiler "$CC" --cross-compiler-options "$CC_OPTIONS"
```

要点如下：

1. 注意`TARGET`的写法，这展示了如何将一个包含空格的字符串作为一个bash变量传递给命令行参数的做法。定义变量时的双引号和使用时的双引号都是必不可少的。

2. 有些backend需要partation，将能执行的op分配到该设备上，因此`TARGET`也包含了两部分（用逗号分隔），其中第一部分用于partation。

相关代码在：

python/tvm/driver/tvmc/composite_target.py

# SPIR-V

SPIR-V是一种用于GPU通用计算和图形学的中间语言。

https://blog.csdn.net/qwertyu1234/article/details/50163847

SPIR-V研究：编译器基本原理

# AI Chip+

https://zhuanlan.zhihu.com/p/57808378

AI芯片0.5与2.0

https://mp.weixin.qq.com/s/XDwTI-gnnFMLjVBbOGKL9w

清华大学团队研制高能效通用神经网络处理器芯片STICKER-T

https://mp.weixin.qq.com/s/xbHP1RFn7F7BbimxgWaKqg

Facebook把服务27亿人的AI硬件系统开源了

https://mp.weixin.qq.com/s/BD-HAILp3TPvBFlIy6QC4w

一文看懂机器视觉芯片

https://mp.weixin.qq.com/s/PMnNay4CRgVghA4fU9oLqg

牛津大学研发类脑光子芯片，运算速度超人脑1000倍

https://mp.weixin.qq.com/s/e333KjLavEvvpNIL3u1Y4Q

NovuMind异构智能核心技术引领智联网

https://mp.weixin.qq.com/s/fSSyOs4-NXbPTbDjpfJBNQ

Google IPU：互联网巨头纷纷进军芯片行业是为何？

https://mp.weixin.qq.com/s/S1y4NEx4_Mgwf68S2pexqA

拿着锤子找钉子，数字芯片领导者比特大陆进军人工智能

https://mp.weixin.qq.com/s/gtgPYf939uYRzxAab_LZLQ

谢源：计算存储一体化，在存储里做深度学习，架构创新实现下一代AI芯片

https://mp.weixin.qq.com/s/s-fYxv4z5kkJUFueU2IR7w

BP表达式与硬件架构：相似性构建更高效的计算单元

https://mp.weixin.qq.com/s/1r7G84les7FihqPbSiS0Ng

华为首款手机端AI芯片麒麟970

https://mp.weixin.qq.com/s/y9dVg9YtfWxu6NcW-fxi6Q

内存带宽与计算能力，谁才是决定深度学习执行性能的关键？

https://mp.weixin.qq.com/s/K_dohaZbCISZlxe1Utu50w

如何用FPGA加速卷积神经网络(CNN)？

https://mp.weixin.qq.com/s/z68hk1yqg60QCjgTyzgG2w

GPU深度学习的“加速神器”

https://mp.weixin.qq.com/s/O-NDsFs6AOwl43LyevXtzg

OpenAI发布“块稀疏”GPU内核：实现文本情感分析与图像生成建模当前最优水平

https://mp.weixin.qq.com/s/Qfbc2iQnXacOqOGIrpRQRw

Tensor Core究竟有多快？全面对比英伟达Tesla V100/P100的RNN加速能力

https://mp.weixin.qq.com/s/XXef4F9HEZizoWRYXwHitw

如何配置一台深度学习工作站?

https://mp.weixin.qq.com/s/_xFRRkVyN9qevLeek7bFxQ

深度学习中GPU和显存分析

https://mp.weixin.qq.com/s/k5Xx-nnaf-yfWqGLIY3LEg

特斯拉的芯片究竟多强

https://zhuanlan.zhihu.com/p/88927564

窥探神经网络加速器的数据复用（一）

https://mp.weixin.qq.com/s/YARcCzQXqnJmWRtV2zd_FQ

国内外AI芯片发展现状

https://mp.weixin.qq.com/s/b_Hy0JSZ5ZGT9AsczCkp9Q

ISSCC 2020：AI芯片架构的转变

https://zhuanlan.zhihu.com/p/115219461

张量在神经网络加速器中的应用

https://pan.baidu.com/s/1zcXdXTaN3dbJ9VGplViHZw 提取码:ews4

AI芯片体系架构和软件专题报告会2020论文下载

https://mp.weixin.qq.com/s/cBVio3W64np8fXwUQs2wIw

机器学习如何用于芯片系统设计？Jeff Dean推荐Google最新《机器学习系统芯片设计》70页ppt为你讲解

https://zhuanlan.zhihu.com/p/150656419

AI芯片技术发展

https://mp.weixin.qq.com/s/9fBe6MsUOCDhtp4MSfP2jg

AI处理器热潮正在消退

https://mp.weixin.qq.com/s/JYTqJDlGw6Q2gNLaYIGLcQ

特斯拉芯片究竟怎么样？

https://mp.weixin.qq.com/s/cA1AXfpV3ZcMNX9k5XKlww

面向深度神经网络的芯片布图规划问题简介

https://zhuanlan.zhihu.com/p/231302709

聊聊GPU峰值计算能力

https://zhuanlan.zhihu.com/p/340775090

从Thinker到Evolver：对可演化AI芯片的探索

https://mp.weixin.qq.com/s/kuIsyX0QEIeFHn8tvFE8vw

AI训练的最大障碍不是算力，而是“内存墙”

https://mp.weixin.qq.com/s/MwZ9j1MIwRBrJK4iWKzRqQ

芯故事：和AMD有渊源的那些AI创业公司

https://zhuanlan.zhihu.com/p/446830540

GPU架构（上）—AI芯片设计入门

https://zhuanlan.zhihu.com/p/446837354

GPU架构（下）—AI芯片设计入门
