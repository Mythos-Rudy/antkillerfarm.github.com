---
layout: post
title:  强化学习（三）——动态规划, Monte-Carlo
category: RL 
---

* toc
{:toc}

# Markov Decision Process（续）

MDP的扩展主要包括：

a) Observation部分可见的情况下，agent state $$\neq$$ environment state，这时一般叫做partially observable Markov decision process(POMDP)。

b) Infinite and continuous MDP

c) Undiscounted, average reward MDP

扩展MDP的Bellman equation都不是线性方程，没有解析解，只有迭代解。相关解法主要使用了概念图模型，这里不再详述。

参考：

https://mp.weixin.qq.com/s/G8zmooqhSKqcLv1u7uM0bw

AlphaGo等智能体是如何炼成的？你需要懂得马尔科夫链

https://mp.weixin.qq.com/s/KQmKb5NrjgxooCnbZaiTzw

强化学习中无处不在的贝尔曼最优性方程，背后的数学原理为何？

# 动态规划

Dynamic programming(DP)用于解决那些可分解为**重复子问题（overlapping subproblems）**并具有**最优子结构（optimal substructure）**的问题。这里的programming和编程并无任何关系。

上世纪40年代，Richard Bellman最早使用动态规划这一概念表述通过遍历寻找最优决策解问题的求解过程。1953年，Richard Bellman将动态规划赋予现代意义，该领域被IEEE纳入系统分析和工程中。

除了Bellman之外，苏联的Lev Pontryagin也做出了很大的贡献，他和Bellman被并称为Optimal control之父。

>Lev Semyonovich Pontryagin，1908~1988，苏联数学家。主要研究代数拓扑和微分拓扑。他14岁时，因为煤气爆炸事故成为盲人。苏联科学院院士，国际数学家联盟副主席。

## 最优子结构

最优子结构即可用来寻找整个问题最优解的子问题的最优解。举例来说，寻找图上某顶点到终点的最短路径，可先计算该顶点所有相邻顶点至终点的最短路径，然后以此来选择最佳整体路径，如下图所示：

![](/images/article/Shortest_path_optimal_substructure.png)

一般而言，最优子结构通过如下三个步骤解决问题：

a) 将问题分解成较小的子问题；

b) 通过递归使用这三个步骤求出子问题的最优解；

c) 使用这些最优解构造初始问题的最优解。

子问题的求解是通过不断划分为更小的子问题实现的，直至我们可以在常数时间内求解。

## 重复子问题

![](/images/article/Fibonacci_dynamic_programming.png)

重复子问题是指通过相同的子问题可以解决不同的较大问题。例如，在Fibonacci序列中，F3 = F1 + F2和F4 = F2 + F3都包含计算F2。由于计算F5需要计算F3和F4，一个比较笨的计算F5的方法可能会重复计算F2两次甚至两次以上。

为避免重复计算，可将已经得到的子问题的解保存起来，当我们要解决相同的子问题时，重用即可。该方法即所谓的**缓存（memoization）**。

动态规划通常采用以下两种方式中的一种：

**自顶向下**：将问题划分为若干子问题，求解这些子问题并保存结果以免重复计算。该方法将递归和缓存结合在一起。

**自下而上**：先行求解所有可能用到的子问题，然后用其构造更大问题的解。该方法在节省堆栈空间和减少函数调用数量上略有优势，但有时想找出给定问题的所有子问题并不那么直观。

需要注意的是：动态规划更多的看作是一种解决问题的方法论，而非具体的数值算法，因此，很多不同领域的算法都可看做是动态规划算法的实例。参考文献中，就列出了不少这样的算法。显然，动态规划是一种**迭代（Iteration）算法**。

由前文的描述可知，MDP正好具备overlapping subproblems和optimal substructure的特性，因此也可以通过DP求解。

在继续下文之前，推荐一波资源：

>David Poole，加拿大不列颠哥伦比亚大学教授。加拿大AI协会终身成就奖（2013年）。   
>个人主页：   
>http://www.cs.ubc.ca/~poole/index.html

David Poole的主页上有很多好东西：

http://www.cs.ubc.ca/~poole/demos/

该网页上有一些RL方面的用java applet做的可视化demo。由于年代比较久远，这些demo无法在目前的浏览器上运行。所以，我对其做了一些改造，使之能够使用。相关代码参见：

https://github.com/antkillerfarm/antkillerfarm_crazy/tree/master/RL

>2019.10   
>由于appletviewer在最新的JDK中已经被移除了，因此，这次选择JS作为目标语言，并对其中的Q-Learning做了移植。   
>代码：   
>https://github.com/antkillerfarm/antkillerfarm_crazy/tree/master/nodejs/js/RL/Q

David Poole还写了一本书《Artificial Intelligence: foundations of computational agents》，目前已经是第2版了。

其中的代码资源参见：

http://artint.info/AIPython/

从该书所用编程语言的变迁，亦可感受到Poole教授不断学习的脚步。要知道Poole教授刚进入学术界的时代（1985年前后），就连Java也还没被发明出来呢。

http://uhaweb.hartford.edu/compsci/ccli/samplep.htm

Hartford大学的这个网站也有些不错的资料，偏重RL、机器人、ML for Game等领域。

## RL与DP

在继续讲述之前，我们首先来明确几个概念：

![](/images/img2/RL_3.png)

$$v_{\pi}$$和$$q_{\pi}$$的定义参见《强化学习（一）》。

$$v_*(s)=\max_{\pi}v_{\pi}(s)$$

$$q_*(s,a)=\max_{\pi}q_{\pi}(s,a)$$

RL领域的DP算法的主要思想是：利用value function构建搜索Good Policy的方法。这里用$$v_*(s)$$或$$q_*(s, a)$$表示最优的value function。

RL DP主要包括以下算法：（为了抓住问题的本质，这里仅列出各算法最关键的Bellman equation，至于流程参照Q-learning算法即可。）

### Policy Iteration

Policy Iteration包含如下两步：

Policy Evaluation：

$$v_{k+1}(s) = \sum_{a \in \mathcal{A}}\pi(a \mid  s)\left(\mathcal{R}_s^a + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a v_k(s')\right)$$

Policy Improvement：

$$\pi_{k+1}(s) = \arg \max_{a \in \mathcal{A}}\left(\mathcal{R}_s^a + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a v_k(s')\right)$$

由于$$q_*(s, a)$$对应的$$v(s)$$必是$$v_*(s)$$，反之亦然，因此Policy Iteration的过程通常如下所示：

![](/images/article/Policy_Iteration.png)

![](/images/article/Policy_Iteration_2.png)

如果只需要Prediction，不需要Control的话，也可以只进行Policy Evaluation，这时也被称为Iterative Policy Evaluation。

### Value Iteration

$$v_{k+1}(s) = \max_{a \in \mathcal{A}}\left(\mathcal{R}_s^a + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a v_k(s')\right)$$

state-value function迭代的复杂度是$$O(mn^2)$$，其中m为action的数量，n为state的数量。而action-value function迭代的复杂度是$$O(m^2n^2)$$

Policy Iteration与Value Iteration的区别在于：

1.Policy Iteration是在迭代过程中不断寻找最优策略。

2.Value Iteration是在迭代过程中寻找状态s下的最优value。如果所有状态的最优value都找到的话，则可通过寻找最大value路径的方式，找到最优策略。例如，之前提到的Q-Learning。

动态规划的主要局限在于：

1.它依赖于概率模型。

2.计算复杂度太高，只适合规模中等（<1M的状态数）的情况。

## DP的一些扩展

异步动态规划（Asynchronous Dynamic Programming）

采样更新（Sample Backups）

近似动态规划（Approximate Dynamic Programming）

## 参考

http://www.cppblog.com/Fox/archive/2008/05/07/Dynamic_programming.html

动态规划算法（Wikipedia中文翻译版）

https://www.zhihu.com/question/23995189

什么是动态规划？动态规划的意义是什么？

http://www.cnblogs.com/steven_oyj/archive/2010/05/22/1741374.html

动态规划算法

http://www.cppblog.com/menjitianya/archive/2015/10/23/212084.html

动态规划

https://segmentfault.com/a/1190000004454832

动态规划

http://www.cnblogs.com/jmzz/archive/2011/06/26/2090702.html

动态规划

http://www.cnblogs.com/jmzz/archive/2011/07/01/2095158.html

DP之Warshall算法和Floyd算法

http://www.cnblogs.com/jmzz/archive/2011/07/02/2096050.html

DP之最优二叉查找树

http://www.cnblogs.com/jmzz/archive/2011/07/02/2096188.html

DP之矩阵连乘问题

http://www.cnblogs.com/jmzz/archive/2011/07/05/2098630.html

DP之背包问题+记忆递归

http://www.cs.upc.edu/~mmartin/Ag4-4x.pdf

Bellman equations and optimal policies

https://mp.weixin.qq.com/s/a1C1ezL59azNfdM3TFGaGw

递推与储存，是动态规划的关键

http://www.cnblogs.com/SDJL/archive/2008/08/22/1274312.html

通过金矿模型介绍动态规划

https://mp.weixin.qq.com/s/fTQbatHpZFxxU6VGhJNVrg

0-1背包问题的动态规划算法

https://mp.weixin.qq.com/s/1yAnUVhzbckCqG2_ie07wg

什么是动态规划（Dynamic Programming）?

https://zhuanlan.zhihu.com/p/81819678

矩阵特征分解求动态规划的终极解法

https://mp.weixin.qq.com/s/630VEIh0Cbu7i8imE7L39w

回溯算法和动态规划，到底谁是谁爹？

https://mp.weixin.qq.com/s/-_DqyBIG3aSDWIYtkD7xuA

看动画轻松理解“递归“与“动态规划”

# Monte-Carlo

## 概率算法

概率算法是和确定性算法相对的概念。概率算法的一个基本特征是对所求解问题的同一实例用同一概率算法求解两次，可能得到完全不同的效果。这两次求解问题所需的时间甚至所得到的结果，可能会有相当大的差别。

常见的概率算法主要有：数值概率算法，舍伍德（Sherwood）算法，蒙特卡罗（Monte Carlo）算法和拉斯维加斯（Las Vegas）算法。

在《数学狂想曲（三）》中我们已经提到了“统计模拟”的概念，这实际上就是数值概率算法的应用，它主要利用了大数定律与强大数定律。

这里有两个容易混淆的概念：**Monte Carlo method**和**Monte Carlo algorithm**。

Monte Carlo method这个概念的范围非常广，它实际上就是概率算法的别名。诸如用随机投针计算圆周率之类的算法，都可以看作是Monte Carlo method。

Monte Carlo algorithm就要狭义的多了，详情见下文。

## 舍伍德（Sherwood）算法

设A是一个确定算法，$$f(x)$$是解某个实例x的执行时间，设n是一整数，$$X_n$$是大小为n的实例的集合.假定$$X_n$$中每一个实例是等可能出现的，则算法A解$$X_n$$中一个实例的平均执行时间是:

$$\overline{f(x)}=\sum_{x\in X_n}f(x)/n$$

假如存在一个实例$$x_0$$使得$$f(x_0)\gg \overline{f(x)}$$，比如快速排序在最坏情况下的复杂度$$O(n)\gg O(n\log n)$$。这时使用sherwood对原始算法进行改进是有价值的。

Sherwood算法通过增加一个较小的额外开销从而使得算法的复杂度与具体实例x无关。例如，在快速排序中，我们是以第一个元素为基准开始排序时，为了避免这样的情况，可以用舍伍德算法解决，也就是使第一个基准元素是随机的。
