---
layout: post
title:  TVM
category: AI 
---

* toc
{:toc}

# TVM

TVM是陈天奇领导的一个DL加速框架项目。它处于DL框架（如tensorflow、pytorch）和硬件后端（如CUDA、OpenCL）之间，兼顾了前者的易用性和后者的执行效率。

官网：

https://tvm.apache.org/

代码：

https://github.com/apache/tvm

![](/images/img3/tvm_stack.png)

论文：

《TVM: End-to-End Optimization Stack for Deep Learning》

和同类项目的差异：

- TFLite和ONNXRuntime只能接收特定格式的模型。而TVM这些都能接收。

- NCNN、MACE之类的项目，一般只考虑了ARM CPU的优化，对于异构计算做的比较少。

- TVM强化了图优化的部分，使之更类似于编译器的架构。

## TVM Runtime

![](/images/img4/tvm_rpc.png)

TVM采用C/S模式进行部署，其中在target机器上的部分，被称为TVM Runtime。

TVM Runtime的代码通常比较薄，只需要把host发过来的优化结果执行即可。

官方文档：

http://tvm.apache.org/docs/dev/runtime.html

参考：

https://zhuanlan.zhihu.com/p/369981405

部署TVM Runtime

## NNVM

NNVM是一个类似于ONNX、NNEF的中间表示。

官网：

https://github.com/dmlc/nnvm

参考：

https://mp.weixin.qq.com/s/qkvX0rmEe0yQ-BhCmWAXSQ

李沐：AWS开源端到端AI框架编译器NNVM

## Relay

Relay是TVM中用来替代NNVM的模块，其本身被认为是NNVM第二代。

参考：

https://zhuanlan.zhihu.com/p/91283238

TVM图编译器Relay简单探究

## backend

python层面：

python/tvm/relay/op/contrib/ethosn.py

```python
@register_pattern_table("ethos-n")
def pattern_table():
@tvm.ir.register_op_attr("nn.max_pool2d", "target.ethos-n")
def max_pool2d(attrs, args):
```

relay层面：

src/relay/backend/contrib/ethosn

```cpp
TVM_REGISTER_GLOBAL("relay.ext.ethos-n").set_body_typed(CompileEthosn);
```

runtime层面：

src/runtime/contrib/ethosn

test：

tests/python/contrib/test_ethosn

## 代码分析

TVM建立了一套类型系统：

```cpp
class BaseExprNode : public Object;
class BaseExpr : public ObjectRef;
```

根据基类，查看实际类型：

`XX->checked_type()`

## Quantize

![](/images/img4/TVM_QNN.png)

## 参考

https://zhuanlan.zhihu.com/p/139552817

一篇关于深度学习编译器架构的综述论文

https://www.zhihu.com/question/396105855

针对神经网络的编译器和传统编译器的区别和联系是什么？

https://mp.weixin.qq.com/s/8bXwxYyNjdThlGQQ70cgWQ

TVM：端到端自动深度学习编译器，244页ppt

https://zhuanlan.zhihu.com/p/333706468

TVM学习系列blog

https://zhuanlan.zhihu.com/p/163717035

AI编译优化

https://www.zhihu.com/question/267167829

如何看待Tensor Comprehensions？与TVM有何异同？（这个问题下的答案不多，但基本都是陈天奇、贾扬清之类的大佬）

https://mp.weixin.qq.com/s/irvBbPKENiZX9G_6wh5c-Q

陈天奇等人提出TVM：深度学习自动优化代码生成器

https://mp.weixin.qq.com/s/28n8g_epHsYB0I9GVc_lww

陈天奇团队TVM重磅更新：直接在浏览器使用GPU

https://mp.weixin.qq.com/s/7JGLm-hkCZBNDLA98qvWNA

自动生成硬件优化内核：陈天奇等人发布深度学习编译器TVM

https://mp.weixin.qq.com/s/YVIvdMznb3oatIXqD5a5_A

陈天奇等人提出AutoTVM：让AI来编译优化AI系统底层算子

https://mp.weixin.qq.com/s/HquT_mKm7x_rbDGz4Voqpw

阿里巴巴最新实践：TVM+TensorFlow提高神经机器翻译性能

https://zhuanlan.zhihu.com/p/50529704

手把手带你遨游TVM

https://mp.weixin.qq.com/s/z5rsU_uAAaRxgD9YAxDkZA

陈天奇：深度学习编译技术的现状和未来

https://zhuanlan.zhihu.com/p/75203171

如何利用TVM快速实现超越Numpy(MKL)的GEMM

https://zhuanlan.zhihu.com/p/58918363

TVM: Deep Learning模型的优化编译器

https://zhuanlan.zhihu.com/p/87664838

也谈TVM和深度学习编译器

https://mp.weixin.qq.com/s/VE3CySjjS2rTpUDPnKcLTg

陈天奇最新研究：递归模型编译器CORTEX

https://zhuanlan.zhihu.com/p/358585143

深度学习编译器及TVM介绍

https://zhuanlan.zhihu.com/p/360385060

TVM中的scheduler

https://mp.weixin.qq.com/s/Kt4xDLo-NRui8Whl0DqcSA

Data Flow和Control Flow

https://zhuanlan.zhihu.com/p/388452164

tvm or mlir？

# GLOW

代码：

https://github.com/pytorch/glow

参考：

https://zhuanlan.zhihu.com/p/102127047

Glow: Graph Lowering Compiler for Neural Networks

# NNFusion

代码：

https://github.com/microsoft/nnfusion

参考：

https://mp.weixin.qq.com/s/CMTOW3cYQkkECpPuzZl0nQ

RAMMER如何进一步“压榨”加速器性能

# Pytorch+

https://zhuanlan.zhihu.com/p/272767300

Pytorch转ONNX-理论篇

https://zhuanlan.zhihu.com/p/273566106

Pytorch转ONNX-实战篇1（tracing机制）

https://zhuanlan.zhihu.com/p/286298001

Pytorch转ONNX-实战篇2（实战踩坑总结）

https://mp.weixin.qq.com/s/EXuFXbPBIbzTyi0fUjvvPw

两行代码统计模型参数量与FLOPs，这个PyTorch小工具值得一试

https://zhuanlan.zhihu.com/p/73711222

巧用torch.backends.cudnn.benchmark减少训练时间

https://mp.weixin.qq.com/s/HginBrMOfEEWsZKq67u6EA

在Pytorch中构建流数据集

https://www.zhihu.com/question/303070254

PyTorch中在反向传播前为什么要手动将梯度清零？

https://mp.weixin.qq.com/s/HQnI8rzPvZN6Q_5c8d1nVQ

唯快不破：基于Apex的混合精度加速

https://mp.weixin.qq.com/s/NupSd4e01cvQ3CRnjy1npw

超原版速度110倍，针对PyTorch的CPU到GPU张量迁移工具开源

https://zhuanlan.zhihu.com/p/87572724

一文看懂align_corners

https://www.zhihu.com/question/274635237

Pytorch有什么节省内存（显存）的小技巧？

https://mp.weixin.qq.com/s/S1dRfmqpiLzR3tnsocmfvw

Pytorch中的数据增强方式最全解释

https://mp.weixin.qq.com/s/BTFMvV2ppmRBXYg95YlK4w

PyTorch实现L2和L1正则化的方法

https://zhuanlan.zhihu.com/p/98535650

研究生应当掌握的并行训练方法（单机多卡）

https://zhuanlan.zhihu.com/p/86441879

pytorch多gpu并行训练

https://mp.weixin.qq.com/s/KP4etDrGlJmRAMQmR1mTJA

基于C++的PyTorch模型部署

https://mp.weixin.qq.com/s/uUxwMFGF9nJiraVQsIqu2Q

PyTorch重大更新：将支持自动混合精度训练！

https://zhuanlan.zhihu.com/p/145427849

PyTorch Parallel Training（单机多卡并行、混合精度、同步BN训练指南文档）

https://mp.weixin.qq.com/s/Y6sJhnmjRwN2uopHgr7nFA

让PyTorch更轻便，这款深度学习框架你值得拥有！

https://mp.weixin.qq.com/s/_3iJCO4gQz7mWcW7G8kimQ

Pytorch实现卷积神经网络训练量化（QAT）

https://zhuanlan.zhihu.com/p/104019160

PyTorch常用代码段

https://mp.weixin.qq.com/s/MJgQqwWa4wyNgtKZaX_ADQ

Tensorboard可视化与Hook机制

https://mp.weixin.qq.com/s/jnV_4REXOR-ema1kOC95Nw

跨越重重“障碍”，我从PyTorch转换为了TensorFlow Lite

https://mp.weixin.qq.com/s/Svh27YIG2jYWqXwhhEuoSw

使用Pytorch和BERT进行多标签文本分类

https://mp.weixin.qq.com/s/5Nq3y8hwhQG1UV9lHuBQvA

13个你一定要知道的PyTorch特性

https://mp.weixin.qq.com/s/cKvkvWgVPuq9y1A5q1OPEQ

跟着指南学PyTorch—迁移学习教程

https://mp.weixin.qq.com/s/gJgxh4l0CXTlaJaQ_FS3YQ

PyTorch的数据增强与数据标准化

https://mp.weixin.qq.com/s/Mo7XhRcPkgurmQPJ3Zu1ug

基于PyTorch的计算机视觉框架

https://mp.weixin.qq.com/s/PWABh72t92pUOJufcmzvag

用PyTorch做深度学习实验！Facebook新框架Ax和BoTorch双双开源

https://mp.weixin.qq.com/s/LcwlCai7PMYOBwsLXPS5HA

PyTorch语义分割开源库semseg

https://mp.weixin.qq.com/s/oDYMTb9NWxVsW07FLQKA_Q

万字综述，核心开发者全面解读PyTorch内部机制

https://www.zhihu.com/question/274635237

Pytorch有什么节省内存（显存）的小技巧？

https://mp.weixin.qq.com/s/xe5zmJklT2sqn_zffmyrLg

Sharded:在相同显存的情况下使pytorch模型的参数大小加倍

https://mp.weixin.qq.com/s/maOnO_o5y19X2D-ZnLjsJA

PyTorch中的In-place操作是什么？为什么要避免使用这种操作？

https://zhuanlan.zhihu.com/p/299736532

使用PyTorch 1.6 for Android

https://mp.weixin.qq.com/s/1ugk6uI6lfWEEUvtKIfYNA

9个让PyTorch模型训练提速的技巧！

https://mp.weixin.qq.com/s/kZvdgWqk1KLi790rly3YYQ

Pytorch中的分布式神经网络训练

https://mp.weixin.qq.com/s/biHcUt55-9RfqYJ_Dg_7Tg

TorchMetrics：PyTorch的指标度量库

https://zhuanlan.zhihu.com/p/363319763

PyTorch vs LibTorch：网络推理速度谁更快？

https://mp.weixin.qq.com/s/RBclQdtaA8prvSoUUdhrEQ

机器学习的Pytorch实现资源集合

https://mp.weixin.qq.com/s/zPv-3fMy1rZwAwPqjs7oAA

Pytorch图像分类从模型自定义到测试

https://zhuanlan.zhihu.com/p/46636027

1张图学会PyTorch+TensorFlow+MXNet+TF Eager

https://mp.weixin.qq.com/s/yS9PAw926Y7AsGRW0eHG0Q

基于PyTorch的GAN框架TorchGAN：用架构级API轻松定制GAN项目

https://github.com/CVBox/PyTorchCV

一个基于pytorch的CV框架

https://mp.weixin.qq.com/s/w09hcJof80m2VGwn7SgKmQ

TorchSeg—基于PyTorch的快速模块化语义分割开源库

https://mp.weixin.qq.com/s/TsR-jgO2c2-dbqnk1mEj8w

想读读PyTorch底层代码？这份内核机制简介送给你

https://mp.weixin.qq.com/s/Lzt3LbO6lBbOebNV1d2pLQ

迁移学习不好懂？这里有一个PyTorch项目帮你理解

https://mp.weixin.qq.com/s/7fK6GNyzYTP0fQy7F01fZw

PyTorch深度学习模型训练加速指南2021

https://mp.weixin.qq.com/s/SReuVBN8WIXFlnwho3wqgQ

最详细的Pytorch底层算子扩展总结

https://mp.weixin.qq.com/s/14_pt0_skKYNw2sAK5Zptw

Pytorch底层算子扩展最详细的总结

https://zhuanlan.zhihu.com/p/363317178

模型转换：由Pytorch到TFlite

https://mp.weixin.qq.com/s/Mj7xI5rFTxKaXswYi9_mRQ

我的PyTorch模型比内存还大，怎么训练呀？

https://mp.weixin.qq.com/s/TjCUCbXL3oSgJNYoEltgrg

7个提升PyTorch性能的技巧

https://zhuanlan.zhihu.com/p/275755543

一款全平台轻量级pytorch推理框架Msnhnet
