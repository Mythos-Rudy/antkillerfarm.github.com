---
layout: post
title:  数学狂想曲（五）——玻尔兹曼分布
category: math 
---

* toc
{:toc}

# 矩阵&向量的积（续）

## 矩阵的张量积

张量积推广到矩阵，即所谓Kronecker product。

>注：Leopold Kronecker，1823～1891，德国数学家。柏林大学博士和教授。导师Dirichlet。他最牛逼的地方是，当Riemann去世的时候，拒绝了哥廷根大学的offer。而这个位置的前任分别是：Carl Gauss、Dirichlet、Riemann。

$$\mathbf{A}\otimes\mathbf{B} = \begin{bmatrix} a_{11} \mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ \vdots & \ddots & \vdots \\ a_{m1} \mathbf{B} & \cdots & a_{mn} \mathbf{B} \end{bmatrix}$$

## Hadamard product

又叫Schur product或entrywise product。

>注：Jacques Salomon Hadamard，1865～1963，法国数学家。巴黎高等师范学校博士，波尔多大学教授。他曾于1936年访华，执教于清华大学。中国偏微分方程研究事业的主要创始人之一——吴新谋教授，就是他的学生。

$$\left(\begin{array}{ccc}
    \mathrm{a}_{11} & \mathrm{a}_{12} & \mathrm{a}_{13}\\
    \mathrm{a}_{21} & \mathrm{a}_{22} & \mathrm{a}_{23}\\
    \mathrm{a}_{31} & \mathrm{a}_{32} & \mathrm{a}_{33}
  \end{array}\right) \circ \left(\begin{array}{ccc}
    \mathrm{b}_{11} & \mathrm{b}_{12} & \mathrm{b}_{13}\\
    \mathrm{b}_{21} & \mathrm{b}_{22} & \mathrm{b}_{23}\\
    \mathrm{b}_{31} & \mathrm{b}_{32} & \mathrm{b}_{33}
  \end{array}\right) = \left(\begin{array}{ccc}
    \mathrm{a}_{11}\, \mathrm{b}_{11} & \mathrm{a}_{12}\, \mathrm{b}_{12} & \mathrm{a}_{13}\, \mathrm{b}_{13}\\
    \mathrm{a}_{21}\, \mathrm{b}_{21} & \mathrm{a}_{22}\, \mathrm{b}_{22} & \mathrm{a}_{23}\, \mathrm{b}_{23}\\
    \mathrm{a}_{31}\, \mathrm{b}_{31} & \mathrm{a}_{32}\, \mathrm{b}_{32} & \mathrm{a}_{33}\, \mathrm{b}_{33}
  \end{array}\right)$$

类似Hadamard product以及矩阵加法之类的操作，又被称为element-wise op或coefficient-wise op。

参考：

https://mp.weixin.qq.com/s/K_aNhzaxmchynDWTE1QFCQ

给你一些点与线，只用动画就能看懂张量乘法，还能证明迹循环定理

## 张量分析

在同构的意义下，第零阶张量（r = 0）为标量（Scalar），第一阶张量（r = 1）为向量（Vector），第二阶张量（r = 2）则成为矩阵（Matrix）。

《张量分析》，黄克智著。

>注：黄克智，1927年生，固体力学家。江西中正大学本科+清华硕士+莫斯科大学博士（因应召回国，放弃博士学位）。清华大学工程力学系教授、工程力学研究所所长，中国科学院院士。断裂力学领域权威。

参考：

https://www.zhihu.com/question/286175595

tensor contraction是什么?和内积有关系吗？

https://www.cnblogs.com/lywangjapan/p/12233140.html

张量网络学习笔记

https://www.cnblogs.com/lywangjapan/p/12041658.html

张量分解与应用-学习笔记（1）

https://www.cnblogs.com/lywangjapan/p/12068703.html

张量分解与应用-学习笔记（2）

https://www.cnblogs.com/lywangjapan/p/12089285.html

张量分解与应用-学习笔记（3）

<a name="Boltzmann"/>

# 玻尔兹曼分布

Boltzmann distribution（又叫Gibbs distribution）本来是脱胎于热力学和统计力学的分布，但在量子力学和机器学习领域也得到了广泛的应用。

>注：Ludwig Eduard Boltzmann，1844～1906，奥地利物理学家，统计力学的奠基人之一。维也纳大学博士，先后执教于格拉茨大学、维也纳大学、慕尼黑大学和莱比锡大学。英国皇家学会会员。

## 信息熵

设概率实验X有n个可能的独立结局，即n个随机事件$$A_1,\dots,A_n$$，每个随机事件的概率为$$p_1,\dots,p_n$$。则信息熵的定义为：

$$H_n=-k\sum_{i=1}^{n}p_i\ln p_i\tag{1}$$

其中k为一正常数。可以看出$$H_n$$实际上是$$p_i$$的泛函。

以下我们主要从信息熵的角度出发研究玻尔兹曼分布。但不能忽视的是，玻尔兹曼分布最早脱胎于热力学，而信息熵与热力学统计物理中的熵，虽然数学形式一致，但本质是不同的。两者相差一个玻尔兹曼常数。参见：

https://www.zhihu.com/answer/50458123

信息熵与热力学统计物理中的熵有什么区别和联系？

>注：信息熵的定义有多种，公式1给出的是Shannon熵的定义，除此之外还有von Neumann熵、Renyi熵等。以下如无特殊指出，信息熵均指Shannon熵。

参考：

https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw

机器学习各种熵：从入门到全面掌握

https://mp.weixin.qq.com/s/6I2-5zWbpAUSgoHRUEkj3Q

通俗理解信息熵

## 最大信息熵原理

事实上，有些随机事件其概率往往不可能直接计算，平常我们对具体问题作出处理时，掌握的仅仅是一些与随机事件有关的随机变量的统计平均值，以及某些其他制约条件。而当一个随机变量的平均值给定时，还可以有多种概率分布与之相容。现在的问题是如何从这些相容的概率分布中挑选出“最可几”的分布来作为实际上的分布。显然，要做到这点，必须有个挑选标准，最大信息熵原理就可作为这种挑选标准。

>注：最可几分布（Most Probable Distribution），又称最概然分布。它表征在特定系统的特定状态下，出现概率最大的分布。从它的定义可以看出，它实际上并不是某种具体的分布，而更像是符合某一特定目标的分布。   
>比如，麦克斯韦-玻尔兹曼分布、费米-狄拉克分布和玻色-爱因斯坦分布，是热力学常见的三种最可几分布。但它们所处的系统，以及系统的状态各不相同。

>基本粒子中所有的物质粒子都是费米子，是构成物质的原材料(如轻子中的电子、组成质子和中子的夸克、中微子)；而传递作用力的粒子(光子、介子、胶子、W和Z玻色子)都是玻色子。

最大熵原理是1957年由E.T.Jaynes提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。其实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。

>注：Edwin Thompson Jaynes，1922～1998，美国统计学家。普林斯顿大学博士，华盛顿大学教授。

假定系统涉及的M个随机变量$$f_i^{(j)}(j=1,\dots,M)$$的期望值$$F_j$$已知，即：

$$F_j=\sum_{i=1}^{n}f_i^{(j)}p_i\tag{2}$$

其中$$f_i^{(j)}$$表示第j个随机变量在状态i中的取值。$$p_i$$满足归一化条件：

$$\sum_{i=1}^{n}p_i=1\tag{3}$$

将公式2和公式3所述的两个约束，代入公式1，可得如下Lagrange乘子：

$$\begin{array}\\
J(p_i)=H_n/k-\alpha(\sum_{i=1}^{n}p_i-1)-\sum_{j=1}^{M}\beta_j(\sum_{i=1}^{n}f_i^{(j)}p_i-F_j)\\
=-\sum_{i=1}^{n}p_i\ln p_i-\alpha\sum_{i=1}^{n}p_i+\alpha-\sum_{j=1}^{M}\sum_{i=1}^{n}\beta_jf_i^{(j)}p_i+\sum_{j=1}^{M}\beta_jF_j
\end{array}
$$

对$$p_i$$求导，可得：

$$\frac{\partial J(p_i)}{\partial p_i}=-\ln p_i-1-\alpha-\sum_{j=1}^{M}\beta_jf_i^{(j)}$$

令上式等于0，可得：

$$p_i=\exp(-1-\alpha-\sum_{j=1}^{M}\beta_jf_i^{(j)})\tag{4}$$

公式4就是一般情况下的最大信息熵公式。

## 相关名词辨析

这里有三个非常容易混淆的名词需要解释。

**Maxwell–Boltzmann statistics**是研究热平衡状态下无相互作用的物体粒子的能量均值分布的统计学分支。

**Boltzmann distribution**表征一个包含各种能量状态的系统中粒子的概率分布。

**Maxwell-Boltzmann distribution**表征粒子的能级和速度的分布。

简而言之，Maxwell–Boltzmann statistics是基于特定假设的统计学分支。而后两者则是具体情况下的概率分布。

与Maxwell–Boltzmann statistics相对应的概念是Bose–Einstein statistics和Fermi–Dirac statistics。后两者分别对应玻色子和费米子的热力学统计。

参考：

https://en.wikipedia.org/wiki/Maxwell–Boltzmann_statistics

https://en.wikipedia.org/wiki/Boltzmann_distribution

https://en.wikipedia.org/wiki/Maxwell–Boltzmann_distribution

## 玻尔兹曼分布的推导

Maxwell–Boltzmann statistics研究的对象是一个由N（N很大）个微观粒子组成的近独立体系，体系的总能量E等于各个粒子能量之和。通常微观粒子的状态和能量是量子化的（即所谓能级），每一能级上又具有若干个量子态。为此，可引入下列符号：

| 名称 | 符号 |
|:--:|:--:|
| 单粒子状态编号 | $$1,\dots,n$$ |
| 能级 | $$E_1,\dots,E_n$$ |
| 量子态数 | $$g_1,\dots,g_n$$ |
| 微观粒子数 | $$N_1,\dots,N_n$$ |

令$$m=g_1+\dots+g_n$$，$$p_s$$为任何一个确定的微观粒子处于第s个量子态上的概率。因此，相应的信息熵和概率分布的归一化条件为：

$$H_m=-k\sum_{s=1}^{m}p_s\ln p_s\tag{5}$$

$$\sum_{s=1}^{m}p_s=1\tag{6}$$

由于N是个大数，可以认为第s个量子态上的微观粒子数为$$N_s=Np_s$$，体系的总能量为：

$$E=\sum_{s=1}^{m}E_sN_s=\sum_{s=1}^{m}E_sNp_s\tag{7}$$

和公式4的推导过程类似，Maxwell–Boltzmann statistics条件下的信息熵最大值的概率分布为：

$$p_s=\exp(-1-\alpha-\beta E_s)\tag{8}$$

由$$p_s$$的归一化条件可得：

$$p_s=\frac{1}{Z}e^{-\beta E_s}\tag{9}$$

其中：

$$Z=\sum_{s=1}^{m}e^{-\beta E_s}$$

于是，当体系处于热平衡状态时，第s个量子态上的粒子数可表为：

$$N_s=Np_s=\frac{N}{Z}e^{-\beta E_s}\tag{10}$$

考虑到同一单粒子能级$$E_i$$上的$$g_i$$个量子态均具有相同的能量$$E_i$$，因此：

$$N_i=N_sg_i\tag{11}$$

由气体分子的速度分布可得：

$$\beta=\frac{1}{kT}\tag{12}$$

其中，k是Boltzmann常数，T是热力学温度。

将公式11、12代入公式10可得：

$$N_i=\frac{Ng_ie^{-E_i/kT}}{\sum_{i=1}^{n}g_ie^{-E_i/kT}}$$

上式就是Boltzmann distribution的PDF。

## 参考

http://wenku.baidu.com/view/ccb7956db84ae45c3b358ca7.html

波尔兹曼统计分布律的推导的新方法

http://www.cnblogs.com/tianchi/archive/2013/03/14/2959716.html

Boltzmann machine

https://mp.weixin.qq.com/s/Z1_ujG7iP_AFUD0gVO8HXQ

信息熵、交叉熵和相对熵

https://mp.weixin.qq.com/s/2_ZkKrwPhaS31JnyYXyfrQ

简单的交叉熵损失函数，你真的懂了吗？

http://blog.csdn.net/shijing_0214/article/details/51116046

理解最大熵模型
