---
layout: post
title:  数学狂想曲（五）——玻尔兹曼分布, 概率分布（2）
category: math 
---

# 玻尔兹曼分布

Boltzmann distribution（又叫Gibbs distribution）本来是脱胎于热力学和统计力学的分布，但在量子力学和机器学习领域也得到了广泛的应用。

>注：Ludwig Eduard Boltzmann，1844～1906，奥地利物理学家，统计力学的奠基人之一。维也纳大学博士，先后执教于格拉茨大学、维也纳大学、慕尼黑大学和莱比锡大学。英国皇家学会会员。

## 信息熵

设概率实验X有n个可能的独立结局，即n个随机事件$$A_1,\dots,A_n$$，每个随机事件的概率为$$p_1,\dots,p_n$$。则信息熵的定义为：

$$H_n=-k\sum_{i=1}^{n}p_i\ln p_i\tag{1}$$

其中k为一正常数。可以看出$$H_n$$实际上是$$p_i$$的泛函。

以下我们主要从信息熵的角度出发研究玻尔兹曼分布。但不能忽视的是，玻尔兹曼分布最早脱胎于热力学，而信息熵与热力学统计物理中的熵，虽然数学形式一致，但本质是不同的。两者相差一个玻尔兹曼常数。参见：

https://www.zhihu.com/question/20992022/answer/50458123

信息熵与热力学统计物理中的熵有什么区别和联系？

>注：信息熵的定义有多种，公式1给出的是Shannon熵的定义，除此之外还有von Neumann熵、Renyi熵等。以下如无特殊指出，信息熵均指Shannon熵。

参考：

https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw

机器学习各种熵：从入门到全面掌握

https://mp.weixin.qq.com/s/6I2-5zWbpAUSgoHRUEkj3Q

通俗理解信息熵

## 最大信息熵原理

事实上，有些随机事件其概率往往不可能直接计算，平常我们对具体问题作出处理时，掌握的仅仅是一些与随机事件有关的随机变量的统计平均值，以及某些其他制约条件。而当一个随机变量的平均值给定时，还可以有多种概率分布与之相容。现在的问题是如何从这些相容的概率分布中挑选出“最可几”的分布来作为实际上的分布。显然，要做到这点，必须有个挑选标准，最大信息熵原理就可作为这种挑选标准。

>注：最可几分布（Most Probable Distribution），又称最概然分布。它表征在特定系统的特定状态下，出现概率最大的分布。从它的定义可以看出，它实际上并不是某种具体的分布，而更像是符合某一特定目标的分布。   
>比如，麦克斯韦-玻尔兹曼分布、费米-狄拉克分布和玻色-爱因斯坦分布，是热力学常见的三种最可几分布。但它们所处的系统，以及系统的状态各不相同。

>基本粒子中所有的物质粒子都是费米子，是构成物质的原材料(如轻子中的电子、组成质子和中子的夸克、中微子)；而传递作用力的粒子(光子、介子、胶子、W和Z玻色子)都是玻色子。

最大熵原理是1957年由E.T.Jaynes提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。其实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。

>注：Edwin Thompson Jaynes，1922～1998，美国统计学家。普林斯顿大学博士，华盛顿大学教授。

假定系统涉及的M个随机变量$$f_i^{(j)}(j=1,\dots,M)$$的期望值$$F_j$$已知，即：

$$F_j=\sum_{i=1}^{n}f_i^{(j)}p_i\tag{2}$$

其中$$f_i^{(j)}$$表示第j个随机变量在状态i中的取值。$$p_i$$满足归一化条件：

$$\sum_{i=1}^{n}p_i=1\tag{3}$$

将公式2和公式3所述的两个约束，代入公式1，可得如下Lagrange乘子：

$$J(p_i)=H_n/k-\alpha(\sum_{i=1}^{n}p_i-1)-\sum_{j=1}^{M}\beta_j(\sum_{i=1}^{n}f_i^{(j)}p_i-F_j)
\\=-\sum_{i=1}^{n}p_i\ln p_i-\alpha\sum_{i=1}^{n}p_i+\alpha-\sum_{j=1}^{M}\sum_{i=1}^{n}\beta_jf_i^{(j)}p_i+\sum_{j=1}^{M}\beta_jF_j$$

对$$p_i$$求导，可得：

$$\frac{\partial J(p_i)}{\partial p_i}=-\ln p_i-1-\alpha-\sum_{j=1}^{M}\beta_jf_i^{(j)}$$

令上式等于0，可得：

$$p_i=\exp(-1-\alpha-\sum_{j=1}^{M}\beta_jf_i^{(j)})\tag{4}$$

公式4就是一般情况下的最大信息熵公式。

## 相关名词辨析

这里有三个非常容易混淆的名词需要解释。

**Maxwell–Boltzmann statistics**是研究热平衡状态下无相互作用的物体粒子的能量均值分布的统计学分支。

**Boltzmann distribution**表征一个包含各种能量状态的系统中粒子的概率分布。

**Maxwell-Boltzmann distribution**表征粒子的能级和速度的分布。

简而言之，Maxwell–Boltzmann statistics是基于特定假设的统计学分支。而后两者则是具体情况下的概率分布。

与Maxwell–Boltzmann statistics相对应的概念是Bose–Einstein statistics和Fermi–Dirac statistics。后两者分别对应玻色子和费米子的热力学统计。

参考：

https://en.wikipedia.org/wiki/Maxwell–Boltzmann_statistics

https://en.wikipedia.org/wiki/Boltzmann_distribution

https://en.wikipedia.org/wiki/Maxwell–Boltzmann_distribution

## 玻尔兹曼分布的推导

Maxwell–Boltzmann statistics研究的对象是一个由N（N很大）个微观粒子组成的近独立体系，体系的总能量E等于各个粒子能量之和。通常微观粒子的状态和能量是量子化的（即所谓能级），每一能级上又具有若干个量子态。为此，可引入下列符号：

| 名称 | 符号 |
|:--:|:--:|
| 单粒子状态编号 | $$1,\dots,n$$ |
| 能级 | $$E_1,\dots,E_n$$ |
| 量子态数 | $$g_1,\dots,g_n$$ |
| 微观粒子数 | $$N_1,\dots,N_n$$ |

令$$m=g_1+\dots+g_n$$，$$p_s$$为任何一个确定的微观粒子处于第s个量子态上的概率。因此，相应的信息熵和概率分布的归一化条件为：

$$H_m=-k\sum_{s=1}^{m}p_s\ln p_s\tag{5}$$

$$\sum_{s=1}^{m}p_s=1\tag{6}$$

由于N是个大数，可以认为第s个量子态上的微观粒子数为$$N_s=Np_s$$，体系的总能量为：

$$E=\sum_{s=1}^{m}E_sN_s=\sum_{s=1}^{m}E_sNp_s\tag{7}$$

和公式4的推导过程类似，Maxwell–Boltzmann statistics条件下的信息熵最大值的概率分布为：

$$p_s=\exp(-1-\alpha-\beta E_s)\tag{8}$$

由$$p_s$$的归一化条件可得：

$$p_s=\frac{1}{Z}e^{-\beta E_s}\tag{9}$$

其中：

$$Z=\sum_{s=1}^{m}e^{-\beta E_s}$$

于是，当体系处于热平衡状态时，第s个量子态上的粒子数可表为：

$$N_s=Np_s=\frac{N}{Z}e^{-\beta E_s}\tag{10}$$

考虑到同一单粒子能级$$E_i$$上的$$g_i$$个量子态均具有相同的能量$$E_i$$，因此：

$$N_i=N_sg_i\tag{11}$$

由气体分子的速度分布可得：

$$\beta=\frac{1}{kT}\tag{12}$$

其中，k是Boltzmann常数，T是热力学温度。

将公式11、12代入公式10可得：

$$N_i=\frac{Ng_ie^{-E_i/kT}}{\sum_{i=1}^{n}g_ie^{-E_i/kT}}$$

上式就是Boltzmann distribution的PDF。

## 参考

http://wenku.baidu.com/view/ccb7956db84ae45c3b358ca7.html

波尔兹曼统计分布律的推导的新方法

http://www.cnblogs.com/tianchi/archive/2013/03/14/2959716.html

Boltzmann machine

https://mp.weixin.qq.com/s/Z1_ujG7iP_AFUD0gVO8HXQ

信息熵、交叉熵和相对熵

https://mp.weixin.qq.com/s/2_ZkKrwPhaS31JnyYXyfrQ

简单的交叉熵损失函数，你真的懂了吗？

http://blog.csdn.net/shijing_0214/article/details/51116046

理解最大熵模型

https://mp.weixin.qq.com/s/yUJDMSU1Fmf36vN2Q2y1TA

最大熵模型原理小结

https://mp.weixin.qq.com/s/a3TlUFwSm9aIptfhy6MnGA

机器学习是如何借鉴物理学思想的？从伊辛模型谈起

https://zhuanlan.zhihu.com/p/39241895

深度学习与物理，有效场论，重整化群

https://mp.weixin.qq.com/s/7NrB0UtmELXD3UNO3C6jGA

我就不信看完这篇你还搞不懂信息熵

https://mp.weixin.qq.com/s/_s8kxlzcL5Nt2ILUIfAuNQ

不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型

# 概率分布（2）

上一篇《概率分布（1）》写的意犹未尽，这里继续写。本篇主要关注$$\chi^2$$分布、t分布和F分布，也就是统计学的三大祖师爷各自的看家本领。

## $$\chi^2$$分布

设$$X_1,\dots,X_n$$是来自总体$$N(0,1)$$的样本，则称统计量

$$\chi^2=X_1^2+\dots+X_n^2\tag{1}$$

服从自由度为n的$$\chi^2$$分布（chi-squared distribution），记作$$\chi^2\sim \chi^2(n)$$。其PDF为：

$$f(x;\,n) =
\begin{cases}
  \dfrac{x^{(n/2-1)} e^{-x/2}}{2^{n/2} \Gamma\left(\frac n 2 \right)},  & x > 0; \\ 0, & \text{otherwise}.
\end{cases}$$

## t分布

设$$X\sim N(0,1),Y\sim\chi^2(n)$$，并且X、Y独立，则称随机变量

$$t=\frac{X}{\sqrt{Y/n}}\tag{2}$$

服从自由度为n的t分布（t distribution），记作$$t\sim t(n)$$。其PDF为：

$$f(t) = \frac{\Gamma(\frac{n+1}{2})} {\sqrt{n\pi}\,\Gamma(\frac{n}{2})} \left(1+\frac{t^2}{n} \right)^{\!-\frac{n+1}{2}}$$

## F分布

设$$U\sim \chi^2(d_1),V\sim\chi^2(d_2)$$，并且U、V独立，则称随机变量

$$F=\frac{U/d_1}{V/d_2}\tag{3}$$

服从自由度为$$(d_1,d_2)$$的F分布（F distribution），记作$$F\sim F(d_1,d_2)$$。其PDF为：

$$\begin{align}
f(x; d_1,d_2) &= \frac{\sqrt{\frac{(d_1\,x)^{d_1}\,\,d_2^{d_2}} {(d_1\,x+d_2)^{d_1+d_2}}}} {x\,\mathrm{B}\!\left(\frac{d_1}{2},\frac{d_2}{2}\right)} \\
&=\frac{1}{\mathrm{B}\!\left(\frac{d_1}{2},\frac{d_2}{2}\right)} \left(\frac{d_1}{d_2}\right)^{\frac{d_1}{2}} x^{\frac{d_1}{2} - 1} \left(1+\frac{d_1}{d_2}\,x\right)^{-\frac{d_1+d_2}{2}}
\end{align}$$

显然：

$$\frac{1}{F}\sim F(d_2,d_1)$$

参考：

https://mp.weixin.qq.com/s/X0nSqiQoJpEur7PM1tM-0A

每个数据科学专家都应该知道的六个概率分布

https://www.zhihu.com/question/26991510

卡方独立性检验和卡方齐性检验的区别是什么？

## 假设检验

假设检验就是根据样本对所提出的假设$$H_0$$作判断。

如果$$P\{拒绝H_0\mid H_0为真\}\le \alpha$$，则接受$$H_0$$。

这里的$$\alpha$$被称作**显著性水平**。假设检验$$H_0$$所涉及的统计量被称作**检验统计量**。

>在英国的Rothamsted实验站，Fisher给一位名叫Muriel Bristol的女士倒了一杯茶，但是Bristol表示，自己更喜欢先将牛奶倒入杯中，再倒入茶。这位女士号称能够分辨先倒茶和先倒牛奶的区别。   
>作为实验设计的鼻祖，Fisher当然想用实验检验一下：这位女士的味觉是否有这么敏锐？Fisher倒了8杯奶茶：其中4杯“先奶后茶”，其余4杯“先茶后奶”。随机打乱次序后，Fisher请Bristol品尝，并选出“先奶后茶”的4杯，看她是否能分辨奶和茶的顺序。   
>Bristol完全答对了。这是一个小概率事件，概率小于0.05（通常的统计显著性水平）。所以，她“没有任何分辨能力”这个假设就和数据不太相容，可以拒绝这个假设。
