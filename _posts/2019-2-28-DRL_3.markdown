---
layout: post
title:  深度强化学习（三）——DDPG, AlphaGo
category: RL 
---

# DQN进化史

## NAF（续）

这个时候，我们有两种选择：

- 弄两个神经网络，一个是Policy网络，输入状态，输出动作，另一个是Q网络，输入状态，输出Q值。这种做法其实就是Actor-Critic算法。

- 只弄一个神经网络，既能输出动作也能输出Q值。这就是本节所要使用的方法。

在《强化学习（八）》中，我们已经指出advantage function不改变策略梯度，但能有效约束variance。这里沿用了A函数定义：

$$A(s,a)=Q(s,a)-V(s)$$

作者设计了如下A函数：

$$A(x,\mu | \theta^A)=-\frac{1}{2}(u-\mu(x | \theta^\mu))^T P(x | \theta^P)(u-\mu(x | \theta^\mu))$$

其中x是状态，u是动作，$$\mu$$是神经网络的输出动作。

它的一阶矩阵形式等价于：$$A(a)=-P(a-x)^2$$

因此，这是一个正定二次型的相反数，即$$A\le 0$$。

我们的目标就是优化A函数，使之尽可能最大（即等于0）。这时A函数对应的动作即为最优动作。

以下是具体的网络结构图。

![](/images/img3/NAF.jpg)

其流程如下：

- 状态经过两层MLP之后（注意：第2层没有RELU），生成三个tensor：V、$$\mu$$、L0。

- 将L0转化为L。也就是将一个列向量转换为下三角矩阵，就是从新排列，然后把对角线的数exp对数化。

- 使用Cholesky分解构建P。

>若A为n阶对称正定矩阵，则存在唯一的主对角线元素都是正数的下三角阵L，使得$$A=LL^T$$，此分解式称为**正定矩阵的乔列斯基（Cholesky）分解**。

- 按照上图的步骤，依此构建A和Q。其中的Q正好可用于DQN的训练。

综上，NAF既可以输出action，也可产生Q值，而Q值可用于DQN训练，从而使算法可以进行下去。

>这里的构造逻辑和VAE很相似，都是假定构造方法成立，然后再用神经网络拟合构造所需的各要素，最后通过训练以达成构造的效果。

## Rainbow

论文：

《Rainbow: Combining Improvements in Deep Reinforcement Learning》

Rainbow算是2017年比较火的一篇DRL方面的论文了。它没有提出新方法，而只是整合了6种DQN算法的变种，达到了SOTA的效果。

这6种DQN算法是：

- DoubleDQN
- PrioritizedExperienceReplay
- DuelingNet
- NoisyNet
- CategoricalDQN
- N-stepLearning

参考：

https://mp.weixin.qq.com/s/SZHMyWOXHM8T3zp_aUt-6A

DeepMind提出Rainbow：整合DQN算法中的六种变体

https://github.com/Curt-Park/rainbow-is-all-you-need

手把手教你从DQN到Rainbow

# DDPG

论文：

《Continuous control with deep reinforcement learning》

DDPG主要从：**PG->DPG->DDPG**发展而来。

Policy Gradient的概念参见《强化学习（七）》，这里不再赘述。

Deterministic Policy Gradient是Deepmind的D.Silver等在2014年提出的，即确定性的行为策略，每一步的行为通过函数$$\mu$$直接获得确定的值：

$$a_{t} = \mu(s_{t} | \theta^{\mu})$$

为何需要确定性的策略？简单来说，PG方法有以下缺陷：

- 即使通过PG学习得到了随机策略之后，在每一步行为时，我们还需要对得到的最优策略概率分布进行采样，才能获得action的具体值；而action通常是高维的向量，比如25维、50维，在高维的action空间的频繁采样，无疑是很耗费计算能力的；

- 在PG的学习过程中，每一步计算policy gradient都需要在整个action space进行积分：

$$\triangledown_{\theta} =  \int_{\mathcal{S}} \int_{A} \rho(s) \pi_{\theta}(a|s)Q^{\pi} (s,a)\mathrm{d}a \mathrm{d}s$$

参考：

https://mp.weixin.qq.com/s/dgLJrn3omUKMqmqTIEcoyg

Tensorflow实现DDPG

https://github.com/jinfagang/rl_atari_pytorch

ReinforcementLearning Learn Play Atari Using DDPG and LSTM.

https://zhuanlan.zhihu.com/p/65931777

强化学习-基于pytorch的DDPG实现

https://mp.weixin.qq.com/s/p2jF2Awmgeem-XGCkix-Lg

深度确定性策略梯度DDPG详解

https://mp.weixin.qq.com/s/_dskX5U8gHAEl6aToBvQvg

从Q学习到DDPG，一文简述多种强化学习算法

https://www.zhihu.com/question/323420831

强化学习中A3C/DDPG/DPPO哪个效果更好？

https://blog.csdn.net/gsww404/article/details/80403150

从确定性策略（DPG）到深度确定性策略梯度(DDPG)算法的原理讲解及tensorflow代码实现

https://blog.csdn.net/qq_39388410/article/details/88828548

强化学习（DDPG，AC3，DPPO）

https://blog.csdn.net/qq_30615903/article/details/80776715

DDPG(Deep Deterministic Policy Gradient)算法详解

https://blog.csdn.net/kenneth_yu/article/details/78478356

DDPG原理和算法

# reward modeling

![](/images/img2/reward_modeling.jpg)

训练一个奖励模型，其中包含来自用户的反馈，从而捕捉他们的意图。与此同时，我们通过强化学习训练一个策略，使奖励模型的奖励最大化。换句话说，我们把学习做什么(奖励模型)和学习怎么做(策略)区分开来。

参考：

https://mp.weixin.qq.com/s/4yGQtHtMqWlaB7MAsr8T_g

DeepMind重磅论文：通过奖励模型，让AI按照人类意图行事

https://mp.weixin.qq.com/s/TIWnnCmVZnFQNH9Fig5aTw

DeepMind发布新奖励机制：让智能体不再“碰瓷”

# AlphaGo

樊麾讲解AlphaGo与李世石的五番棋：

https://deepmind.com/research/alphago/alphago-games-simplified-chinese/

论文：

《Mastering the game of Go with deep neural networks and tree search》

2017.1

岁首，业界最重要的事情莫过于AlphaGo以Master之名再度出山，60场快棋未遇敌手。牛逼！

http://goeye-app.com/resources.html#cn

精选在线棋谱资源

## DarkForest

DarkForest是田渊栋2015年11月的作品，虽然棋力和稍后的AlphaGo相去甚远，但毕竟也算是用到了RL和DNN了。

代码：

https://github.com/facebookresearch/darkforestGo

## Leela Zero

Leela Zero是比利时人Gian-Carlo Pascutto开源的围棋AI。它的算法与AlphaGo Zero相同。而训练采用GTP协议，集合全球算力，进行分布式训练。

官网：

http://zero.sjeng.org/

代码：

https://github.com/gcp/leela-zero

>十多年前，当我还是一个中二青年的时候，就幻想有朝一日能够拿围棋世界冠军。当然，就算再中二，我自己也明白靠实力那是不可能的，当时做梦的法宝是制造一个AI，然后碾压一下所谓的国手。   
>按照当时(2000年前后)人们的预计，这个AI在2030年之前，都不可能造出来，然而，最终的结果实际上只花了一半左右的时间。   
>再之后，随着AI围棋的平民化，我的中二梦终于也有人将之付诸实现了：   
>https://mp.weixin.qq.com/s/npt2zZrKwPnNdY-hsa2RjQ   
>AI再乱围棋圈：“食言之战”柯洁落败；首例素人作弊引风波

这次作弊风波所使用的AI就是Leela Zero，可见目前（2018.5）它的棋力已经超过了顶尖棋手。

## ELF OpenGo

ELF OpenGo是Facebook开源的围棋AI，它是FB的AI游戏框架ELF的一部分。

官网：

https://github.com/pytorch/ELF

参考：

https://mp.weixin.qq.com/s/lOAx3suLIS-pEWyi8xZl6Q

“全民体验”AlphaZero：FAIR田渊栋首次开源超级围棋AI

## PhoenixGo

PhoenixGo是腾讯微信团队的AlphaGo Zero复刻版。

官网：

https://github.com/Tencent/PhoenixGo

参考：

https://mp.weixin.qq.com/s/tJDmxsuS1QigYS75ZIdzRA

微信团队开源围棋AI技术PhoenixGo，复现AlphaGo Zero论文

## 参考

https://mp.weixin.qq.com/s/Sfv-jzQAkN0PsZOGZUQhkQ

AlphaGo Zero横空出世，DeepMind Nature论文解密不使用人类知识掌握围棋

https://mp.weixin.qq.com/s/oAxouYX7-wDC5okbu--Wuw

Nature重磅：人工智能从0到1, 无师自通完爆阿法狗100-0

https://zhuanlan.zhihu.com/p/30262872

关于AlphaGo Zero

https://zhuanlan.zhihu.com/p/30263585

DeepMind新一代围棋程序AlphaGo Zero再次登上Nature

https://www.zhihu.com/question/66861459

如何评价DeepMind发表在Nature上的AlphaGo Zero？

http://www.alphago-games.com/

AlphaGo的棋谱

https://deepmind.com/blog/alphago-zero-learning-scratch/

AlphaGo Zero官方声明

https://zhuanlan.zhihu.com/mathNote

某牛的专栏，主要讲自制AlphaGo

https://mp.weixin.qq.com/s/DC9QqHdWT0xFnowEBuJDbw

自动化所解读“深度强化学习”：从AlphaGo到AlphaGoZero

https://mp.weixin.qq.com/s/uZtaxRwROCqYmL2k6Muxaw

从阿尔法狗元(AlphaGo Zero)的诞生看终极算法的可能性
