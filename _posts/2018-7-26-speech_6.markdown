---
layout: post
title:  语音识别（六）——语音识别的评价指标, 声学模型进阶, 语言模型进阶, GMM-HMM, WFST（1）
category: speech 
---

# 语音识别的评价指标

语音识别的评价指标主要是Word Error Rate（WER）。

错误的情况包括三种：

1.Substitutions：错词。

2.Deletions：漏词。

3.Insertions：多词。

$$WER=100\%\times \frac{Subs+Dels+Ins}{\text{word in correct sentence}}$$

类似的还有CER/PER：Character/Phoneme Error Rate。

需要注意的是，评价WER时，需要在ASR output和Label之间进行对齐操作，而不是简单的从左往右匹配，否则将无法正确处理Deletions和Insertions的情形。

还有根据公式可知，WER是可以大于100%的。

参考：

https://blog.csdn.net/quhediegooo/article/details/56834417

语音识别评估标准-WER

# 声学模型进阶

## 语音质量

更高的采样率可以降低WER。一般来说，16KHz相比8KHz的WER要小10%左右。

## Voice Detection

长时间的silence会增加WER，因此我们需要判断当前是否在说话。

Voice Detection包括两个方面：

1.Beginning-Point Detection。也叫做Voice Activity Detection（VAD）或者Acoustic event detection (AED)。有些类似于唤醒检测，但并不局限于设备的开机时刻。

2.End-Point Detection。

参考：

https://zhuanlan.zhihu.com/p/24432663

Voice Active Detection(VAD)的过去时与现在时

https://blog.csdn.net/wxb1553725576/article/details/78069089

Kaldi特征提取之-VAD

https://blog.csdn.net/shichaog/article/details/78257068

VAD综述

## Feature normalization

有时候需要对Feature进行normalization。例如，对MFCC特征减去均值，可以有效提升在噪声环境下的识别率。

## Tri-phone Models

英语一般包含43个音素，因此Tri-phone共有$$43^3\approx 80K$$种不同组合。

但是这些组合的概率是众寡悬殊的，有些组合很常见，而有些组合很罕见。因此我们需要合并相似的发音组合。这通常采用CART决策树来进行聚类。这样做还可以减少状态数量，提高计算效率。

## 发音词典

Pronunciation Dict用于将文本转换成对应的发音。比较常用的有CMU的发音词典，用于美国英语，包含了100K的单词。用法参见《LSTM Speech Recognition实战》。

然而，无论多大的词典都会有遇到Unknown Words的情况。一般可根据现有发音构建统计模型，来预测发音。这也是符合人们的认知规律的：人遇到一个陌生的新词，也会根据过往的经验，来预测词的发音。通常这样做，会有70%～85%的准确率。

# 语言模型进阶

## 语言理论

语言理论方面主要是Chomsky提出的Chomsky hierarchy。

>Avram Noam Chomsky，1928年生，University of Pennsylvania本硕（1951）+Harvard University博士。MIT教授。美国科学院、美国艺术科学院院士。   
>他的《生成语法》被认为是20世纪理论语言学研究上最伟大的贡献，他也被誉为“现代语言学之父”。他对于形式语言的研究，在计算机科学的各领域产生了巨大的影响。这些领域包括：编译原理、AI等。

| 名称 | 概述 | 形式化 | 时间复杂度 | 空间复杂度 |
|:--:|:--:|:--:|:--:|:--:|
| **Finite State Machines** | 正则化的语言，无嵌套 | $$a^*b^*$$ | O(n) | O(1) |
| **Context Free Grammars** | 允许嵌套 | $$a^nb^n$$ | O(n^3) | 取决于嵌套的层数 |
| **Context Sensitive Grammars** | 有特定的语法规则 | $$a^nb^nc^n$$ | 多项式复杂度 |  |
| **Generalized Rewrite Rules/Turing machines** | 无任何限制 |  | NP complete |  |

从中可以看出，编程语言最多也就到Context Sensitive Grammars的程度。

## N-gram

可以用Entropy和Perplexity评价N-gram模型的效果。一般来说，Perplexity越低，识别效果越好。

## Recognition Network

![](/images/img2/Recognition_Network.png)

## JSGF

JSpeech Grammar Format是一个用于描述ASR grammar的规范。

官网：

https://www.w3.org/TR/jsgf/

# GMM-HMM

混合高斯模型是为了计算某个观察状态的mfcc分布和某个特定音子的mfcc之间的似然度的，但由于音子的mfcc分布并不是真的符合高斯分布，因此需要叠加多个高斯混合模型，来逼近某个特定音子的mfcc特征分布。我们叠加的模型数量越多，拟合的效果就越好。

![](/images/img2/GMM.png)

最简单的情况下，每个音子（例如ah这个音子）都需要至少一个39维的多维高斯模型进行建模，但是这样做的前提是，ah这个音的39维mfcc特征的分布符合高斯分布。

但事实上，ah并不完全符合高斯分布，于是我们就用一组（比如10个）高斯模型，混合在一起，去逼近这个音的mfcc特征分布。这样当有一帧通过计算得到一个39维的向量的时候，我们就可以把这个向量带入加权混合后的高斯模型中，计算这个向量属于ah这个音的概率。

以thchs为例，一共有3000个高斯核对音素进行建模，那是如何分配的呢？

如果是triphone模型，那么3000个高斯核每个都会分配一个三音子态。也就是说，每个核只用来建模一个三音子态。要确定这件事，只要看模型文件里有没有用来分配高斯混合模型权重的文件就可以了。

比如在sphinx这个库里面，模型文件中会有一个叫mixture_weight的文件，就是用来储存权重的，如果没有这个文件，意味着每个高斯核只用来建模一个音子态。

虽然音子只有60个，但是排列组合后的三音子态就多达几千个了。

参考：

https://blog.csdn.net/abcjennifer/article/details/27346787

GMM-HMM语音识别模型 原理篇

https://zhuanlan.zhihu.com/p/22482625

无痛理解GMM-HMM语音识别算法

https://mp.weixin.qq.com/s/Sn4RPdghzzQhQc-r4z8Iuw

Kaldi单音素GMM学习笔记

# LDA-MLLT

Maximum Likelihood Linear Transform (MLLT)，又名Global Semi-tied Covariance (STC)。因此，在科技文献中，常被称作STC/MLLT。

《Semi-tied Covariance Matrices for Hidden Markov Models》

《Improved feature processing for Deep Neural Networks》

![](/images/img2/MLLT.png)

https://blog.csdn.net/xmdxcsj/article/details/78512652

声学特征变换 STC/MLLT

# WFST

## 概述

Weighted Finite State Transducer是目前解码器模块的关键技术。

论文：

《Speech Recognition with Weighted Finite-State Transducers》

>Mehryar Mohri，法国人，Ecole Polytechnique本科+ENS Ulm硕士+University of Paris 7博士。New York University教授。Google研究顾问。   
>个人主页：   
>https://cs.nyu.edu/~mohri/

https://cs.nyu.edu/~mohri/courses.html

这是Mohri的课程主页。

https://cs.nyu.edu/~mohri/asr12/

作为WFST的发明人，Mohri的Speech Recognition课程在解码器方面有相当大的篇幅。

此外，OpenFst的官网也有一些教程：

http://openfst.org/twiki/bin/view/FST/FstBackground

OpenFst Background Material

其中，最重要的是两个tutorial：

http://openfst.org/twiki/bin/view/FST/FstHltTutorial

OpenFst: An Open-Source, Weighted Finite-State Transducer Library and its Applications to Speech and Language

http://openfst.org/twiki/bin/view/FST/FstSltTutorial

OpenFst: a General and Efficient Weighted Finite-State Transducer Library

以下内容主要参考下文：

http://vsooda.github.io/2016/08/28/wfst/

加权有限状态转换器

## FSM

![](/images/img2/FSM.png)

上图是finite-state machine（也叫finite-state automaton，FSA）的示意图。图中的Node表示State，顾名思义，FSM的State数量是有限的。图中的Edge表示Transition，Edge上的Label表示Input/Event。

FSM的含义是，在某一状态下，获得一个输入，从而产生一个状态转换。例如，上图中在Sleep状态下，如果输入是hungry的话，那么状态就会切换到Eat状态。当然了，输入也可以不改变状态，比如在Sleep状态下，输入是tired的时候。

## FST

![](/images/img2/FST.png)

上图是finite-state transducers的示意图。FST和FSM的差别主要在Edge上的Label。FST收到Input的时候，不仅会发生状态改变，还会产生Output序列。因此，其Label的格式为`input:output`。

## WFST

![](/images/img2/WFST.png)

上图是WFST的示意图。顾名思义，Label上不仅有Input、Output，还有Weight信息，其格式为`input:output/weight`。

在有些图中会碰到$$\epsilon$$. 这个符号在输入时表示不消耗任何输入，在输出位置表示不产生任何输出。

此外，还有格式为`input/weight`的FSM，一般被称为Weighted Finite-State Acceptors。

![](/images/img2/WA.png)

不仅Edge上的Label可以有权重，Node上的Label也可以有权重，如上图所示。没有权重的Label，其权重为1。

例如，上图中：

$$[A](ab)=1\times 1\times 2+2\times 3\times 2=14$$

## 相关的群论知识

WFST是基于半环代数理论的，在介绍半环之前我先简单的说一下群和半群。

**群（Group）**：G为非空集合，如果在G上定义的二元运算*，满足：

（1）封闭性（Closure）：对于任意$$a,b\in G$$,有$$a*b\in G$$;

（2）结合律（Associativity）：对于任意$$a,b,c\in G,(a*b)*c=a*(b*c)$$;

（3）幺元（Identity）：存在幺元e，使得对于任意$$a\in G,e*a=a*e=a$$;

（4）逆元：对于任意$$a\in G$$,存在逆元$$a^{-1}*a=a*a^{-1}=e$$。

则称（G,*）为群。

**半群（Semigroup）**：仅满足封闭性和结合律群称为半群；如果还包含幺元，则成为幺元半群。

**半环（semiring）**：指具有两个二元运算$$+$$和$$\cdot$$的非空集合S，且满足：

（1）$$(S,+),(S,\cdot)$$都是半群；

（2）$$\forall a,b,c\in S,(a+b)c = ac+bc, c(a+b) = ca+cb$$

半环的形式化表示如下：

$$(K, \bigoplus, \bigotimes，0， 1)$$

其中K是一个数集，$$\bigoplus, \bigotimes$$是两个二元操作，’0’和’1’是特定的（designated）零元素和幺元素（不一定是真正的数0和数1）。

常用的半环如下表所示：

| Semiring | Set | $$\oplus$$ | $$\otimes$$ | 0 | 1 |
|:--:|:--:|:--:|:--:|:--:|:--:|
| Boolean | $$\{0,1\}$$ | $$\lor$$ | $$\land$$ | 0 | 1 |
| Probability | $$R_+$$ | $$+$$ | $$\times$$ | 0 | 1 |
| Log | $$R\cup\{-\infty，+\infty\}$$ | $$\oplus_{log}$$ | + | $$+\infty$$ | 0 |
| Tropical | $$R\cup\{-\infty，+\infty\}$$ | min | + | $$+\infty$$ | 0 |
| String | $$\Sigma^*\cup\{\infty\}$$ | $$\land$$ | $$\cdot$$ | $$\infty$$ | $$\epsilon$$ |
