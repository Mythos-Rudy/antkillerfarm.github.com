---
layout: post
title:  语音识别（六）——语言模型进阶, GMM-HMM, WFST
category: speech 
---

# 声学模型进阶（续）

## 发音词典

Pronunciation Dict用于将文本转换成对应的发音。比较常用的有CMU的发音词典，用于美国英语，包含了100K的单词。用法参见《LSTM Speech Recognition实战》。

然而，无论多大的词典都会有遇到Unknown Words的情况。一般可根据现有发音构建统计模型，来预测发音。这也是符合人们的认知规律的：人遇到一个陌生的新词，也会根据过往的经验，来预测词的发音。通常这样做，会有70%～85%的准确率。

# 语言模型进阶

## 语言理论

语言理论方面主要是Chomsky提出的Chomsky hierarchy。

>Avram Noam Chomsky，1928年生，University of Pennsylvania本硕（1951）+Harvard University博士。MIT教授。美国科学院、美国艺术科学院院士。   
>他的《生成语法》被认为是20世纪理论语言学研究上最伟大的贡献，他也被誉为“现代语言学之父”。他对于形式语言的研究，在计算机科学的各领域产生了巨大的影响。这些领域包括：编译原理、AI等。

| 名称 | 概述 | 形式化 | 时间复杂度 | 空间复杂度 |
|:--:|:--:|:--:|:--:|:--:|
| **Finite State Machines** | 正则化的语言，无嵌套 | $$a^*b^*$$ | O(n) | O(1) |
| **Context Free Grammars** | 允许嵌套 | $$a^nb^n$$ | O(n^3) | 取决于嵌套的层数 |
| **Context Sensitive Grammars** | 有特定的语法规则 | $$a^nb^nc^n$$ | 多项式复杂度 |  |
| **Generalized Rewrite Rules/Turing machines** | 无任何限制 |  | NP complete |  |

从中可以看出，编程语言最多也就到Context Sensitive Grammars的程度。

## N-gram

可以用Entropy和Perplexity评价N-gram模型的效果。一般来说，Perplexity越低，识别效果越好。

## Recognition Network

![](/images/img2/Recognition_Network.png)

## JSGF

JSpeech Grammar Format是一个用于描述ASR grammar的规范。

官网：

https://www.w3.org/TR/jsgf/

# GMM-HMM

混合高斯模型是为了计算某个观察状态的mfcc分布和某个特定音子的mfcc之间的似然度的，但由于音子的mfcc分布并不是真的符合高斯分布，因此需要叠加多个高斯混合模型，来逼近某个特定音子的mfcc特征分布。我们叠加的模型数量越多，拟合的效果就越好。

![](/images/img2/GMM.png)

最简单的情况下，每个音子（例如ah这个音子）都需要至少一个39维的多维高斯模型进行建模，但是这样做的前提是，ah这个音的39维mfcc特征的分布符合高斯分布。

但事实上，ah并不完全符合高斯分布，于是我们就用一组（比如10个）高斯模型，混合在一起，去逼近这个音的mfcc特征分布。这样当有一帧通过计算得到一个39维的向量的时候，我们就可以把这个向量带入加权混合后的高斯模型中，计算这个向量属于ah这个音的概率。

以thchs为例，一共有3000个高斯核对音素进行建模，那是如何分配的呢？

如果是triphone模型，那么3000个高斯核每个都会分配一个三音子态。也就是说，每个核只用来建模一个三音子态。要确定这件事，只要看模型文件里有没有用来分配高斯混合模型权重的文件就可以了。

比如在sphinx这个库里面，模型文件中会有一个叫mixture_weight的文件，就是用来储存权重的，如果没有这个文件，意味着每个高斯核只用来建模一个音子态。

虽然音子只有60个，但是排列组合后的三音子态就多达几千个了。

参考：

https://blog.csdn.net/abcjennifer/article/details/27346787

GMM-HMM语音识别模型 原理篇

https://zhuanlan.zhihu.com/p/22482625

无痛理解GMM-HMM语音识别算法

https://mp.weixin.qq.com/s/Sn4RPdghzzQhQc-r4z8Iuw

Kaldi单音素GMM学习笔记

# LDA-MLLT

Maximum Likelihood Linear Transform (MLLT)，又名Global Semi-tied Covariance (STC)。因此，在科技文献中，常被称作STC/MLLT。

《Semi-tied Covariance Matrices for Hidden Markov Models》

《Improved feature processing for Deep Neural Networks》

![](/images/img2/MLLT.png)

https://blog.csdn.net/xmdxcsj/article/details/78512652

声学特征变换 STC/MLLT

# WFST

## 概述

Weighted Finite State Transducer是目前解码器模块的关键技术。

论文：

《Speech Recognition with Weighted Finite-State Transducers》

>Mehryar Mohri，法国人，Ecole Polytechnique本科+ENS Ulm硕士+University of Paris 7博士。New York University教授。Google研究顾问。   
>个人主页：   
>https://cs.nyu.edu/~mohri/

https://cs.nyu.edu/~mohri/courses.html

这是Mohri的课程主页。

https://cs.nyu.edu/~mohri/asr12/

作为WFST的发明人，Mohri的Speech Recognition课程在解码器方面有相当大的篇幅。

此外，OpenFst的官网也有一些教程：

http://openfst.org/twiki/bin/view/FST/FstBackground

OpenFst Background Material

其中，最重要的是两个tutorial：

http://openfst.org/twiki/bin/view/FST/FstHltTutorial

OpenFst: An Open-Source, Weighted Finite-State Transducer Library and its Applications to Speech and Language

http://openfst.org/twiki/bin/view/FST/FstSltTutorial

OpenFst: a General and Efficient Weighted Finite-State Transducer Library

以下内容主要参考下文：

http://vsooda.github.io/2016/08/28/wfst/

加权有限状态转换器

## FSM

![](/images/img2/FSM.png)

上图是finite-state machine（也叫finite-state automaton，FSA）的示意图。图中的Node表示State，顾名思义，FSM的State数量是有限的。图中的Edge表示Transition，Edge上的Label表示Input/Event。

FSM的含义是，在某一状态下，获得一个输入，从而产生一个状态转换。例如，上图中在Sleep状态下，如果输入是hungry的话，那么状态就会切换到Eat状态。当然了，输入也可以不改变状态，比如在Sleep状态下，输入是tired的时候。

## FST

![](/images/img2/FST.png)

上图是finite-state transducers的示意图。FST和FSM的差别主要在Edge上的Label。FST收到Input的时候，不仅会发生状态改变，还会产生Output序列。因此，其Label的格式为`input:output`。

## WFST

![](/images/img2/WFST.png)

上图是WFST的示意图。顾名思义，Label上不仅有Input、Output，还有Weight信息，其格式为`input:output/weight`。

在有些图中会碰到$$\epsilon$$. 这个符号在输入时表示不消耗任何输入，在输出位置表示不产生任何输出。

此外，还有格式为`input/weight`的FSM，一般被称为Weighted Finite-State Acceptors。

![](/images/img2/WA.png)

不仅Edge上的Label可以有权重，Node上的Label也可以有权重，如上图所示。没有权重的Label，其权重为1。

例如，上图中：

$$[A](ab)=1\times 1\times 2+2\times 3\times 2=14$$

## 相关的群论知识

WFST是基于半环代数理论的，在介绍半环之前我先简单的说一下群和半群。

**群（Group）**：G为非空集合，如果在G上定义的二元运算*，满足：

（1）封闭性（Closure）：对于任意$$a,b\in G$$,有$$a*b\in G$$;

（2）结合律（Associativity）：对于任意$$a,b,c\in G,(a*b)*c=a*(b*c)$$;

（3）幺元（Identity）：存在幺元e，使得对于任意$$a\in G,e*a=a*e=a$$;

（4）逆元：对于任意$$a\in G$$,存在逆元$$a^{-1}*a=a*a^{-1}=e$$。

则称（G,*）为群。

**半群（Semigroup）**：仅满足封闭性和结合律群称为半群；如果还包含幺元，则成为幺元半群。

**半环（semiring）**：指具有两个二元运算$$+$$和$$\cdot$$的非空集合S，且满足：

（1）$$(S,+),(S,\cdot)$$都是半群；

（2）$$\forall a,b,c\in S,(a+b)c = ac+bc, c(a+b) = ca+cb$$

半环的形式化表示如下：

$$(K, \bigoplus, \bigotimes，0， 1)$$

其中K是一个数集，$$\bigoplus, \bigotimes$$是两个二元操作，’0’和’1’是特定的（designated）零元素和幺元素（不一定是真正的数0和数1）。

常用的半环如下表所示：

| Semiring | Set | $$\oplus$$ | $$\otimes$$ | 0 | 1 |
|:--:|:--:|:--:|:--:|:--:|:--:|
| Boolean | $$\{0,1\}$$ | $$\lor$$ | $$\land$$ | 0 | 1 |
| Probability | $$R_+$$ | $$+$$ | $$\times$$ | 0 | 1 |
| Log | $$R\cup\{-\infty，+\infty\}$$ | $$\oplus_{log}$$ | + | $$+\infty$$ | 0 |
| Tropical | $$R\cup\{-\infty，+\infty\}$$ | min | + | $$+\infty$$ | 0 |
| String | $$\Sigma^*\cup\{\infty\}$$ | $$\land$$ | $$\cdot$$ | $$\infty$$ | $$\epsilon$$ |

在WFST中用的比较多的是log半环和tropical半环。前者对路径概率进行了对数运算，而后者在log半环的基础上，进行了viterbi approximation，也就是用若干路径的概率极值，作为当前概率值，这和动态规划中的viterbi算法是一致的。

接下来定义WFST上的二元运算：

一整条路径的权重$$w[\pi ]=w[e_1]\bigotimes \cdots \bigotimes w[e_k]$$。

多个有限路径集合的权重$$w[R]=\bigoplus_{\pi \in R} w[\pi]$$。

参考：

http://hongjiang.info/semigroup-and-monoid/

半群(semigroup)与幺半群(monoid)

## Sum(Union)

介绍完WFST的定义，再来介绍一下定义在它之上的运算。

![](/images/img2/sum.png)

Sum运算的形式化描述为：

$$[T_1 \oplus T_2](x,y)=[T_1](x,y)\oplus [T_2](x,y)$$

## Product(Concatenation)

![](/images/img2/product.png)

Product运算的形式化描述为：

$$[T_1 \otimes T_2](x,y)=\bigoplus_{x=x_1x_2,y=y_1y_2} [T_1](x_1,y_1)\otimes [T_2](x_2,y_2)$$

## Closure

![](/images/img2/closure.png)

Closure运算的形式化描述为：

$$[T^*](x,y)=\bigoplus_{n=0}^{\infty}[T^n](x,y)$$

## Reversal

![](/images/img2/reversal.png)

Reversal运算的形式化描述为：

$$[\widetilde{T}](x,y)=[T](\widetilde{x},\widetilde{y})$$

## Inversion

![](/images/img2/inversion.png)

Inversion运算的形式化描述为：

$$[T^{-1}](x,y)=[T](y,x)$$

## Projection

![](/images/img2/projection.png)

Projection运算的形式化描述为：

$$[\prod_1(T)](x)=\bigoplus_y[T](x,y)$$

## Composition

Composition用来合并不同级别的转换器。用$$T=T_1\circ T_2$$表示这种操作。

![](/images/img2/composition.png)

Composition运算的形式化描述为：

$$[T_1\circ T_2](x,y)=\bigoplus_z [T_1](x,z)\bigotimes [T_2](z,y)$$

通俗一些的说法就是：

（1）起始状态应该是$$T_1，T_2$$的起始状态

（2）结束状态是$$T_1，T_2$$的结束状态

（3）如果$$q_1$$到$$r_1$$的边$$t_1$$的输出等于$$q_2$$到$$r_2$$的边$$t_2$$的输入，那么$$(q_1,q_2)$$和$$(r_1,r_2)$$应该有一条边，如果是tripical半环，则该边权重是以上两边权重之和。

当$$T_1$$的输出包含$$\epsilon$$，$$T_2$$的输入包含$$\epsilon$$的时候，会导致产生大量多余的边，使得最终结果不正确。这时需要去除$$\epsilon$$-path。细节略。
