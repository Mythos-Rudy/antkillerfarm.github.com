---
layout: post
title:  Attention（七）——Transformer进阶, Attention进阶
category: Attention 
---

* toc
{:toc}

# BERT进阶（续）

https://mp.weixin.qq.com/s/zqlWx3e4LOJ3_Zy2DEbCjw

从语言模型看Bert的善变与GPT的坚守

https://mp.weixin.qq.com/s/LngE10Hnqe9bgFzpNfUwLQ

NLP中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert

https://mp.weixin.qq.com/s/MgLLPEY3ynJGkuTgnIXndQ

站在BERT肩膀上的NLP新秀们（PART I）

https://mp.weixin.qq.com/s/yPq1cGnhcbaNLOjadj91pw

Bert时代的创新：Bert在NLP各领域的应用进展

https://mp.weixin.qq.com/s/l-de0vfx-L24g58IxK-NKQ

Jeff Dean强推：可视化Bert网络，发掘其中的语言、语法树与几何学

https://mp.weixin.qq.com/s/nlFXfgM5KKZXnPdwd97JYg

哈工大讯飞联合实验室发布基于全词覆盖的中文BERT预训练模型

https://zhuanlan.zhihu.com/p/70389596

一批高质量中文BERT预训练模型请查收（上）

https://mp.weixin.qq.com/s/h1VUSY7_UZF3PmjSN0DMSg

从One-hot, Word embedding到Transformer，一步步教你理解Bert

https://zhuanlan.zhihu.com/p/132554155

超细节的BERT/Transformer知识点

https://mp.weixin.qq.com/s/UJlmjFHWhnlXXJoRv4zkEQ

虽被BERT碾压，但还是有必要谈谈BERT时代与后时代的NLP

https://mp.weixin.qq.com/s/e4dgIdwzDzcLSkdgr1yZpg

LeCun力荐：Facebook推出十亿参数超大容量存储器

https://mp.weixin.qq.com/s/zXXtbuSvyMOkgrWJwB83kg

预训练语言模型的最新探索

https://mp.weixin.qq.com/s/WzGa5XVi2Op4Lz-1uQXfxQ

SpanBERT：提出基于分词的预训练模型，多项任务性能超越现有模型！

https://zhuanlan.zhihu.com/p/76912493

nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)

https://mp.weixin.qq.com/s/pYSs6NhIAB6DuwNnKZhkZQ

Bert改进：如何融入知识

https://mp.weixin.qq.com/s/in5SDWlQg8ts4E8DTmHxMQ

BERT在推荐系统领域可能会有什么作为？

https://mp.weixin.qq.com/s/kJhOrz0VaYc-k-6XJS02ag

8篇论文梳理BERT相关模型进展与反思

https://mp.weixin.qq.com/s/hI9XAiqKaHLq-Z9JkaWA_A

解决自然语言歧义问题，斯坦福教授、IJCAI卓越研究奖得主提出SenseBERT模型

https://mp.weixin.qq.com/s/55B0ToIKDusiPI5farR19w

NLP这两年：15个预训练模型对比分析与剖析

https://mp.weixin.qq.com/s/SPfa17p3QetZXCC01DwmQA

解密BERT

https://zhuanlan.zhihu.com/p/72805778

BERT的演进和应用

https://mp.weixin.qq.com/s/9YuBY0wLLVQ8ZrT9fiNICA

语音版BERT？滴滴提出无监督预训练模型，中文识别性能提升10%以上

https://mp.weixin.qq.com/s/OXkXjPHhaMXsKw2YevV6sw

邱锡鹏：从Transformer到BERT--自然语言处理中的表示学习进展

https://mp.weixin.qq.com/s/dV4RkxZOC9o2BxNi0GljKQ

谷歌最强NLP模型BERT官方中文版来了！多语言模型支持100种语言

https://zhuanlan.zhihu.com/p/49271699

从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史

https://mp.weixin.qq.com/s/k_33UK1RkMyHn6TSudU6Kg

详解谷歌最强NLP模型BERT

https://mp.weixin.qq.com/s/d2MZQbamdo0EC_MVtf-HZA

BERT详解：开创性自然语言处理框架的全面指南

https://mp.weixin.qq.com/s/pD4it8vQ-aE474uSMQG0YQ

两行代码玩转Google BERT句向量词向量

https://mp.weixin.qq.com/s/osmUZxAAX3x-oTHYJbzemA

谷歌BERT模型fine-tune终极实践教程

https://mp.weixin.qq.com/s/XmeDjHSFI0UsQmKeOgwnyA

小数据福音！BERT在极小数据下带来显著提升的开源实现

https://mp.weixin.qq.com/s/HXYDO5PM8UIoXgEPGe8p-w

图解当前最强语言模型BERT：NLP是如何攻克迁移学习的？

https://mp.weixin.qq.com/s/zz3j9HEuzw5e92MQXxSQsA

遗珠之作？谷歌Quoc Le这篇NLP预训练模型论文值得一看

https://mp.weixin.qq.com/s/IN4YfoZnlBozwEFdhSvLZg

用可视化解构BERT，我们从上亿参数中提取出了6种直观模式

https://mp.weixin.qq.com/s/nIT3GIU0dUIYyGChxsiOWw

Google BERT应用之《红楼梦》对话人物提取

https://mp.weixin.qq.com/s/dcp_ANYijRmicMYX7OpJmA

如何用最强模型BERT做NLP迁移学习？

https://mp.weixin.qq.com/s/DR4SkgOfUT7KYiaXm5NynQ

跨语言版BERT：Facebook提出跨语言预训练模型XLM

https://mp.weixin.qq.com/s/epjjHmlmMFhWtRO_cCUITA

用BERT进行多标签文本分类

https://mp.weixin.qq.com/s/Wk6gvOS_Qnud6ib1esMFXA

加入Transformer-XL，这个PyTorch包能调用各种NLP预训练模型！

https://mp.weixin.qq.com/s/GqqU3Ixht1BzMnQeRYQEqQ

谷歌NLP深度学习模型BERT特征的可解释性表现怎么样？

https://mp.weixin.qq.com/s/2f91Ksj19rk_emoFpEmPfA

从BERT看大规模数据的无监督利用

https://mp.weixin.qq.com/s/hF4EcKqmaTm_gemxX7Kftg

BERT的嵌入层是如何实现的？

https://mp.weixin.qq.com/s/CdjNQKSNuklVUsXe4InSoA

FastBERT：放飞BERT的推理速度

https://zhuanlan.zhihu.com/p/132361501

BERT是如何分词的

https://mp.weixin.qq.com/s/Tld9V1jdmWs06zNxiJNkZg

BART&MASS自然语言生成任务上的进步

https://mp.weixin.qq.com/s/G995ulqe6Ifxml_AJqapAw

BERT在小米NLP业务中的实战探索

https://www.cnblogs.com/gczr/p/12874409.html

Sentence-BERT: 一种能快速计算句子相似度的孪生网络

https://mp.weixin.qq.com/s/0hUNG6tC-hlfyTJtuzwU5w

NLP中的Mask全解

https://mp.weixin.qq.com/s/cyNcVNImoCOmTrsS0QVq4w

用Siamese和Dual BERT来做多源文本分类

https://mp.weixin.qq.com/s/uv74FKtUNtgjIBQZbsX7Qw

你finetune BERT的姿势可能不对哦？

https://mp.weixin.qq.com/s/BvM5zx-3XrsZj8BQ5WEa4A

一文带你了解MultiBERT

https://mp.weixin.qq.com/s/mFRhp9pJRa9yHwqc98FMbg

BERT在美团搜索核心排序的探索和实践

https://mp.weixin.qq.com/s/MPGF3tkNn3PBA_7S-fo9eg

谷歌新模型突破BERT局限：NLP版“芝麻街”新成员Big Bird长这样

https://zhuanlan.zhihu.com/p/165893466

BERT及其变种

https://mp.weixin.qq.com/s/5HZULHPI3-HJypvAMXEOcQ

MT-BERT在文本检索任务中的实践

https://mp.weixin.qq.com/s/0aZdGzcGW5ZA020rhX0qSQ

BERT4Rec:使用Bert进行序列推荐

https://mp.weixin.qq.com/s/fr-THgOeaTspKsv_hXnU2Q

CogLTX：将BERT应用于长文本

https://www.cnblogs.com/zhouxiaosong/p/11397655.html

使用BERT模型生成token级向量

https://mp.weixin.qq.com/s/JLP4-5IR6HPK4SRQoC9FAQ

BERT预训练实操总结

https://mp.weixin.qq.com/s/FuO8zY3XoIF-s6_8aXAusw

BERT相关模型汇总梳理

https://zhuanlan.zhihu.com/p/348373259

史上最细节的自然语言处理NLP/Transformer/BERT/Attention面试问题与答案

https://mp.weixin.qq.com/s/Jrs8okgVAh0fymIq-jCqgA

模型压缩与蒸馏！BERT的忒修斯船

https://mp.weixin.qq.com/s/UNHu1eVNorWWKbDb0XBJcA

模型压缩与蒸馏！BERT家族的瘦身之路

https://mp.weixin.qq.com/s/oD_Vibp4Ygraix23K_oV2Q

BERT在58搜索的实践

https://mp.weixin.qq.com/s/bqvEeCyX8pqhJQfCUvUkEw

图解BERT：通俗的解释BERT是如何工作的

# Transformer进阶

https://mp.weixin.qq.com/s/MjCIAlDWyHPLj_sGSPc4rg

复旦邱锡鹏组最新综述：A Survey of Transformers

https://mp.weixin.qq.com/s/-Y7Qy-5aJNJ5bx8QJf3k2w

Transformer及其变种

https://mp.weixin.qq.com/s/nSokDcIkOSSrRnhHCuu4Mg

Transformer家族简史（PART I）

https://mp.weixin.qq.com/s/p919Kfv-1GSDM6u6FpnBsA

Transformer家族简史（PART II）

https://mp.weixin.qq.com/s/M0zLw9hA5xzontKB7Zj23Q

Memory Transformer，一种简单明了的Transformer改造方案

https://mp.weixin.qq.com/s/FJeZ8X9gtyciqCTs9zvlLA

Transformer是CNN是GNN是RNN，Attention is all you need！

https://mp.weixin.qq.com/s/d1qqRw7sWyLdoyfnqMBdJQ

深度自适应Transformer

https://mp.weixin.qq.com/s/UowNtBm_hqnes-Lz3POXGQ

Transformers中的Beam Search高效实现

https://mp.weixin.qq.com/s/KdKbOrjeeo7Db095V7mSFA

Transformer之自适应宽度注意力

https://mp.weixin.qq.com/s/EuCCeWz_rkktwLuFJ75BXA

Transformer+AutoML: 遗传搜索在序列式任务上的应用

https://mp.weixin.qq.com/s/OEpLpWzkdfFUQf4cKNuG4w

Performer:基于正交随机特征的快速注意力计算

https://mp.weixin.qq.com/s/eWQLkiJ_XIo7LpTUE9c0qA

Transformer中的相对位置编码

https://mp.weixin.qq.com/s/mZBHjuHJG9Ffd0nSoJ2ISQ

什么是Transformer位置编码？

https://mp.weixin.qq.com/s/V0NAOgluyZN9P8iuhMKRwQ

Transformer为啥在NER上表现不好

https://mp.weixin.qq.com/s/ANFSNW1-mcjPqjcroNHeZQ

RealFormer：Real简单，Real有效

https://mp.weixin.qq.com/s/u-Twg6Cj6VfL6m4K0seBlw

谷歌研究院出品：高效Transformer模型最新综述

https://mp.weixin.qq.com/s/2S_2Z5-ioCNxH1kqFcUuQA

竞赛中的Transformer家族

https://mp.weixin.qq.com/s/mc6M2vEcPG6oMfKe3_apzQ

Transformer变体层出不穷，它们都长什么样？

https://mp.weixin.qq.com/s/IWUxVzpdGIX1Oxn4KxjhHA

一个Transformer，很强；两个，更强？（TransGAN）

https://mp.weixin.qq.com/s/IWUxVzpdGIX1Oxn4KxjhHA

TransGAN：两个Transformer可以构造一个强大的GAN

# Attention进阶

https://mp.weixin.qq.com/s/wrmjMLPuvpLIcF5VQBqZxg

最新“注意力机制Attention”大综述论文，66页pdf

https://mp.weixin.qq.com/s/rrbwItXt-1EaGiqtDEGvog

为节约而生：从标准Attention到稀疏Attention

https://mp.weixin.qq.com/s/MzHmvbwxFCaFjmMkjfjeSg

遍地开花的Attention，你真的懂吗？

https://mp.weixin.qq.com/s/e_LEhLf2Rh-1zkEBmqS4nA

NLP这两年：15个预训练模型对比分析与剖析

https://mp.weixin.qq.com/s/LAInpFPa-3R1rfv6idILnw

注意力机制发展如何了，如何学习它在各类任务中的应用？

https://zhuanlan.zhihu.com/p/40920384

真正的完全图解Seq2Seq Attention模型

https://mp.weixin.qq.com/s/ZSzHOu6uowRSoWrqB7vOaQ

深度学习注意力机制-Attention in Deep learning-附101页PPT

https://mp.weixin.qq.com/s/FlA1YrR0sLQGJoJZnSXpRw

DeepMind：深度学习注意力与记忆机制，附70页ppt

https://mp.weixin.qq.com/s/_mBa-GTdILrvluJtegz8fw

南洋理工大学：注意力神经网络，Attention Neural Networks，78页ppt

https://mp.weixin.qq.com/s/pKxqPB9qIGmE_PslPa6EyA

长文详解Attention的前世今生

https://mp.weixin.qq.com/s/2kFZAmb_WnTNYUXT_jVMqg

Attention注意力机制的前世今生

https://mp.weixin.qq.com/s/eq1iTyKguQm5t6Xoc7KUgw

一文搞懂NLP中的Attention机制

https://zhuanlan.zhihu.com/p/106662375

More About Attention

https://mp.weixin.qq.com/s/MFxQSUMFNRXjdLQwzveO4w

深度学习中的注意力机制

https://mp.weixin.qq.com/s/vkyPwsaxH-SvHqQx09VhVw

深度学习中的注意力机制（二）

https://mp.weixin.qq.com/s/fbAAA7fO2voP_v-NsavQew

深度学习中的注意力机制（三）

https://mp.weixin.qq.com/s/CftkSOmAx0UTtCixdxj6_A

深度学习中的注意力机制（完结篇）
