---
layout: post
title:  图像处理理论（十一）——CV/CG参考资源
category: graphics 
---

# CV/CG参考资源

https://github.com/gzr2017/ImageProcessing100Wen

图像处理入门100题

https://zhuanlan.zhihu.com/p/32752535

立体匹配成像算法BM，SGBM，GC，SAD一览

https://mp.weixin.qq.com/s/cVTfk0xK6F_gHRnoHYUxSw

计算机视觉基本原理介绍—RANSAC

https://blog.csdn.net/wanghanthu/article/details/52777512

Tracking算法：Discriminative Correlation Filter (DCF)

https://blog.csdn.net/wanghanthu/article/details/53375393

Kernelized Correlation Filters (KCF) Tracking算法

https://mp.weixin.qq.com/s/1lLrbi_Dtyq4ixMfR4pPnA

线性卷积积分及其在图像增强和特效方面的应用

https://blog.csdn.net/iverson_49/article/details/38160081

薄板样条函数(Thin plate splines)的讨论与分析

https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247508603&idx=2&sn=cfb091884ccc7a0ea03af38b1a629823

巧解图像处理经典难题之图像配准

https://blog.csdn.net/autocyz/article/details/48136473

相关滤波跟踪（MOSSE）

https://blog.csdn.net/qq_17783559/article/details/82254996

MOSSE原理及代码解析

https://mp.weixin.qq.com/s/XqFe9d72CepdfTSOq2gWJg

半全局匹配SGM

# 博弈论

## 概述（续）

**风险态度**：

- 中立风险（risk neutral）：认为同样期望回报的价值相同。
- 厌恶风险（risk averse）：倾向于一个确定性的回报，不愿意采用一个拥有同样期望回报的不确定性方案。
- 喜爱风险（risk loving）：更严格地倾向于采用拥有同样期望回报的赌注。

## 囚徒困境

![](/images/img3/prisoner_dilemma.jpg)

上图是囚徒困境（prisoner's dilemma）的**策略矩阵**。

参与者为：囚徒A和囚徒B。动作空间为：{坦白、抵赖}，回报函数由矩阵给出。即：

- 当囚徒A和囚徒B都坦白时，囚徒A被判处3年有期徒刑、囚徒B也被判处3年有期徒刑。
- 当囚徒A坦白、囚徒B抵赖时，囚徒A被当场释放、囚徒B被判处5年有期徒刑。
- 当囚徒A抵赖、囚徒B坦白时，囚徒A被判处5年有期徒刑、囚徒B当场释放。
- 当囚徒A抵赖、囚徒B抵赖时，囚徒A和B都被判处1年有期徒刑。

很明显，如果两个囚徒都选择抵赖，那么它们总的惩罚最低。然而，选择抵赖对于囚徒个人来说是理性的吗？

答案是：选择抵赖对于个人来说并不理性。因为，就个人而言，囚徒并不知道另外一个囚徒选择的策略是什么。在这种情况下，选择坦白对于个人来说是理性的，而且是最优的。

即，不管其他囚徒选择什么动作，选择坦白总比选择抵赖要优。

比如，对于囚徒A来说：

当囚徒B选择坦白时，如果囚徒A选择坦白被判处3年有期徒刑；而这时如果A选择抵赖则被判处5年有期徒刑，所以这时囚徒A选择坦白要好。

当囚徒B选择抵赖时，如果囚徒A选择坦白，则当场释放；而这时如果A选择抵赖，则被判处1年有期徒刑，所以这时囚徒A选择坦白要好。

综合这两种情况，对于囚徒A不管囚徒B如何选择，选择坦白都是最好的。

在该例中（坦白，坦白）是**占优策略（dominated strategy）**。所谓占优策略是指如果一方在任何情况下从某种策略中得到的回报均大于从另外一种策略得到的回报，那么我们称为这种策略为占优策略。

## 纳什均衡

Nash equilibrium

![](/images/img3/Nash.png)

>John Nash，1928～2015，数学家、经济学家。Princeton博士（1950），Princeton教授。主要研究博弈论、微分几何学和偏微分方程。诺贝尔经济学奖获得者（1994）。奥斯卡金像奖电影《美丽心灵》男主角原型。

## 智猪博弈问题

智猪博弈问题是John Nash于1950年提出的问题。

在一个猪圈里养着一头大猪和一头小猪，在猪圈的一端放有一个猪食槽，在另一端安装有一个按钮，它控制着猪食的供应量。假定：

- 猪按一下按钮，就有8单位猪食进槽，但谁按按钮就会首先付出2单位成本；
- 若大猪先到食槽，则大猪吃到7单位食物，而小猪仅能吃到1单位食物；
- 若小猪先到，则大猪和小猪各吃到4单位食物；
- 若两猪同时到，则大猪吃到5单位，小猪仅吃到3单位。

显然，在这里按按钮有两个成本：

- 直接成本：2单位成本。
- 间接成本：先按按钮的猪，肯定会最后到达食槽。

因此，这个问题写成策略矩阵，则是：

|  |  |  | 小猪 |
|  |  | 按 | 等待 |
|  | 按 | 3,1 | 2,4 |
| 大猪 | 等待 | 7,-1 | 0,0 |

该博弈不存在占优战略均衡，因为尽管小猪有一个严格占优战略，但大猪并没有占优战略。

为了解决这个问题，Nash提出了重复剔除的占优战略均衡（iterated dominance equilibrium）。

其具体做法如下：

**Step 1**：大猪没有劣战略，策略保持不变。

**Step 2**：小猪有一个劣战略: “按”。

“按”的支付值： 1, -1

“等待”的支付值： 4, 0

**Step 3**：剔除小猪的劣战略“按”。

**Step 4**：剔除之后，大猪有一个劣战略：“等待”。

**Step 5**：剔除大猪的劣战略“等待”，剩下最后一个战略组合：

大猪：“按” + 小猪：“等待”

在小企业经营中，学会如何“搭便车”是一个精明的职业经理人最为基本的素质。在某些时候，如果能够注意等待，让其他大的企业首先开发市场，是一种明智的选择。这时候有所不为才能有所为！

## 其他博弈问题

当年英国政府将流放澳洲的犯人交给往来于澳洲之间的商船来完成，由此经常会发生因商船主或水手虐待犯人，致使大批流放人员因此死在途中(葬身大海)的事件发生。

后来大英帝国对运送犯人的办法(制度)稍加改变，流放人员仍然由往来于澳洲的商船来运送，只是运送犯人的费用要等到犯人送到澳洲后才由政府按实到犯人人数支付给商船。

仅就这样一点小小的“改变”，几乎再也没有犯人于中途死掉的事情发生。

## 参考

https://www.cnblogs.com/steven-yang/tag/博弈论/

一个博弈论的专栏

https://mp.weixin.qq.com/s/5o3m8RLwYkZJEhqNxOLq_A

不对称多代理博弈中的博弈理论解读

https://mp.weixin.qq.com/s/D9bRjYVJOMT0Jkh59XZ-Rg

DeepMind于Nature子刊发文提出非对称博弈的降维方法

https://mp.weixin.qq.com/s/1t6WuTQpltMtP-SRF1rT4g

当博弈论遇上机器学习：一文读懂相关理论

https://news.mbalib.com/story/242878

智猪博弈、龟兔悖论、谷堆悖论...这些有趣的博弈论值得一看！

https://blog.csdn.net/qq_27351341/article/details/81119533

偏好函数、无差异曲线、帕累托标准、卡尔多-希克斯标准等基础概念

https://blog.csdn.net/qq_27351341/article/details/81138774

囚徒困境、智猪博弈、纳什均衡与一致预期

https://blog.csdn.net/qq_27351341/article/details/81268801

多重均衡与制度和文化

https://blog.csdn.net/qq_27351341/article/details/81276298

动态博弈、威胁与承诺

https://blog.csdn.net/sobermineded/article/details/79541511

几个经典博弈模型（囚徒的困境、赌胜博弈、产量决策的古诺模型）

# 模仿学习

https://zhuanlan.zhihu.com/p/27935902

机器人学习Robot Learning之模仿学习Imitation Learning的发展

https://zhuanlan.zhihu.com/p/25688750

模仿学习（Imitation Learning）完全介绍

https://mp.weixin.qq.com/s/naq73D27vsCOUBperKto8A

从监督式到DAgger，综述论文描绘模仿学习全貌

https://mp.weixin.qq.com/s/LNNqp2KsEAljG26hY43mUw

ICML2018 模仿学习教程

https://mp.weixin.qq.com/s/f9vSgH1HQwGXBDb0UGHQyQ

深度学习进阶之无人车行为克隆

# RL与神经科学

Pavlov Model（1901）

Rescorla-Wagner Model（1972）

Thorndike’s Puzzle Box（1910）

参考：

https://zhuanlan.zhihu.com/p/24437724

学习理论之Rescorla-Wagner模型

# RL参考资源+

https://mp.weixin.qq.com/s/Fn1s9Ia8L1ckgn6iP24FhQ

如何让神经网络具有好奇心

https://mp.weixin.qq.com/s/PBf-YrkhwhPYXuiOGyahxQ

强化学习遭遇瓶颈！分层RL将成为突破的希望

https://zhuanlan.zhihu.com/p/58815288

强化学习之值函数学习

https://mp.weixin.qq.com/s/8Cqknze_iosz6Z6cqnuK5w

谷歌提出强化学习新算法SimPLe，模拟策略学习效率提高2倍

https://mp.weixin.qq.com/s/hKGS4Ek5prwTRJoMCaxQLA

强化学习Exploration漫游

https://zhuanlan.zhihu.com/p/65116688

值分布强化学习（01）

https://zhuanlan.zhihu.com/p/65364484

值分布强化学习（02）

https://zhuanlan.zhihu.com/p/62363784

强化学习之策略搜索

https://mp.weixin.qq.com/s/j9Cs5M9gyITu2u_XDkKm-g

Policy Gradient——一种不以loss来反向传播的策略梯度方法

https://mp.weixin.qq.com/s/x6gKTuYIx8y25KX-fCc5bA

蒙特卡洛梯度估计方法（MCGE）简述
