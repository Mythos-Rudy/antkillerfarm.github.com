---
layout: post
title:  图像处理理论（十一）——CV/CG参考资源
category: graphics 
---

# CV/CG参考资源

https://github.com/gzr2017/ImageProcessing100Wen

图像处理入门100题

https://zhuanlan.zhihu.com/p/32752535

立体匹配成像算法BM，SGBM，GC，SAD一览

https://mp.weixin.qq.com/s/cVTfk0xK6F_gHRnoHYUxSw

计算机视觉基本原理介绍—RANSAC

https://blog.csdn.net/wanghanthu/article/details/52777512

Tracking算法：Discriminative Correlation Filter (DCF)

https://blog.csdn.net/wanghanthu/article/details/53375393

Kernelized Correlation Filters (KCF) Tracking算法

https://mp.weixin.qq.com/s/1lLrbi_Dtyq4ixMfR4pPnA

线性卷积积分及其在图像增强和特效方面的应用

https://blog.csdn.net/iverson_49/article/details/38160081

薄板样条函数(Thin plate splines)的讨论与分析

https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247508603&idx=2&sn=cfb091884ccc7a0ea03af38b1a629823

巧解图像处理经典难题之图像配准

https://blog.csdn.net/autocyz/article/details/48136473

相关滤波跟踪（MOSSE）

https://blog.csdn.net/qq_17783559/article/details/82254996

MOSSE原理及代码解析

https://mp.weixin.qq.com/s/XqFe9d72CepdfTSOq2gWJg

半全局匹配SGM

https://mp.weixin.qq.com/s/UKxQQx0IJGWOFikz3pncAw

最新的图像质量评价

# Integrating Learning and Planning

## Dyna-Q Algorithm（续）

![](/images/img3/Dyna_4.png)

上面的例子模拟了某时刻（虚线所示）环境发生了改变，变得更加困难，此时使用Dyna-Q算法在经历过一段时间的平台期后又找到了最优解决方案。在平台期，模型持续的给以个体原先的策略，也就是错误的策略，但个体通过与实际的交互仍然能够找到最优方案。

上二个例子表明，Dyn-Q算法赋予了个体一定的应对环境变化的能力，当环境发生一定改变时，个体一方面可以利用模型，一方面也可以通过与实际环境的交互来重新构建模型。

>上2个示例图中的Q+算法与Q的差别体现在：Q+算法鼓励探索，给以探索额外的奖励。

## Simulation-Based Search

Planning主要有两个用途：

- 改善policy或value function。这种方式通常叫做background planning。它不在乎当前的状态，而在乎对于所有状态都有效的policy或value function。

- 直接根据当前状态选择动作。这种方式通常叫做decision-time planning。它更在乎于当前状态。

这里以最简单的前向搜索(Forward Search)算法来讨论一下decision-time planning的特点。

![](/images/img3/FS.png)

上图是Forward Search的示意图，图中绿色方格节点指的是终止状态。

这种算法以当前状态$$S_t$$作为根节点构建了一个搜索树（Search Tree），使用MDP模型进行前向搜索。前向搜索不需要解决整个MDP，而仅仅需要构建一个从当前状态开始与眼前的未来相关的次级（子）MDP。这相当于一个登山运动员的登山问题，在某一个时刻，他只需要关注当前位置（状态）时应该采取什么样的行为才能更有利于登山（或者下撤），而不需要考虑第二天中饭该吃什么，那是登顶并安全撤退后才要考虑的事情。

从中可以看出，background planning相当于是**广度优先遍历**，虽然考虑的比较全面，但决策的深度较浅，而decision-time planning虽然只考虑当前状态的情况，但决策的深度较深（通常是一个完整的episode），相当于是**深度优先遍历**。

sample-based planning+Forward search就得到了Simulation-Based Search。

首先，从Model中采样，形成从当前状态开始到终止状态的一系列Episode：

$$\{s_t^k, A_t^k, R_{t+1}^k, \dots, S_T^k\}_{k=1}^K\sim M_v$$

有了这些Episode资源，我们可以使用Model-Free的强化学习方法，如果我们使用Monte-Carlo control，那么这个算法可以称作蒙特卡罗搜索(Monte-Carlo Search)，如果使用Sarsa方法，则称为TD搜索(TD Search)。

## Monte-Carlo Search

我们首先看一下Simple Monte-Carlo Search的步骤：

- 给定一个模型M和一个模拟的策略$$\pi$$。

- 针对行为空间里的每一个行为$$a\in A$$：

从这个当前状态$$s_t$$开始模拟出K个Episodes：

$$\{s_t^k, a, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k,\dots, S_T^k\}_{k=1}^K\sim M_v,\pi$$

使用平均收获（蒙特卡罗评估）来评估当前行为a的行为价值：

$$Q(s_t, a)=\frac{1}{K}\sum_{k=1}^KG_t\xrightarrow[]{P} q_\pi(s_t, a)$$

- 选择一个有最大Q值的行为作为实际要采取的行为：

$$a_t = \mathop{\arg\max}_{a\in A}Q(s_t, a)$$

Simple Monte-Carlo Search的主要问题在于模拟出的Episodes的质量依赖于Model的质量，如果Model不准确，就会导致一系列的偏差，最终会导致结果的不准确。

我们可以采用优化领域常用的Evaluate/Improve方法来改进Monte-Carlo Search，于是就得到了Monte-Carlo Tree Search（MCTS）方法。

这里先看一下MCTS的Evaluation阶段的流程：

- 给定一个模型M。

- 从当前状态$$s_t$$开始，使用当前的模拟策略$$\pi$$,模拟生成K个Episodes：

$$\{s_t^k, a, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k,\dots, S_T^k\}_{k=1}^K\sim M_v,\pi$$

- 构建一个包括所有个体经历过的状态和行为的**搜索树**。

- 对于搜索树种每一个状态行为对$$(s,a)$$，计算从该$$(s,a)$$开始的所有完整Episode收获的平均值，以此来估算该状态行为对的价值$$Q(s,a)$$:

$$Q(s_t, a)=\frac{1}{N(s,a)}\sum_{k=1}^K \sum_{u=t}^T 1(S_u,A_u=s,a) G_u\xrightarrow[]{P} q_\pi(s, a)$$

- 选择一个有最大Q值的行为作为实际要采取的行为：

$$a_t = \mathop{\arg\max}_{a\in A}Q(s_t, a)$$

MCTS的一个特点是在构建搜索树的过程中，更新了搜索树内状态行为对的价值，积累了丰富的信息，利用这些信息可以更新模拟策略，使得模拟策略得到改进。换句话说，MCTS的策略不是一成不变的。

MCTS的Simulation也是有讲究的。

首先，改进策略时要分情况对待：

- 树内（in-tree）确定性策略（Tree Policy）：对于在搜索树中存在的状态行为对，策略的更新倾向于最大化Q值，这部分策略随着模拟的进行是可以得到持续改进的。

- 树外（out-of-tree）默认策略（Default Policy）：对于搜索树中不包括的状态，可以使用固定的随机策略。

随着不断地重复模拟，状态行为对的价值将得到持续地得到评估。同时基于$$\epsilon-greedy(Q)$$的搜索策略使得搜索树不断的扩展，策略也将不断得到改善。

下面我们结合实例（下围棋）和示意图，来实际了解MCTS的运作过程。

![](/images/img3/MCTS.png)

**第一次迭代**：五角形表示的状态是个体第一次访问的状态，也是第一次被录入搜索树的状态。我们构建搜索树：将当前状态录入搜索树中。使用基于蒙特卡罗树搜索的策略（两个阶段），由于当前搜索树中只有当前状态，全程使用的应该是一个搜索第二阶段的默认随机策略，基于该策略产生一个直到终止状态的完整Episode。图中那些菱形表示中间状态和方格表示的终止状态，在此次迭代过程中并不录入搜索树。终止状态方框内的数字1表示（黑方）在博弈中取得了胜利。此时我们就可以更新搜索树种五角形的状态价值，以分数1/1表示从当前五角形状态开始模拟了一个Episode，其中获胜了1个Episode。

![](/images/img3/MCTS_2.png)

**第二次迭代**：当前状态仍然是树内的圆形图标指示的状态，从该状态开始决定下一步动作。根据目前已经访问过的状态构建搜索树，依据模拟策略产生一个行为模拟进入白色五角形表示的状态，并将该状态录入搜索树，随后继续该次模拟的对弈直到Episode结束，结果显示黑方失败，因此我们可以更新新加入搜索树的五角形节点的价值为0/1，而搜索树种的圆形节点代表的当前状态其价值估计为1/2，表示进行了2次模拟对弈，赢得了1次，输了1次。

经过前两次的迭代，当位于当前状态（黑色圆形节点）时，当前策略会认为选择某行为进入上图中白色五角形节点状态对黑方不利，策略将得到更新：当前状态时会个体会尝试选择其它行为。

![](/images/img3/MCTS_3.png)

**第三次迭代**：假设选择了一个行为进入白色五角形节点状态，将该节点录入搜索树，模拟一次完整的Episode，结果显示黑方获胜，此时更新新录入节点的状态价值为1/1，同时更新其上级节点的状态价值，这里需要更新当前状态的节点价值为2/3，表明在当前状态下已经模拟了3次对弈，黑方获胜2次。

随着迭代次数的增加，在搜索树里录入的节点开始增多，树内每一个节点代表的状态其价值数据也越来越丰富。在搜索树内依据$$\epsilon$$-greedy策略会使得当个体出于当前状态（圆形节点）时更容易做出到达图中五角形节点代表的状态的行为。

![](/images/img3/MCTS_4.png)

**第四次迭代**：当个体位于当前（圆形节点）状态时，树内策略使其更容易进入左侧的蓝色圆形节点代表的状态，此时录入一个新的节点（五角形节点），模拟完Episode提示黑方失败，更新该节点以及其父节点的状态价值。

![](/images/img3/MCTS_5.png)

**第五次迭代**：更新后的策略使得个体在当前状态时仍然有较大几率进入其左侧圆形节点表示的状态，在该节点，个体避免了进入刚才失败的那次节点，录入了一个新节点，基于模拟策略完成一个完整Episode，黑方获得了胜利，同样的更新搜索树内相关节点代表的状态价值。

如此反复，随着迭代次数的增加，当个体处于当前状态时，其搜索树将越来越深，那些能够引导个体获胜的搜索树内的节点将会被充分的探索，其节点代表的状态价值也越来越有说服力；同时个体也忽略了那些结果不好的一些节点（上图中当前状态右下方价值估计为0/1的节点）。需要注意的是，仍然要对这部分节点进行一定程度的探索，以确保这些节点不会被完全忽视。
