---
layout: post
title:  强化学习（二）——K-摇臂赌博机, Q-learning, MDP, 模仿学习
category: RL 
---

# K-摇臂赌博机（续）

这里显然有两个最简单的策略：

**exploration-only**：将所有尝试机会平均分配给每个摇臂。这种策略可以很好的估计每个摇臂的奖励，然而却会失去很多选择最优摇臂的机会。

**exploitation-only**：只按下目前最优的摇臂。这种策略下有可能选不到最优的摇臂。

显然，欲奖励最大，需要在exploration和exploitation之间达成较好的折中。

## $$\epsilon$$-贪心算法

$$\epsilon$$-greedy基于概率对exploration和exploitation进行折中，即：**以$$\epsilon$$进行exploration，而以$$1-\epsilon$$进行exploitation。**

若摇臂奖励的不确定性较大，即概率分布较宽时，需要较大的$$\epsilon$$值，反之则小。

另外，开始时由各种信息较少，$$\epsilon$$需要设置的大一些，随着探索的深入，各摇臂的奖励基本弄清楚之后，$$\epsilon$$就可以小一些了。因此通常令$$\epsilon=1/\sqrt{t}$$。

当$$\epsilon=0$$时，该算法也被称为greedy算法，也就是之前提到的exploitation-only策略。

必须指出的是，k-armed Bandit只是真实问题的一个极度简化模型：它只有action和reward，没有input或sequentiality，也没有state。

## Softmax算法

Softmax算法和$$\epsilon$$-贪心算法类似，其公式为：

$$P(k)=\frac{e^{\frac{Q(k)}{\tau}}}{\sum_{i=1}^{K}e^{\frac{Q(i)}{\tau}}}$$

这实际上是一个Boltzmann分布的公式，其中$$\tau$$表示温度，$$\tau \to 0$$表示优先利用，$$\tau \to \infty$$表示优先探索。

## 求均值的小技巧

在k-armed Bandit问题中，一般用reward的均值来近似它的数学期望值。由于action是个序列，因此相应的算法一般是个不断迭代的过程，或者也可以说是一个增量（incremental）过程。

在增量过程中，保存所有的reward值来计算均值显然是个笨办法。这里常用的办法是：

$$Q_{n+1}=Q_n+\frac{1}{n}[R_n-Q_n]$$

不光reward值可以这样更新，其他均值也可采用这个方法：

$$NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]$$

## 非平稳问题

在之前的假设中，我们认为每个摇臂的吐币概率和数量是不随时间变化的，这样的问题被称为Stationary Problem。

如果每个摇臂的吐币概率和数量随时间**缓慢变化**的话，则称之为Non-stationary Problem。

>注：快速变化的系统，不光RL无能，其他方法估计也没什么好效果。所以系统保持一定的惯性，对于研究问题是很重要的。

这时一般采用如下公式滤波：

$$Q_{n+1}=Q_n+\alpha[R_n-Q_n]$$

详细内容参见《数学狂想曲（四）》的“软件滤波算法”一节的“一阶滞后滤波法”和“加权递推平均滤波法”。

## Gradient-Bandit算法

Gradient-Bandit算法的定义如下：

$$Pr\{A_t=a\}=\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}=\pi_t(a)$$

$$H_{t+1}(a)=H_t(a)+\alpha(R_t-\overline R_t)(1\{A_t=a\}-\pi_t(a))$$

$$\overline R_t=\frac{1}{t}\sum_{i=1}^tR_i$$

其中，$$H_t(a)$$被称作策略偏好（preference）。这实际上是一个Softmax算法的变种。

## 多步强化学习

对于多步强化学习任务，虽然可以将其中的每一步看作一个k-armed Bandit问题，然而由于这种方法忽视了决策过程之间的联系，存在很多局限，因此不如MDP相关的算法。

## 参考

http://www.xfyun.cn/share/?p=2606

Bandit算法与推荐系统

https://mp.weixin.qq.com/s/9dXqXmRkINjwJYB4csjU5A

强化学习初探-从多臂老虎机问题说起

# Q-learning

Q-learning是强化学习中很重要的算法，也是最早被引入DL领域的强化学习算法，对它的研究催生了Deep Q-learning Networks。

下面用一个例子来讲述Q-learning算法。

![](/images/article/q_learning.gif)

上图中有5个房间，编号为0～4，将户外定义为编号5，房间之间通过门相连，则房间的联通关系可抽象为下图：

![](/images/article/q_learning_1.gif)

这里我们将每个房间称为一个**state**，将agent从一个房间到另一个房间称为一个**action**。

开始时，我们将agent放置在任意房间中，并设定目标——走到户外（即房间5），则上图可变为：

![](/images/article/q_learning_2.gif)

这里的每条边上的数值就是reward值。Q-Learning的目标就是达到reward值最大的state。因此当agent到达户外之后，它就停留在那里了，这样的目标被称作**吸收目标**。

如果以state为行，action为列，则上图又可转化为如下的reward矩阵：

![](/images/article/r_matrix.gif)

其中，-1表示两个state之间没有action。

类似的，我们可以构建一个和R同阶的矩阵Q，来表示Q-Learning算法学到的知识。

开始时，agent对外界一无所知，所以Q可以初始化为零矩阵。

Q-Learning算法的**transition rule**为：

$$Q(s,a)=R(s,a)+\gamma \max(Q(\tilde s,\tilde a))\tag{1}$$

其中，(s,a)表示当前的state和action，$$(\tilde s,\tilde a)$$表示下一个state和action，$$0 \le \gamma < 1$$为学习参数。这个公式也被称作**Bellman equation**。

>Richard Ernest Bellman，1920～1984，美国应用数学家、控制论学家、数理生物学家。布鲁克林学院本科+威斯康星大学麦迪逊分校硕士+普林斯顿博士。二战期间曾在Los Alamos研究理论物理，后任职于美国智库RAND Corporation，南加州大学教授。美国艺术科学院院士，美国科学院院士。   
>Bellman–Ford算法的发明人之一。以他命名的奖项有Richard E. Bellman Control Heritage Award和Bellman Prize in Mathematical Biosciences。

在无监督的情况下，agent不断从一个状态转至另一状态进行探索，直到到达目标。我们将agent的每一次探索（从任意初始状态开始，经历若干action，直到到达目标状态的过程）称为一个**episode**。

Q-Learning算法的计算步骤如下：

>**Step 1**：给定参数$$\gamma$$和reward矩阵R。   
>**Step 2**：初始化Q=0。   
>**Step 3**：For each episode:
>>3.1 随机选择一个初始状态s。   
>>3.2 若未达到目标状态，则：
>>>(1)在当前s的所有action中，随机选择一个行为a。   
>>>(2)利用a，得到下一个状态$$\tilde s$$。   
>>>(3)利用公式1，计算$$Q(s,a)$$。   
>>>(4)令$$s=\tilde s$$。

由马尔可夫过程的性质（参见《机器学习（十六、二十）》）可知，Q矩阵最终会趋于一个极限，如下所示：

![](/images/article/q_matrix5.gif)

上面的矩阵可用下图表示：

![](/images/article/map5.gif)

显然，沿着Q值最大的边走，就是这个探索问题的最佳答案。（如上图红线所示）即：

$$\pi (s)=\arg \max_a(Q(s,a))$$

参考：

http://blog.csdn.net/itplus/article/details/9361915

一个Q-learning算法的简明教程

http://blog.csdn.net/young_gy/article/details/73485518

强化学习之Q-learning简介

https://mp.weixin.qq.com/s/34E1tEQMZuaxvZA66_HRwA

通过Q-learning深入理解强化学习

https://mp.weixin.qq.com/s/kmk3mKASZPixA8VS9sipcA

走近流行强化学习算法：最优Q-Learning

# Markov Decision Process

上边Q-Learning的例子中，由于action能够唯一确定状态的变换，因此又被称为**Markov Reward Process**：

$$<\mathcal{S},\mathcal{P},\mathcal{R},\gamma>$$

这个四元组依次代表：states、state transition probability matrix、reward function、discount factor。

实际中，执行特定的动作并不一定能得到特定的状态。

![](/images/article/Markov_Decision_Process.png)

比如上图中，在状态$$S_0$$，执行$$a_0$$，只有0.5的机会，会到达$$S_2$$。这也就是之前提到过的MDP。

标准MDP中的Bellman equation可改为如下形式：

$$v = \mathcal{R} + \gamma \mathcal{P}v$$

其中,$$\mathcal{P},\mathcal{R}$$均为已知。

这里的Bellman equation是线性方程，它的直接解法如下：

$$(I-\gamma \mathcal{P})v = \mathcal{R}$$

$$v = (I-\gamma \mathcal{P})^{-1}\mathcal{R}$$

然而这个方法的复杂度是$$O(n^3)$$（n是状态的个数），这对于大的MDP来说，并不好用。这种情况下，常用的解法有：Dynamic programming（动态规划）、Monte-Carlo evaluation和Temporal-Difference learning。

由于MDP对于RL任务进行了Markov假设，这属于一种建模行为，因此它也被归为一种model-based learning的算法。

MDP的扩展主要包括：

a) Observation部分可见的情况下，agent state $$\neq$$ environment state，这时一般叫做partially observable Markov decision process(POMDP)。

b) Infinite and continuous MDP

c) Undiscounted, average reward MDP

扩展MDP的Bellman equation都不是线性方程，没有解析解，只有迭代解。相关解法主要使用了概念图模型，这里不再详述。

参考：

https://mp.weixin.qq.com/s/G8zmooqhSKqcLv1u7uM0bw

AlphaGo等智能体是如何炼成的？你需要懂得马尔科夫链

# A2C

https://zhuanlan.zhihu.com/p/51652845

强化学习这都学不会的话，咳咳，你过来下！

# 模仿学习

https://zhuanlan.zhihu.com/p/27935902

机器人学习Robot Learning之模仿学习Imitation Learning的发展

https://zhuanlan.zhihu.com/p/25688750

模仿学习（Imitation Learning）完全介绍

https://mp.weixin.qq.com/s/naq73D27vsCOUBperKto8A

从监督式到DAgger，综述论文描绘模仿学习全貌

https://mp.weixin.qq.com/s/LNNqp2KsEAljG26hY43mUw

ICML2018 模仿学习教程

# 世说新语+

## 2019.1（续）

![](/images/img2/U.jpg)

伊朗时任总统内贾德视察伊朗首枚首枚燃料棒装填作业。从图中可以看出完全不需要任何防护，左边小哥戴的手套以及右边大叔戴的口罩与其说是在保护他们自己，不如说是在保护燃料棒束，避免其沾染汗渍和唾液。

https://www.zhihu.com/question/26946942

天然铀的放射性类型是什么？近距离接触对人有危害吗？

## 2019.2

国外的ai和国内的差距？

有github的话，半年；没有github的话，十年。

----

https://www.zhihu.com/question/60253584

有哪些科幻作品中设定的事件发生时间已经过去了？

----

https://mp.weixin.qq.com/s/jHswcZDCjBH6qp46W8Vc4g

漫画：为什么新疆永远不包邮？

----

![](/images/img2/l_hires.jpg)

原图地址：

http://www.lenna.org/full/l_hires.jpg

----

https://mp.weixin.qq.com/s/-CF69Soq9Bdw-Wsffyv9Hw

这40个冷知识，据说只有1%的人知道……颠覆你的认知！

----

http://inews.ifeng.com/51320479/news.shtml

丈夫不能满足她，她在家阁楼养了个小鲜肉，这一养，就是10年...
