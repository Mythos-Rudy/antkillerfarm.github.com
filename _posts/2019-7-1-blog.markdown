---
layout: post
title: Blog维护日志
category: essay 
---

# Blog维护日志

2019.11 添了一个National Taiwan University的follower。

2019.10.9 将RL分类拆分为RL和DRL两类。

2019.7 添加Attention、DL acceleration分类。

2019.6.19 文章数达到300篇。可命名为v4.5。

2019.4.26 img2的图片太多，特添加img3。

2019.3.27 NLP参考资源已经积攒了很多，特进行细分类，使之更有条理。

2019.3.19 添加GAN & VAE分类。

2018.11.19 添加RL分类。

2018.11.16 文章数达到250篇。一个分类达到50篇，可读性的问题就比较大了，修订也比较麻烦。特创建Deep object detection分类。由于文章较多，新分类的迁移将缓慢进行。可命名为v4.0。

2018.11.14 DL专栏文章达到50篇。

2018.8.1 在知名公众号上看到自己的文章。里程碑啊！

2018.7.26 近来blog相关的事情比较多。

1.CSDN编辑拉我进高级作者微信群。另一不知名小编居然和我约稿。。。太瞧得起我的水平了。。。

2.添了一个Google的follower，居然还是tensorflow的committer，就连做的工作都和我类似。。。

3.添了一个Azerbaijan的follower，有没有搞错啊！你确定你看得懂中文吗？

2018.7.19 添加Speech分类。

2018.2.22 文章数达到200篇。

2018.2.1 又添了几个清华、头条、商汤的follower。其中有个人，我俩几乎同时关注了对方，挺有缘分的。此外，居然还有人把我的blog给fork了。。。

2018.1.10 近来转战DL以后，blog的产量日增，也渐有重量级的follower：几个中科院、浙大的硕士。并收到了第一笔打赏。

2017.12.21 blog图片太多，特添加第2个图片文件夹。

2017.10.22 文章分为Language、Linux、Technology、Essay、Resource、Graphics、AI、ML、Math、DL十类。可命名为v3.5。

2017.10.18 文章数达到150篇。

2017.5.17 修改标题的css。可命名为v3.0。

2017.4.5 添加微信打赏二维码图片。

2017.3.22 评论系统切换到Github Issue。

2017.3.4 文章数达到100篇。

2016.11.3 文章分为Technology、Theory、Essay三类。

2016.8.1 添加blog数量的统计信息，将MathJax升级到2.6.1。

2016.5.12 为了广而告之，特将blog登录CSDN。但由于编辑方式的差异，Github仍为主站，内容较新较全。

2016.4.12 将MathJax等资源切换为国内的cdn。

2016.4.10 首页将技术文章和非技术文章分栏显示。

2016.3.30 白底黑字太晃眼，特切换为浅灰底色。

2016.3.18 评论系统切换到多说。

2016.2.22 文章数达到50篇。

2016.2.2 代码区的配色实在看起来费眼，屡次微调，仍不满意。这次借鉴原blog（也就是大徐抄袭的源头）的配色方案，最终达到基本满意的效果。可命名为v2.0。

2015.1.7 blog搬家基本完成。

2014.12.23 围观大徐的blog有感，特在github上安家落户，废弃了原来的搜狐blog。

# Integrating Learning and Planning

## Monte-Carlo Search（续）

MCTS的优点：

- 蒙特卡罗树搜索是具有高度选择性的（Highly selective)、导致越好结果的行为越被优先选择（best-first）的一种搜索方法。

- 它可以动态的评估各状态的价值，这种动态更新价值的方法与动态规划不同，后者聚焦于整个状态空间，而蒙特卡罗树搜索是立足于当前状态，动态更新与该状态相关的状态价值。

- 使用采样避免了维度灾难；同样由于它仅依靠采样，因此适用于那些“黑盒”模型（black-box models）。

- MCTS是可以高效计算的、不受时间限制以及可以并行处理的。

参考：

https://mp.weixin.qq.com/s/4qeHfU9GS4aDWOHsu4Dw2g

记忆增强蒙特卡洛树搜索细节解读

https://zhuanlan.zhihu.com/p/30458774

如何学习蒙特卡罗树搜索（MCTS）

https://zhuanlan.zhihu.com/p/26335999

蒙特卡洛树搜索MCTS入门

https://zhuanlan.zhihu.com/p/25345778

蒙特卡洛树搜索（MCTS）基础

## Game Tree

上面围棋示例中的搜索树，在博弈论中，也叫做博弈树（Game Tree）。

### Minimax

极小化极大算法（Minimax）是零和游戏常见的策略。通俗的说法就是：选择对自己最有利，而对对手最不利的动作。

### Alpha Beta pruning algorithm

在博弈过程中，我们不仅要思考自己的最优走法，还要思考对手的最优走法。而且这个过程是可以递归下去的，直到终局为止。

这样就会形成一整套如下所示的策略序列（以围棋为例，黑棋先下）：

黑最优->白最优->黑最优->白最优->...

然后轮到白棋下子：

<p><font color="red">黑最优</font>->白最优->黑最优->白最优->...</p>

这里的红色字体表示的是已经发生的事件。

可以看的出来，在零和游戏中，我们只需要考虑一方的Game Tree就行了，另一方的Game Tree只是一个镜像而已。这样的话，搜索空间直接就只有原来的一半了。

能不能进一步裁剪呢？这里介绍一下Alpha Beta pruning algorithm。

详细的步骤参见：

http://web.cs.ucla.edu/~rosen/161/notes/alphabeta.html

Minimax with Alpha Beta Pruning

这里只提一下要点：

- $$\alpha$$是指the maximum lower bound of possible solutions；$$\beta$$是指the minimum upper bound of possible solutions。

- 我方下棋会寻找使我方获得最大值的策略，因此叫做MAX结点；敌方下棋显然会寻找能使我方收益最小的策略，因此叫做MIN结点。从Game Tree的角度来看，就是：一层MAX结点->一层MIN结点->一层MAX结点->一层MIN结点->...

- 初始状态：根节点：$$\alpha=-\infty,\beta=\infty$$，叶子结点：包含value值。

- 深度优先遍历Game Tree，利用MAX结点的值，更新父结点的$$\beta$$值，利用MIN结点的值，更新父结点的$$\alpha$$值。

- 一旦出现$$\alpha > \beta$$的情况，则说明遇到了异常分支，直接剪去该分支即可。

### 参考

https://zhuanlan.zhihu.com/p/55750669

博弈树与α-β剪枝

https://segmentfault.com/a/1190000013527949

Minimax和Alpha-beta剪枝算法简介，以及以此实现的井字棋游戏（Tic-tac-toe）

https://materiaalit.github.io/intro-to-ai-17/part2/

Games

## MCTS和MC的区别

![](/images/img3/MCTS.jpg)

上图是一局围棋的Game Tree的局部。最下面的叶子结点给出了黑棋赢棋的百分比。现在的问题是，黑棋该如何走呢？（假设根节点为当前状态）

如果是用蒙特卡洛方法，趋近的会是其下所有胜率的平均值。例如经过蒙特卡洛模拟，会发现b1后续的胜率是49% = (50+48)/2，而b2后续的胜率是55% = (62+45+58)/3。

于是蒙特卡洛方法说应该走b2，因为55%比49%的胜率高。但这是错误的。因为如果白棋够聪明，会在黑棋走b1的时候回应以w2（尽量降低黑棋的胜率），在黑棋走b2的时候回应以w4（尽量降低黑棋的胜率）。

如果用蒙特卡洛方法做上一百万次模拟，b1和b2的胜率仍然会固定在49%和55%，不会进步，永远错误。所以它的结果存在偏差（Bias），当然，也有方差（Variance）。

而蒙特卡洛树搜索在一段时间模拟后，b1和b2的胜率就会向48%和45%收敛，从而给出正确的答案。所以它的结果不存在偏差（Bias），只存在方差（Variance）。但是，对于复杂的局面，它仍然有可能长期陷入陷阱，直到很久之后才开始收敛到正确答案。

## Dyna-2

如果我们把基于模拟的前向搜索应用到Dyna算法中来，就变成了Dyna-2算法。

使用该算法的个体维护了两套特征权重：

- 一套反映了个体的长期记忆，该记忆是从真实经历中使用TD学习得到，它反映了个体对于某一特定强化学习问题的普遍性的知识、经验；

- 另一套反映了个体的短期记忆，该记忆从基于模拟经历中使用TD搜索得到，它反映了个体对于某一特定强化学习在特定条件（比如某一Episode、某一状态下）下的特定的、局部适用的知识、经验。

# Exploration & Exploitation

几个基本的探索方法：

- **朴素探索(Naive Exploration)**: 在贪婪搜索的基础上增加一个$$\epsilon$$以实现朴素探索；

- **乐观初始估计(Optimistic Initialization)**: 优先选择当前被认为是最高价值的行为，除非新信息的获取推翻了该行为具有最高价值这一认知；

- **不确定优先(Optimism in the Face of Uncertainty)**: 优先尝试不确定价值的行为；

- **概率匹配（Probability Matching)**: 根据当前估计的概率分布采样行为；

- **信息状态搜索(Information State Search)**: 将已探索的信息作为状态的一部分联合个体的状态组成新的状态，以新状态为基础进行前向探索。

根据搜索过程中使用的数据结构，可以将搜索分为：

- **依据状态行为空间的探索(State-Action Exploration)**。针对每一个当前的状态，以一定的算法尝试之前该状态下没有尝试过的行为。

- **参数化搜索（Parameter Exploration)**。直接针对策略的函数近似，此时策略用各种形式的参数表达，探索即表现为尝试不同的参数设置。

Parameter Exploration的优点：得到基于某一策略的一段持续性的行为。

缺点：对个体曾经到过的状态空间毫无记忆，也就是个体也许会进入一个之前曾经进入过的状态而并不知道其曾到过该状态，不能利用已经到过这个状态这个信息。

## UCB

$$\epsilon$$-greedy和softmax算法的缺陷：

只关心回报是多少，并不关心每个臂被拉下了多少次，这就意味着，这些算法不再会选中初始回报特别低的臂，即使这个臂的回报只测试了一次。

UCB（The Upper Confidence Bound Algorithm）算法将会不仅仅关注于回报，同样会关注每个臂被探索的次数。

为了进一步刻画每个臂被探索的次数和回报之间的关系，UCB引入了**置信区间**的概念：

- 每个item的回报均值都有个置信区间，随着试验次数增加，置信区间会变窄（逐渐确定了到底回报丰厚还是可怜）。
- 每次选择前，都根据已经试验的结果重新估计每个item的均值及置信区间。
- 选择置信区间上限最大的那个item。

显然：

- 如果item置信区间很宽（被选次数很少，还不确定），那么它会倾向于被多次选择，这个是算法冒风险的部分；
- 如果item置信区间很窄（备选次数很多，比较确定其好坏了），那么均值大的倾向于被多次选择，这个是算法保守稳妥的部分；
