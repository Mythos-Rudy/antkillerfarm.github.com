---
layout: post
title: Blog维护日志
category: essay 
---

# Blog维护日志

2020.1.1 文章数达到350篇。添加Recommender system分类。

2019.11 添了一个National Taiwan University的follower。

2019.10.9 将RL分类拆分为RL和DRL两类。

2019.7 添加Attention、DL acceleration分类。

2019.6.19 文章数达到300篇。可命名为v4.5。

2019.4.26 img2的图片太多，特添加img3。

2019.3.27 NLP参考资源已经积攒了很多，特进行细分类，使之更有条理。

2019.3.19 添加GAN & VAE分类。

2018.11.19 添加RL分类。

2018.11.16 文章数达到250篇。一个分类达到50篇，可读性的问题就比较大了，修订也比较麻烦。特创建Deep object detection分类。由于文章较多，新分类的迁移将缓慢进行。可命名为v4.0。

2018.11.14 DL专栏文章达到50篇。

2018.8.1 在知名公众号上看到自己的文章。里程碑啊！

2018.7.26 近来blog相关的事情比较多。

1.CSDN编辑拉我进高级作者微信群。另一不知名小编居然和我约稿。。。太瞧得起我的水平了。。。

2.添了一个Google的follower，居然还是tensorflow的committer，就连做的工作都和我类似。。。

3.添了一个Azerbaijan的follower，有没有搞错啊！你确定你看得懂中文吗？

2018.7.19 添加Speech分类。

2018.2.22 文章数达到200篇。

2018.2.1 又添了几个清华、头条、商汤的follower。其中有个人，我俩几乎同时关注了对方，挺有缘分的。此外，居然还有人把我的blog给fork了。。。

2018.1.10 近来转战DL以后，blog的产量日增，也渐有重量级的follower：几个中科院、浙大的硕士。并收到了第一笔打赏。

2017.12.21 blog图片太多，特添加第2个图片文件夹。

2017.10.22 文章分为Language、Linux、Technology、Essay、Resource、Graphics、AI、ML、Math、DL十类。可命名为v3.5。

2017.10.18 文章数达到150篇。

2017.5.17 修改标题的css。可命名为v3.0。

2017.4.5 添加微信打赏二维码图片。

2017.3.22 评论系统切换到Github Issue。

2017.3.4 文章数达到100篇。

2016.11.3 文章分为Technology、Theory、Essay三类。

2016.8.1 添加blog数量的统计信息，将MathJax升级到2.6.1。

2016.5.12 为了广而告之，特将blog登录CSDN。但由于编辑方式的差异，Github仍为主站，内容较新较全。

2016.4.12 将MathJax等资源切换为国内的cdn。

2016.4.10 首页将技术文章和非技术文章分栏显示。

2016.3.30 白底黑字太晃眼，特切换为浅灰底色。

2016.3.18 评论系统切换到多说。

2016.2.22 文章数达到50篇。

2016.2.2 代码区的配色实在看起来费眼，屡次微调，仍不满意。这次借鉴原blog（也就是大徐抄袭的源头）的配色方案，最终达到基本满意的效果。可命名为v2.0。

2015.1.7 blog搬家基本完成。

2014.12.23 围观大徐的blog有感，特在github上安家落户，废弃了原来的搜狐blog。

# Inverse Reinforcement Learning（续）

通俗的说法就是：**我们用优化控制或强化学习得到的策略能用来解释人类的行为吗？**

比如让一个人去拿桌子上的一个橘子，那手的轨迹一定不是一条从起点到目标的直线，而是有一些弯曲的轨迹，也就是带有偏差的较优行为，但是这种偏差其实并不重要，只要最后拿到橘子就行了，也就是说：

1.有过程中一些偏差是不重要的，但另一些偏差就比较重要了（如最后没拿到）。

2.每次拿橘子的动作也是不一样的，因此人的动作带有一定的随机性。

3.我们可以认为人类行为轨迹分布是以最优策略为峰的随机分布。

![](/images/img3/GAN_vs_IRL.png)

参考：

https://zhuanlan.zhihu.com/p/61674994

Algorithms for Inverse Reinforcement Learning

https://zhuanlan.zhihu.com/p/32502503

CS 294：IRL

# 模仿学习

https://zhuanlan.zhihu.com/p/27935902

机器人学习Robot Learning之模仿学习Imitation Learning的发展

https://zhuanlan.zhihu.com/p/25688750

模仿学习（Imitation Learning）完全介绍

https://mp.weixin.qq.com/s/naq73D27vsCOUBperKto8A

从监督式到DAgger，综述论文描绘模仿学习全貌

https://mp.weixin.qq.com/s/LNNqp2KsEAljG26hY43mUw

ICML2018 模仿学习教程

https://mp.weixin.qq.com/s/f9vSgH1HQwGXBDb0UGHQyQ

深度学习进阶之无人车行为克隆

https://mp.weixin.qq.com/s/To3pnx1hVq_4p7UnQVMw9A

斯坦福大学&DeepMind联合提出机器人控制新方法，RL+IL端到端地学习视觉运动策略

https://mp.weixin.qq.com/s/O0Q1XoTA-7Yshr1ZqOZ90w

加州理工：什么是模仿学习, 这62页ppt带你了解进展

# RL与神经科学

Pavlov Model（1901）

Rescorla-Wagner Model（1972）

Thorndike’s Puzzle Box（1910）

参考：

https://zhuanlan.zhihu.com/p/24437724

学习理论之Rescorla-Wagner模型

# RL参考资源

https://mp.weixin.qq.com/s/Fn1s9Ia8L1ckgn6iP24FhQ

如何让神经网络具有好奇心

https://mp.weixin.qq.com/s/PBf-YrkhwhPYXuiOGyahxQ

强化学习遭遇瓶颈！分层RL将成为突破的希望

https://zhuanlan.zhihu.com/p/58815288

强化学习之值函数学习

https://mp.weixin.qq.com/s/8Cqknze_iosz6Z6cqnuK5w

谷歌提出强化学习新算法SimPLe，模拟策略学习效率提高2倍

https://mp.weixin.qq.com/s/hKGS4Ek5prwTRJoMCaxQLA

强化学习Exploration漫游

https://zhuanlan.zhihu.com/p/65116688

值分布强化学习（01）

https://zhuanlan.zhihu.com/p/65364484

值分布强化学习（02）

https://zhuanlan.zhihu.com/p/62363784

强化学习之策略搜索

https://mp.weixin.qq.com/s/j9Cs5M9gyITu2u_XDkKm-g

Policy Gradient——一种不以loss来反向传播的策略梯度方法

https://mp.weixin.qq.com/s/x6gKTuYIx8y25KX-fCc5bA

蒙特卡洛梯度估计方法（MCGE）简述
