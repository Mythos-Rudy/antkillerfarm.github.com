---
layout: post
title: Blog维护日志
category: essay 
---

# Blog维护日志

2019.10.9 将RL分类拆分为RL和DRL两类。

2019.7 添加Attention、DL acceleration分类。

2019.6.19 文章数达到300篇。可命名为v4.5。

2019.4.26 img2的图片太多，特添加img3。

2019.3.27 NLP参考资源已经积攒了很多，特进行细分类，使之更有条理。

2019.3.19 添加GAN & VAE分类。

2018.11.19 添加RL分类。

2018.11.16 文章数达到250篇。一个分类达到50篇，可读性的问题就比较大了，修订也比较麻烦。特创建Deep object detection分类。由于文章较多，新分类的迁移将缓慢进行。可命名为v4.0。

2018.11.14 DL专栏文章达到50篇。

2018.8.1 在知名公众号上看到自己的文章。里程碑啊！

2018.7.26 近来blog相关的事情比较多。

1.CSDN编辑拉我进高级作者微信群。另一不知名小编居然和我约稿。。。太瞧得起我的水平了。。。

2.添了一个Google的follower，居然还是tensorflow的committer，就连做的工作都和我类似。。。

3.添了一个Azerbaijan的follower，有没有搞错啊！你确定你看得懂中文吗？

2018.7.19 添加Speech分类。

2018.2.22 文章数达到200篇。

2018.2.1 又添了几个清华、头条、商汤的follower。其中有个人，我俩几乎同时关注了对方，挺有缘分的。此外，居然还有人把我的blog给fork了。。。

2018.1.10 近来转战DL以后，blog的产量日增，也渐有重量级的follower：几个中科院、浙大的硕士。并收到了第一笔打赏。

2017.12.21 blog图片太多，特添加第2个图片文件夹。

2017.10.22 文章分为Language、Linux、Technology、Essay、Resource、Graphics、AI、ML、Math、DL十类。可命名为v3.5。

2017.10.18 文章数达到150篇。

2017.5.17 修改标题的css。可命名为v3.0。

2017.4.5 添加微信打赏二维码图片。

2017.3.22 评论系统切换到Github Issue。

2017.3.4 文章数达到100篇。

2016.11.3 文章分为Technology、Theory、Essay三类。

2016.8.1 添加blog数量的统计信息，将MathJax升级到2.6.1。

2016.5.12 为了广而告之，特将blog登录CSDN。但由于编辑方式的差异，Github仍为主站，内容较新较全。

2016.4.12 将MathJax等资源切换为国内的cdn。

2016.4.10 首页将技术文章和非技术文章分栏显示。

2016.3.30 白底黑字太晃眼，特切换为浅灰底色。

2016.3.18 评论系统切换到多说。

2016.2.22 文章数达到50篇。

2016.2.2 代码区的配色实在看起来费眼，屡次微调，仍不满意。这次借鉴原blog（也就是大徐抄袭的源头）的配色方案，最终达到基本满意的效果。可命名为v2.0。

2015.1.7 blog搬家基本完成。

2014.12.23 围观大徐的blog有感，特在github上安家落户，废弃了原来的搜狐blog。

# 博弈论

## 最大最小策略（续）

重复剔除不仅要求每个人是理性的，而且要求每个人知道其他人都是理性的，每个人知道每个人知道每个人是理性的，如此等等，即理性是“共识”。

**优势策略均衡与纳什均衡的概念是建立在博弈者理性行为的基础上的**，任何出现的一点错误将可能使博弈者蒙受巨大的损失，因而可能有player会采取比较保守的策略。

其中一种保守的策略是最大最小策略（Maximin strategy)。

它是指博弈者所采取的策略是使自己能够获得的最小收入最大化。所谓最小收入是指采取某种策略所能获得的最小收入。

最大最小策略是一种保守的策略而不是利润最大化的策略。

很显然，博弈者往往是在信息不完全的情况下才采取最大最小策略。在信息完全的情形下，他肯定是会采取促使他利润最大化的策略。

**在囚徒困境问题中，两人都坦白是最大最小策略，而两人都抵赖则是利润最大化策略。**

最大最小均衡存在以下问题：

- “最大最小”均衡没有考虑到局中人之间在策略选择上的互动。

- 由“最大最小”方法得到的“均衡”很难说是一种“均衡”。

## 其他博弈问题

当年英国政府将流放澳洲的犯人交给往来于澳洲之间的商船来完成，由此经常会发生因商船主或水手虐待犯人，致使大批流放人员因此死在途中(葬身大海)的事件发生。

后来大英帝国对运送犯人的办法(制度)稍加改变，流放人员仍然由往来于澳洲的商船来运送，只是运送犯人的费用要等到犯人送到澳洲后才由政府按实到犯人人数支付给商船。

仅就这样一点小小的“改变”，几乎再也没有犯人于中途死掉的事情发生。

----

枪手博弈是指，在三个枪手A、B、C之间一场对决即将展开，枪手A的枪法最好，命中率达到80%；枪手B的枪法次之命中率60%；而枪手C的命中率最差只有50%。

而此时由于是三方对决，先瞄准谁成了关键，对于A枪手来说，当然先瞄准仅次于他的B枪手，但对于B和C枪手而言，威胁最大的当然是A，在A倒下后B面对C的胜率会大很多，同时C的存活率也会提高，所以枪口都会对上A。

结论：

- 第一轮枪战，枪法最差的C竟然存活概率最大——肯定存活，而枪法好的A和B存活概率远低于C。启示：韬晦很重要。

- 如果在第一轮枪战中A、B均被击中，则C成为最终幸存者；只要A、B在第一轮枪战中有一人存活，那最终胜出的很可能是A和B中的幸存者。启示：实力很关键。能力较差的C靠着策略虽然能在第一轮枪战中暂时获胜，但只要A、B在第一轮枪战中有一人存活，那么第二轮枪战里C的存活的概率就会比对手低了。

## 参考

https://www.cnblogs.com/steven-yang/tag/博弈论/

一个博弈论的专栏

https://mp.weixin.qq.com/s/5o3m8RLwYkZJEhqNxOLq_A

不对称多代理博弈中的博弈理论解读

https://mp.weixin.qq.com/s/D9bRjYVJOMT0Jkh59XZ-Rg

DeepMind于Nature子刊发文提出非对称博弈的降维方法

https://mp.weixin.qq.com/s/1t6WuTQpltMtP-SRF1rT4g

当博弈论遇上机器学习：一文读懂相关理论

https://news.mbalib.com/story/242878

智猪博弈、龟兔悖论、谷堆悖论...这些有趣的博弈论值得一看！

https://blog.csdn.net/qq_27351341/article/details/81119533

偏好函数、无差异曲线、帕累托标准、卡尔多-希克斯标准等基础概念

https://blog.csdn.net/qq_27351341/article/details/81138774

囚徒困境、智猪博弈、纳什均衡与一致预期

https://blog.csdn.net/qq_27351341/article/details/81268801

多重均衡与制度和文化

https://blog.csdn.net/qq_27351341/article/details/81276298

动态博弈、威胁与承诺

https://blog.csdn.net/sobermineded/article/details/79541511

几个经典博弈模型（囚徒的困境、赌胜博弈、产量决策的古诺模型）

# 模仿学习

https://zhuanlan.zhihu.com/p/27935902

机器人学习Robot Learning之模仿学习Imitation Learning的发展

https://zhuanlan.zhihu.com/p/25688750

模仿学习（Imitation Learning）完全介绍

https://mp.weixin.qq.com/s/naq73D27vsCOUBperKto8A

从监督式到DAgger，综述论文描绘模仿学习全貌

https://mp.weixin.qq.com/s/LNNqp2KsEAljG26hY43mUw

ICML2018 模仿学习教程

https://mp.weixin.qq.com/s/f9vSgH1HQwGXBDb0UGHQyQ

深度学习进阶之无人车行为克隆

https://mp.weixin.qq.com/s/To3pnx1hVq_4p7UnQVMw9A

斯坦福大学&DeepMind联合提出机器人控制新方法，RL+IL端到端地学习视觉运动策略

# RL与神经科学

Pavlov Model（1901）

Rescorla-Wagner Model（1972）

Thorndike’s Puzzle Box（1910）

参考：

https://zhuanlan.zhihu.com/p/24437724

学习理论之Rescorla-Wagner模型

# RL参考资源+

https://mp.weixin.qq.com/s/Fn1s9Ia8L1ckgn6iP24FhQ

如何让神经网络具有好奇心

https://mp.weixin.qq.com/s/PBf-YrkhwhPYXuiOGyahxQ

强化学习遭遇瓶颈！分层RL将成为突破的希望

https://zhuanlan.zhihu.com/p/58815288

强化学习之值函数学习

https://mp.weixin.qq.com/s/8Cqknze_iosz6Z6cqnuK5w

谷歌提出强化学习新算法SimPLe，模拟策略学习效率提高2倍

https://mp.weixin.qq.com/s/hKGS4Ek5prwTRJoMCaxQLA

强化学习Exploration漫游

https://zhuanlan.zhihu.com/p/65116688

值分布强化学习（01）

https://zhuanlan.zhihu.com/p/65364484

值分布强化学习（02）

https://zhuanlan.zhihu.com/p/62363784

强化学习之策略搜索

https://mp.weixin.qq.com/s/j9Cs5M9gyITu2u_XDkKm-g

Policy Gradient——一种不以loss来反向传播的策略梯度方法

https://mp.weixin.qq.com/s/x6gKTuYIx8y25KX-fCc5bA

蒙特卡洛梯度估计方法（MCGE）简述
