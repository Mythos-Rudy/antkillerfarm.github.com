---
layout: post
title:  强化学习（四）——Temporal-Difference Learning
category: RL 
---

# Monte-Carlo（续）

## 拉斯维加斯（Las Vegas）算法

假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯算法——**尽量找最好的，但不保证能找到**。

比如N皇后的排列问题，除了顺序枚举法之外，随机枚举也是一种策略。

## Monte Carlo method与RL

MDP的缺点在于model是已知的，但在实际应用中，更多的是Model未知（或部分未知）或者建模困难的情况，这种情况下就需要使用MC method来生成相应的Model。

MC method在RL中主要有两种使用方式：

model-free：完全不依赖Model。

Simulated：简单的模拟，而不需要完整的Model。

MC method用experience替代了MDP中的transitions/rewards（也可以说是用empirical mean替代了expected），但需要注意这些experience不能是重复采样的，而且它只适用于周期性的MDP。

## Monte Carlo Policy Evaluation

Monte Carlo Policy Evaluation的目标是对状态s进行估值。它的步骤是：

>当s被访问到(visited)时:   
>>增加计数：$$N(s)\leftarrow N(s) + 1$$   
>>增加总奖励：$$S(s)\leftarrow S(s) + G_t$$   
>>$$V(s) = S(s)/N(s)$$   
>反复多次：$$N(s)\to \infty,V(s)\to v_{\pi}(s)$$

根据Visit的策略不同，Monte Carlo Policy Evaluation又可分为：First-visit MC和Every-Visit MC。

两者的差别在于：First-visit MC的每次探索，一旦抵达状态s，就结束了，Every-Visit MC到达状态s之后还可以继续探索。

## Incremental Monte-Carlo Updates

借用《强化学习（二）》中，求均值的小技巧，我们可以得到Incremental Mean。

用Incremental Mean进行更新，被称作Incremental Monte-Carlo Updates：

$$V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))$$

对于非平稳（non-stationary）问题，我们也可采用如下公式更新：

$$V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))$$

## 参考

https://mp.weixin.qq.com/s/F9VlxVV4nXELyKxdRo9RPA

强化学习——蒙特卡洛

https://www.zhihu.com/question/20254139

蒙特卡罗算法是什么？

http://www.cnblogs.com/2010Freeze/archive/2011/09/19/2181016.html

概率算法-sherwood算法

http://www.cnblogs.com/chinazhangjie/archive/2010/11/11/1874924.html

概率算法

https://mp.weixin.qq.com/s/h9S5HZQwQKstDAKth-9Qeg

蒙特卡洛算法(MC)

https://mp.weixin.qq.com/s/wfCyii6bS-GxMZPg2TPaLA

蒙特卡洛树搜索是什么？如何将其用于规划星际飞行？

https://www.zhihu.com/question/39916945

蒙特卡洛树是什么算法？

https://mp.weixin.qq.com/s/tqhPGG2Djl4gnd09RdHsUA

一个彻底改变世界的思想

https://mp.weixin.qq.com/s/vKVX-aJ7n7VVDXOpoTo1GQ

通过Python实现马尔科夫链蒙特卡罗方法的入门级应用

https://mp.weixin.qq.com/s/TMHaIRFdgJxG__1oqRq70Q

蒙特卡罗树搜索之初学者指南

https://mp.weixin.qq.com/s/HlDRI1s16k2k8RDh0GoXcw

详解蒙特卡洛方法：这些数学你搞懂了吗？

https://mp.weixin.qq.com/s/yGQSihfBDcXgqBnvvpQDog

蒙特卡罗方法入门

# Temporal-Difference Learning

## TD

时序差分学习和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习不完整的Episode，通过bootstrapping，猜测Episode的结果，同时持续更新这个猜测。

最简单的TD算法——TD(0)的更新公式如下：

$$V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$$

其中，$$R_{t+1}+\gamma V(S_{t+1})$$被称作TD target，而$$\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$$被称作TD error。

下面用驾车返家的例子直观解释蒙特卡洛策略评估和TD策略评估的差别。

想象一下你下班后开车回家，需要预估整个行程花费的时间。假如一个人在驾车回家的路上突然碰到险情：对面迎来一辆车感觉要和你相撞，严重的话他可能面临死亡威胁，但是最后双方都采取了措施没有实际发生碰撞。

如果使用蒙特卡洛学习，路上发生的这一险情可能引发的负向奖励不会被考虑进去，不会影响总的预测耗时；

但是在TD学习时，碰到这样的险情，这个人会立即更新这个状态的价值，随后会发现这比之前的状态要糟糕，会立即考虑决策降低速度赢得时间，也就是说你不必像蒙特卡洛学习那样直到他死亡后才更新状态价值。实际上那种情况下，也无法更新状态价值了。。。囧

TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。

| 状态 | 已消耗时间 | 预计仍需耗时 | 预测总耗时 |
|:--:|:--:|:--:|:--:|
| 离开办公室 | 0 | 30 | 30 |
| 取车，发现下雨 | 5 | 35 | 40 |
| 离开高速公路 | 20 | 15 | 35 |
| 被迫跟在卡车后面 | 30 | 10 | 40 |
| 到达家所在街区 | 40 | 3 | 43 |
| 到家 | 43 | 0 | 43 |

基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来更新价值函数（各个状态的价值）。这里使用的是从某个状态预估的到家还需耗时来间接反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。

对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策。

而对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。

![](/images/img2/TD.png)

## TD vs. MC---1

通过这个例子，我们可以直观的了解到：

1.TD在知道结果之前可以学习，MC必须等到最后结果才能学习；

2.TD可以在没有结果时学习，可以在持续进行的环境里学习。

## TD vs. MC---2

$$G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T−1}R_T$$是$$V_{\pi}(S_t)$$的无偏估计值。

True TD target：$$R_{t+1}+\gamma V_{\pi}(S_{t+1})$$，也是$$V_{\pi}(S_t)$$的无偏估计值。

TD target：$$R_{t+1}+\gamma V(S_{t+1})$$，是$$V_{\pi}(S_t)$$的有偏估计值。

MC没有bias，但有着较高的Variance，且对初始值不敏感；

TD低variance, 但有一定程度的bias，对初始值较敏感，通常比MC更高效；

## TD vs. MC---3

再来看如下示例：

现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。

| Episode | 状态转移及奖励 |
|:--:|:--:|
| 1 | A:0,B:0 |
| 2 | B:1 |
| 3 | B:1 |
| 4 | B:1 |
| 5 | B:1 |
| 6 | B:1 |
| 7 | B:1 |
| 8 | B:0 |

问题：依据仅有的Episode，计算状态A，B的价值分别是多少，即V(A)=？， V(B)=？

答案：V(B) = 6/8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6/8。

解释：应用MC算法，由于需要完整的Episode,因此仅Episode1可以用来计算A的状态价值，很明显是0；同时B的价值是6/8。应用TD算法时，TD算法试图利用现有的Episode经验构建一个MDP（图见下节），由于存在一个Episode使得状态A有后继状态B，因此状态A的价值是通过状态B的价值来计算的，同时经验表明A到B的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于B的状态价值。

MC算法试图收敛至一个能够最小化状态价值与实际收获的均方差的解决方案。TD算法则收敛至一个根据已有经验构建的最大可能的Markov模型的状态价值。

通过比较可以看出，TD算法使用了MDP问题的Markov属性，在Markov环境下更有效；而MC算法并不利用Markov属性，通常在非Markov环境下更有效。

## DP & MC & TD

Monte-Carlo, Temporal-Difference和Dynamic Programming都是计算状态价值的一种方法，区别在于：

- 前两种是在不知道Model的情况下的常用方法，这其中MC方法需要一个完整的Episode来更新状态价值，而TD则不需要完整的Episode；DP方法则是基于Model（知道模型的运作方式）的计算状态价值的方法，它通过计算一个状态S所有可能的转移状态S’及其转移概率以及对应的即时奖励来计算这个状态S的价值。

- 关于是否Bootstrap：MC没有bootstrapping，只使用实际收获；DP和TD都有bootstrapping。

- 关于是否用采样来计算: MC和TD都是应用样本来估计实际的价值函数；而DP则是利用模型直接计算得到实际价值函数，没有采样之说。

![](/images/img2/DP_MC_TD.png)

上图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：采样深度和广度。

- 当使用单个采样，同时不走完整个Episode就是TD；

- 当使用单个采样但走完整个Episode就是MC；

- 当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP；

- 当既考虑所有样本，又把Episode从开始到终止遍历完，就变成了穷举法。

需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。
