---
layout: post
title:  Attention（四）——Attention进阶
category: Attention 
---

# BERT

## 参考（续）

https://mp.weixin.qq.com/s/HXYDO5PM8UIoXgEPGe8p-w

图解当前最强语言模型BERT：NLP是如何攻克迁移学习的？

https://mp.weixin.qq.com/s/zz3j9HEuzw5e92MQXxSQsA

遗珠之作？谷歌Quoc Le这篇NLP预训练模型论文值得一看

https://mp.weixin.qq.com/s/IN4YfoZnlBozwEFdhSvLZg

用可视化解构BERT，我们从上亿参数中提取出了6种直观模式

https://mp.weixin.qq.com/s/nIT3GIU0dUIYyGChxsiOWw

Google BERT应用之《红楼梦》对话人物提取

https://mp.weixin.qq.com/s/dcp_ANYijRmicMYX7OpJmA

如何用最强模型BERT做NLP迁移学习？

https://mp.weixin.qq.com/s/DR4SkgOfUT7KYiaXm5NynQ

跨语言版BERT：Facebook提出跨语言预训练模型XLM

https://mp.weixin.qq.com/s/epjjHmlmMFhWtRO_cCUITA

用BERT进行多标签文本分类

https://mp.weixin.qq.com/s/Wk6gvOS_Qnud6ib1esMFXA

加入Transformer-XL，这个PyTorch包能调用各种NLP预训练模型！

https://mp.weixin.qq.com/s/GqqU3Ixht1BzMnQeRYQEqQ

谷歌NLP深度学习模型BERT特征的可解释性表现怎么样？

https://mp.weixin.qq.com/s/2f91Ksj19rk_emoFpEmPfA

从BERT看大规模数据的无监督利用

https://mp.weixin.qq.com/s/g6-NjoFMPpxjsh38X-wTFQ

BERT，GPT-2这些顶尖工具到底该怎么用到我的模型里?

https://mp.weixin.qq.com/s/N6xBFZ82dkSGCbj6vC5nLQ

上下文预训练模型最全整理：原理、应用、开源代码、数据分享

https://mp.weixin.qq.com/s/-6XpuO7_ve_EdSPCMeWE7g

Attention isn’t all you need！BERT的力量之源远不止注意力

https://mp.weixin.qq.com/s/Y2bs2QegRadSR7lbiFFnWg

BERT一作Jacob Devlin斯坦福演讲PPT：BERT介绍与答疑

https://zhuanlan.zhihu.com/p/58425003

从Word2Vec到Bert，聊聊词向量的前世今生（一）

https://mp.weixin.qq.com/s/SfMIKfF_B4agFCHN_U_mzQ

BAM！利用知识蒸馏和多任务学习构建的通用语言模型

https://mp.weixin.qq.com/s/6G5Mu7-1omGtQ_9Gt9lUBw

基于预训练自然语言生成的文本摘要方法

https://mp.weixin.qq.com/s/yysnPauB22YgprpOi1ZWSQ

深入理解BERT Transformer，不仅仅是注意力机制

https://mp.weixin.qq.com/s/kFABJJ3fBC48-4DXK8PERQ

10大任务超越BERT，微软提出多任务深度神经网络MT-DNN

https://mp.weixin.qq.com/s/jlGfxkT_o9sgFlUuR_x5Tw

微软开源用于学习通用语言嵌入的MT-DNN模型

https://mp.weixin.qq.com/s/D68YzjYvpc2epGWFBP6rIQ

谷歌实习生新算法提速惊人！BERT训练从三天三夜，缩短到一个小时

https://mp.weixin.qq.com/s/iDGofh_ycWJzfqQriPEXGQ

如何用Python和BERT做中文文本二元分类？

https://zhuanlan.zhihu.com/p/62308732

浅谈Bert：语言理解中的预训练编码器

https://mp.weixin.qq.com/s/1Cz6js4kYdvc8g4oKjVPeA

BERT烹饪之法：fintune的艺术

https://mp.weixin.qq.com/s/nVM2Kxc_Mn7BAC6-Pig2Uw

BERT模型的标准调优和花式调优

https://mp.weixin.qq.com/s/FwmEIZ3ugeZBbLIGHmH-_g

BERT之后，GLUE基准升级为SuperGLUE：难度更大

https://mp.weixin.qq.com/s/SDVxn3Ra1dKmr-XgKNg5IA

罗玲：From Word Representation to BERT

https://mp.weixin.qq.com/s/-bh8BL4LxnevS8xnW5U9ZA

中科院自动化所提出BIFT模型：面向自然语言生成，同步双向推断

https://mp.weixin.qq.com/s/7yCnAHk6x0ICtEwBKxXpOw

序列到序列自然语言生成任务超越BERT、GPT！微软提出通用预训练模型MASS

https://mp.weixin.qq.com/s/7sIUaSON53hsXUJjq8uVUA

马聪：NLP中的生成式预训练模型

https://mp.weixin.qq.com/s/s1bQFdA6gtoHeeQMJKQ8UQ

Bert时代的创新：Bert应用模式比较及其它

https://mp.weixin.qq.com/s/zqlWx3e4LOJ3_Zy2DEbCjw

从语言模型看Bert的善变与GPT的坚守

https://mp.weixin.qq.com/s/LngE10Hnqe9bgFzpNfUwLQ

NLP中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert

https://mp.weixin.qq.com/s/MgLLPEY3ynJGkuTgnIXndQ

站在BERT肩膀上的NLP新秀们（PART I）

https://mp.weixin.qq.com/s/RjeuHXa8O3MzSpTOuOHMkQ

站在BERT肩膀上的NLP新秀们：XLMs、MASS和UNILM

https://mp.weixin.qq.com/s/yPq1cGnhcbaNLOjadj91pw

Bert时代的创新：Bert在NLP各领域的应用进展

https://mp.weixin.qq.com/s/l-de0vfx-L24g58IxK-NKQ

Jeff Dean强推：可视化Bert网络，发掘其中的语言、语法树与几何学

https://mp.weixin.qq.com/s/nlFXfgM5KKZXnPdwd97JYg

哈工大讯飞联合实验室发布基于全词覆盖的中文BERT预训练模型

https://zhuanlan.zhihu.com/p/70389596

一批高质量中文BERT预训练模型请查收（上）

https://mp.weixin.qq.com/s/h1VUSY7_UZF3PmjSN0DMSg

从One-hot, Word embedding到Transformer，一步步教你理解Bert

https://mp.weixin.qq.com/s/UJlmjFHWhnlXXJoRv4zkEQ

虽被BERT碾压，但还是有必要谈谈BERT时代与后时代的NLP

https://mp.weixin.qq.com/s/e4dgIdwzDzcLSkdgr1yZpg

LeCun力荐：Facebook推出十亿参数超大容量存储器

https://mp.weixin.qq.com/s/zXXtbuSvyMOkgrWJwB83kg

预训练语言模型的最新探索

https://mp.weixin.qq.com/s/WzGa5XVi2Op4Lz-1uQXfxQ

SpanBERT：提出基于分词的预训练模型，多项任务性能超越现有模型！

https://zhuanlan.zhihu.com/p/76912493

nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)

https://mp.weixin.qq.com/s/pYSs6NhIAB6DuwNnKZhkZQ

Bert改进：如何融入知识

https://mp.weixin.qq.com/s/in5SDWlQg8ts4E8DTmHxMQ

BERT在推荐系统领域可能会有什么作为？

https://mp.weixin.qq.com/s/kJhOrz0VaYc-k-6XJS02ag

8篇论文梳理BERT相关模型进展与反思

https://mp.weixin.qq.com/s/-eBL9gFbAGFtmqkLMAoUTw

文本+视觉，多篇Visual/Video BERT论文介绍

https://mp.weixin.qq.com/s/hI9XAiqKaHLq-Z9JkaWA_A

解决自然语言歧义问题，斯坦福教授、IJCAI卓越研究奖得主提出SenseBERT模型

https://mp.weixin.qq.com/s/55B0ToIKDusiPI5farR19w

NLP这两年：15个预训练模型对比分析与剖析

https://mp.weixin.qq.com/s/SPfa17p3QetZXCC01DwmQA

解密BERT

https://zhuanlan.zhihu.com/p/72805778

BERT的演进和应用

https://mp.weixin.qq.com/s/9YuBY0wLLVQ8ZrT9fiNICA

语音版BERT？滴滴提出无监督预训练模型，中文识别性能提升10%以上

https://mp.weixin.qq.com/s/OXkXjPHhaMXsKw2YevV6sw

邱锡鹏：从Transformer到BERT--自然语言处理中的表示学习进展

https://mp.weixin.qq.com/s/6i9eQISKsWU0jawKzWg8nQ

超越bert，最新预训练模型ELECTRA论文阅读笔记

https://zhuanlan.zhihu.com/p/91052495

当BERT遇上知识图谱

https://mp.weixin.qq.com/s/wQW-JT-sGMj60OtXwTssyQ

BERT模型推理加速总结

https://mp.weixin.qq.com/s/HOD1Hb70NhTXXCXlopzfng

BERT推理加速实践

# Attention进阶

https://mp.weixin.qq.com/s/ZSzHOu6uowRSoWrqB7vOaQ

深度学习注意力机制-Attention in Deep learning-附101页PPT

https://mp.weixin.qq.com/s/_mBa-GTdILrvluJtegz8fw

南洋理工大学：注意力神经网络，Attention Neural Networks，78页ppt

https://mp.weixin.qq.com/s/y_hIhdJ1EN7D3p2PVaoZwA

阿里北大提出新attention建模框架，一个模型预测多种行为

https://mp.weixin.qq.com/s/Yq3S4WrsQRQC06GvRgGjTQ

打入神经网络思维内部

https://mp.weixin.qq.com/s/MJ1578NdTKbjU-j3Uuo9Ww

基于文档级问答任务的新注意力模型

https://mp.weixin.qq.com/s/C4f0N_bVWU9YPY34t-HAEA

UNC&Adobe提出模块化注意力模型MAttNet，解决指示表达的理解问题

https://mp.weixin.qq.com/s/V3brXuey7Gear0f_KAdq2A

基于注意力机制的交易上下文感知推荐，悉尼科技大学和电子科技大学最新工作

http://mp.weixin.qq.com/s/Bt6EMD4opHCnRoHKYitsUA

结合人类视觉注意力进行图像分类

https://mp.weixin.qq.com/s/POYTh4Jf7HttxoLhrHZQhw

基于双向注意力机制视觉问答pyTorch实现

https://mp.weixin.qq.com/s/2gxp7A38epQWoy7wK8Nl6A

谷歌翻译最新突破，“关注机制”让机器读懂词与词的联系

https://zhuanlan.zhihu.com/p/25928551

用深度学习（CNN RNN Attention）解决大规模文本分类问题-综述和实践

http://blog.csdn.net/leo_xu06/article/details/53491400

视觉注意力的循环神经网络模型

https://mp.weixin.qq.com/s/l4HN0_VzaiO-DwtNp9cLVA

循环注意力区域实现图像多标签分类

https://mp.weixin.qq.com/s/zhZLK4pgJzQXN49YkYnSjA

自适应注意力机制在Image Caption中的应用

https://mp.weixin.qq.com/s/uvr-G5-_lKpyfyn5g7ES0w

基于注意力机制，机器之心带你理解与训练神经机器翻译系统

https://mp.weixin.qq.com/s/ANpBFnsLXTIiW6WHzGrv2g

自注意力机制学习句子embedding

https://mp.weixin.qq.com/s/49fQX8yiOIwDyof3PD01rA

CMU&谷歌大脑提出新型问答模型QANet：仅使用卷积和自注意力，性能大大优于RNN

https://mp.weixin.qq.com/s/c64XucML13OwI26_UE9xDQ

滴滴披露语音识别新进展：基于Attention显著提升中文识别率

https://mp.weixin.qq.com/s/7OYY3L7gL4wVv_EjoosOHA

如何增强Attention Model的推理能力

https://mp.weixin.qq.com/s/9Kt6_DfeYRnhsb10aCSFGw

FAGAN：完全注意力机制（Full Attention）GAN，Self-attention+GAN

https://mp.weixin.qq.com/s/lZOIK5BRXZrmL_Z9crl6sA

机器翻译新突破！“普适注意力”模型：概念简单参数少，性能大增

https://mp.weixin.qq.com/s/JoTzaInn_uAA9oZgMcfskw

计算机视觉技术self-attention最新进展

https://mp.weixin.qq.com/s/jRfOzKO6OlQLokIzipbqUQ

为什么使用自注意力机制？

https://mp.weixin.qq.com/s/_3pA8FZwzegSpyz_cK63BQ

Self-Attention GAN中的self-attention机制

https://mp.weixin.qq.com/s/h7sLwVXb_UI8jvJU-oe3Cg

Google AI提出“透明注意力”机制，实现更深层NMT模型

https://mp.weixin.qq.com/s/1LYz5SH5rVnPPJ0tZvRQAA

从各种注意力机制窥探深度学习在NLP中的神威

https://zhuanlan.zhihu.com/p/32928645

计算机视觉中的注意力机制

https://zhuanlan.zhihu.com/p/56501461

计算机视觉中的注意力机制

https://zhuanlan.zhihu.com/p/33078323

数字串识别：基于位置的硬性注意力机制

https://zhuanlan.zhihu.com/p/32971586

图像描述：基于项的注意力机制

https://zhuanlan.zhihu.com/p/33158614

图像识别：基于位置的柔性注意力机制

https://mp.weixin.qq.com/s/tVKEJ9rqlMaZ9bx6ngIelw

自注意力机制在计算机视觉中的应用

https://mp.weixin.qq.com/s/6q2q9OVhOYjk4ZrhLvAdkA

Non-local Neural Networks及自注意力机制思考

https://mp.weixin.qq.com/s/-gAISWjSiG6ccPuOPAEg3A

五张动图，看清神经机器翻译里的Attention！

https://mp.weixin.qq.com/s/Qs6tm50YvzHaJv2rh60WMw

撩一发深度文本分类之RNN via Attention
