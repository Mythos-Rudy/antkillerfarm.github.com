---
layout: post
title:  数学狂想曲（六）——自相关&互相关&卷积, 马氏距离
category: math 
---

# 概率分布（2）

## 一元线性回归的显著性检验

假设y关于x的回归具有形式$$a+bx$$，则$$H_0:b=0$$。

这里使用t检验法进行假设检验。

首先，不加证明的给出如下结论：

**推论1**：$$\overline y\sim N(a+b\overline x,\sigma^2/n)$$

**推论2**：$$\hat b\sim N(b,\sigma^2/S_{xx})$$

**推论3**：$$\hat y_0=\hat a+\hat b x_0=\overline y+\hat b(x_0-\overline x)\sim N\left(a+bx_0,\left[\frac{1}{n}+\frac{(x_0-\overline x)^2}{S_{xx}}\right]\sigma^2\right)$$

**推论4**：$$Q_e/\sigma^2\sim \chi^2(n-2)$$

**推论5**：$$\overline y,\hat b,Q_e$$相互独立。

**推论6**：若$$y_0=a+bx_0+\epsilon_0$$与$$y_1,\dots,y_n$$独立，则$$y_0,\hat y_0,Q_e$$相互独立。

其中，$$\overline y$$表示y的均值，而$$\hat y$$表示y的估计值,$$S_{xx}$$表示方差，$$Q_e$$为残差平方和$$\sum_{i=1}^n(y_i-\hat y_i)^2$$。

由推论4可得：

$$E(Q_e/\sigma^2)=n-2$$

即：

$$Q_e=\hat\sigma^2(n-2)\tag{4}$$

由推论2和5、公式2和4，可得：

$$\frac{\hat b-b}{\sqrt{\sigma^2/S_{xx}}}\bigg /\sqrt{\frac{(n-2)\hat \sigma^2}{\sigma^2}\bigg /(n-2)}\sim t(n-2)$$

即：

$$\frac{\hat b-b}{\hat \sigma}\sqrt{S_{xx}}\sim t(n-2)$$

当假设$$H_0$$被拒绝时，认为回归效果是显著的，反之就认为回归效果不显著。

不显著的原因可能有以下几种：

1.影响y取值的，除了x，还有其他不可忽略因素。

2.y与x的关系不是线性的，存在其他的关系。

3.y与x不存在关系。

上面这些都是正态样本的参数检验。

对于非参数检验或者非正态样本检验，其他的检验方法还有Wilcoxon signed-rank test、Kruskal–Wallis test、Friedman test等。

>注：Frank Wilcoxon，1892～1965，美国化学家。康奈尔大学博士。先后供职于几家美国化工企业的研究机构。

>William Henry ("Bill") Kruskal，1919～2005，美国数学家。哥伦比亚大学博士，芝加哥大学教授。

>Milton Friedman，1912～2006，美国经济学家。哥伦比亚大学博士，芝加哥大学教授。1976年获诺贝尔经济学奖。芝加哥学派第二代的领军人物。

>Wilson Allen Wallis，1912～1998，美国经济学家。先后就读于明尼苏达大学和芝加哥大学，但是没有博士学位。罗彻斯特大学校长。从艾森豪威尔到里根的历届共和党总统的顾问。Milton Friedman的至交。其父Wilson Dallam Wallis为美国人类学家，明尼苏达大学教授。

参考：

https://mp.weixin.qq.com/s/xd2Xw093Wp7O7yQLZHSL4Q

一文学会统计学中的显著性概念

https://mp.weixin.qq.com/s/9-sWv5QDcNYSSYOfuEIcQg

“假设”家族大起底！如何正确区分科学假设、统计假设和机器学习假设？

## KS检验

Kolmogorov–Smirnov test用于对样本是否属于某种分布进行假设检验。

>注：Andrey Nikolaevich Kolmogorov，1903～1987，二十世纪俄国最伟大的数学家之一。莫斯科州立大学博士和教授。俄罗斯科学院院士，挪威科学院和英国皇家学会外籍院士。沃尔夫奖获得者（1980年）。他在数学的许多领域都有重要贡献，以他的名字命名的理论竟有30项之多。

>由于Nobel Prizes没有数学奖，因此数学界的最高奖一般有三个：   
>1.**Fields Medal**。获奖难度最高，因为有40岁的年龄限制。在国内比较知名的丘成桐、陶哲轩都是该奖的获奖者。   
>不过他们还不是最屌的。Grigori Perelman（Poincaré conjecture的证明者）直接拒绝了Fields Medal。除此之外，他还拒绝了EMS Prize和Millennium Prize，其中后者奖金高达100万美元，而且还不知道下一个获奖者什么时候诞生（该奖不是年度奖，而是数学难题奖，数学难题的解决周期，你懂的）。   
>Perelman犹如一个特立独行的隐士，谁的账都不买，包括名利。他将他的伟大证明随手扔进arXiv这样一个非正规网站，但却被《Science》评为年度科学突破。数学界已经很多年没有这样的荣誉了。   
>补充一下，Perelman就读的中学是Kolmogorov创建的。   
>2.**Abel Prize**。和Nobel Prizes的规则相同，由于不限年龄，水平是最高的。缺点是这个奖是2001年才创建的，影响力略差。   
>3.**Wolf Prize**。在Abel Prize创建之前，被誉为数学界的Nobel Prizes。

>Nikolai Vasilyevich Smirnov，1900～1966，俄国数学家。莫斯科大学博士，斯塔克罗夫数学研究所研究员。

>Vladimir Andreevich Steklov，1864～1926，俄国数学家、物理学家。哈尔科夫大学博士，其导师是圣彼得堡学派第二代人物中，仅次于Andrey Markov的Aleksandr Lyapunov。哈尔科夫大学和圣彼得堡大学教授，1919年创建斯塔克罗夫数学研究所。

>斯塔克罗夫数学研究所是一家专职研究没有教学任务和科研任务的研究机构。Grigori Perelman在这里，曾有6年时间没有发表一篇论文。二十世纪俄罗斯绝大多数的数学发现都源自这里。

![](/images/article/KS.png)

上图的红线是某随机变量假设分布的CDF，而蓝线是该随机变量样本的累积分布曲线，即ECDF（Empirical Distribution Function）。

显然若假设正确的话，两条曲线应该是基本重合的。反之，若两条曲线差异较大，则该假设检验不成立。这就是KS检验的基本原理。

KS检验的统计量定义如下：

$$D_n= \sup_x |F_n(x)-F(x)|$$

其中$$\sup$$表示最小上界，

$$F_n(x)={1 \over n}\sum_{i=1}^n I_{[-\infty,x]}(X_i)$$

$$I_{[-\infty,x]}(X_i)=\begin{cases}
1, & X_i \le x \\
0, & \text{otherwise} \\
\end{cases}$$

KS检验更深入的解释，涉及到布朗运动和维纳过程，这里不再赘述。

# 自相关&互相关&卷积

![](/images/article/Comparison_convolution_correlation.svg)

1.**自相关（Autocorrelation）**。这个最简单，就是平移之后，自己和自己比。显然当平移为0的时候，自相关值最大，因此这类操作通常用于信号的检测。信号接收端模拟发射端的信号序列，对实际接收到的信号进行相关操作，只有当两者的序列接近重合时，才会检测到信号峰值。

2.**互相关（Cross-correlation）**。检测两个序列的相似度，显然两者越相似，互相关值越大。这在统计学方面用的比较多。

在信号处理领域，Cross-correlation常用于信号的调制和解调。下图就是常见的正交调制解调（orthogonal modulation and demodulation）的原理图：

![](/images/article/iq_modulator.png)

![](/images/article/iq_demodulator.png)

3.**卷积（Convolution）**。卷积主要用于线性时不变系统的信号处理。相比于互相关操作，卷积有个旋转180度的操作，这里解释一下它的物理意义。

例如，当一个拳击选手遭到对方连续两次击打身体的同一部位时，第二次被击打时他感觉到的疼痛是第一次被击打所遗留的疼痛与第二次被击打的疼痛之和。即：

$$f(2)=f_1(2)+f_2(1)$$

其中，$$f_i(t)$$中，i表示第i次击打，t表示击打发生之后经过的时间。可以看出i和t的顺序正好是相反的，这也就是Convolution这个名词的本意。这里假设g为常数。

4.这三个操作在离散域最终都可以变为求和操作，也就是向量内积运算。我们一般使用$$a\cdot b$$或者$$\langle a,b\rangle$$表示向量的内积运算。即：

$$\langle a,b\rangle=a_0b_0+a_1b_1+\dots+a_nb_n$$

卷积最经典的应用，当属信号处理领域的傅立叶变换：

$$\mathcal{F}\{f * g\} = k\cdot \mathcal{F}\{f\}\cdot \mathcal{F}\{g\}$$

不管怎么组合，**自相关&互相关&卷积实际上都是向量的内积运算**。

除此之外，还有利用**向量张量积运算**的**相关矩阵**。同样的，也有自相关矩阵和互相关矩阵。

4.**循环卷积（Circular convolution）**。

![](/images/img2/Circular_convolution.png)

https://mp.weixin.qq.com/s/R0LTEEYY1jCbSS-f6KlIzw

有限长序列的线性卷积和圆周卷积

# 马氏距离

Mahalanobis Distance是印度现代统计学之父Prasanta Chandra Mahalanobis于1936年提出的概念。

>注：Prasanta Chandra Mahalanobis，1893~1972，印度统计学家，剑桥大学博士，印度统计研究所创始人。   
>印度的重点研究所一般叫做Institute of National Importance，共92所。Indian Statistical Institute是其中唯一一所和统计相关的研究所。教师255，学生375，这得是多精英的教育啊。其计算机科学专业排名印度第2。

p维空间的两点（两个p维向量x,y）的欧氏距离定义为：

$$d_E(x,y)=\sqrt{(x_1-y_1)^2+\dots+(x_p-y_p)^2}=\sqrt{(x-y)^T(x-y)}（公式1）$$

因此，x到原点的距离为：

$$\parallel x\parallel=d_E(x,0)=\sqrt{(x_1)^2+\dots+(x_p)^2}（公式2）$$

也就是：

$$x_1^2+\dots+x_p^2=\parallel x\parallel^2（公式3）$$

这实际上是个正球体的方程，也就是说观测数据x的各个分量对x至中心的欧氏距离贡献是相等的。然而在统计学中我们希望寻求这样一种距离，它的各个分量的作用程度是不同的。差别较大的分量应该接受较小的权重。

于是，公式3可变形为椭球体方程：

$$(\frac{x_1}{s_1})^2+\dots+(\frac{x_p}{s_p})^2=\parallel x\parallel^2（公式4）$$

其中的$$s_i$$表示i分量的权重。

公式4进一步整理，并扩展到两个p维向量x,y，可得马氏距离定义：

$$d_M(x,y)=\sqrt{(\frac{x_1-y_1}{s_1})^2+\dots+(\frac{x_p-y_p}{s_p})^2}=\sqrt{(x-y)^TD^{-1}(x-y)}（公式5）$$

其中，$$D=diag(s_1^2,\dots,s_p^2)$$。

>注意：这里p维向量是正交基，否则的话，D将不是主对角线矩阵，而是一个普通的协方差矩阵。显然如果D为单位矩阵的话，马氏距离就变成了欧氏距离。

马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关。比如体重和身高数据，如果采用欧氏距离，则会由于量纲的不同，而无法比较。

由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。它的缺点是夸大了变化微小的变量的作用。

马氏距离的特点还包括：

1）马氏距离的计算是建立在总体样本的基础上的，这一点可以从上述协方差矩阵的解释中可以得出，也就是说，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；
