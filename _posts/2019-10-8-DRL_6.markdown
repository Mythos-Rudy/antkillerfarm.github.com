---
layout: post
title:  深度强化学习（六）——MARL
category: DRL 
---

# AlphaGo

### Representation Network（续）

上文已经指出Model-based方法的困难在于：有的时候Model是很难刻画的，而环境本身也许并不如模拟器那么纯粹、简单。

一个很自然的思路就是：既然NN能表示Policy和Value，那么能不能表示Model呢？



![](/images/img3/MuZero_2.png)

### 参考

https://mp.weixin.qq.com/s/BLQF8WNsIe4a_rDeGWo_Vg

通用AlphaGo诞生？DeepMind的MuZero在多种棋类游戏中超越人类

https://www.zhihu.com/question/356976342

如何评价DeepMind新提出的MuZero算法？

## Leela Zero

Leela Zero是比利时人Gian-Carlo Pascutto开源的围棋AI。它的算法与AlphaGo Zero相同。而训练采用GTP协议，集合全球算力，进行分布式训练。

官网：

http://zero.sjeng.org/

代码：

https://github.com/gcp/leela-zero

>十多年前，当我还是一个中二青年的时候，就幻想有朝一日能够拿围棋世界冠军。当然，就算再中二，我自己也明白靠实力那是不可能的，当时做梦的法宝是制造一个AI，然后碾压一下所谓的国手。   
>按照当时(2000年前后)人们的预计，这个AI在2030年之前，都不可能造出来，然而，最终的结果实际上只花了一半左右的时间。   
>再之后，随着AI围棋的平民化，我的中二梦终于也有人将之付诸实现了：   
>https://mp.weixin.qq.com/s/npt2zZrKwPnNdY-hsa2RjQ   
>AI再乱围棋圈：“食言之战”柯洁落败；首例素人作弊引风波

这次作弊风波所使用的AI就是Leela Zero，可见目前（2018.5）它的棋力已经超过了顶尖棋手。

## ELF OpenGo

ELF OpenGo是Facebook开源的围棋AI，它是FB的AI游戏框架ELF的一部分。

官网：

https://github.com/pytorch/ELF

参考：

https://mp.weixin.qq.com/s/lOAx3suLIS-pEWyi8xZl6Q

“全民体验”AlphaZero：FAIR田渊栋首次开源超级围棋AI

## PhoenixGo

PhoenixGo是腾讯微信团队的AlphaGo Zero复刻版。

官网：

https://github.com/Tencent/PhoenixGo

参考：

https://mp.weixin.qq.com/s/tJDmxsuS1QigYS75ZIdzRA

微信团队开源围棋AI技术PhoenixGo，复现AlphaGo Zero论文

## 参考

https://blog.csdn.net/amds123/article/details/70666594

论文笔记：Mastering the game of Go with deep neural networks and tree search

https://mp.weixin.qq.com/s/Sfv-jzQAkN0PsZOGZUQhkQ

AlphaGo Zero横空出世，DeepMind Nature论文解密不使用人类知识掌握围棋

https://mp.weixin.qq.com/s/oAxouYX7-wDC5okbu--Wuw

Nature重磅：人工智能从0到1, 无师自通完爆阿法狗100-0

https://zhuanlan.zhihu.com/p/30262872

关于AlphaGo Zero

https://zhuanlan.zhihu.com/p/30263585

DeepMind新一代围棋程序AlphaGo Zero再次登上Nature

https://www.zhihu.com/question/66861459

如何评价DeepMind发表在Nature上的AlphaGo Zero？

https://deepmind.com/blog/alphago-zero-learning-scratch/

AlphaGo Zero官方声明

https://zhuanlan.zhihu.com/mathNote

某牛的专栏，主要讲自制AlphaGo

https://mp.weixin.qq.com/s/DC9QqHdWT0xFnowEBuJDbw

自动化所解读“深度强化学习”：从AlphaGo到AlphaGoZero

https://mp.weixin.qq.com/s/uZtaxRwROCqYmL2k6Muxaw

从阿尔法狗元(AlphaGo Zero)的诞生看终极算法的可能性

https://mp.weixin.qq.com/s/i5OmLu8aNbypiTUmP4teeQ

刘遥行：深入浅出看懂AlphaGo Zero

https://mp.weixin.qq.com/s/aBrwbB_DOGTen-6XL7LGFQ

邓侃：白话蒙特卡洛树搜索和ResNet

https://mp.weixin.qq.com/s/nbTkr0PImlXUSYl91HD91Q

AlphaGo背后的力量：蒙特卡洛树搜索入门指南

https://mp.weixin.qq.com/s/-tH7DQo1cK9gA0bcpBJSDA

AlphaGo Zero：笔记与伪代码

https://mp.weixin.qq.com/s/CJuVoOf7idUChFIn7dH0Lg

围棋中的数学原理

https://mp.weixin.qq.com/s/d46qNFaftt4wxpV4sZnG-w

一张图看懂AlphaGo Zero

https://zhuanlan.zhihu.com/p/31749249

比AlphaGo Zero更强的AlphaZero问世，8小时解决一切棋类！

https://mp.weixin.qq.com/s/L7bZMkqyncwEt6D5tK1OdQ

AlphaZero炼成最强通用棋类AI，DeepMind强化学习算法8小时完爆人类棋类游戏

https://mp.weixin.qq.com/s/tFdnxqV5a5xZrFtB6E0AiQ

新AlphaZero出世称霸棋界，8小时搞定一切棋类！自对弈通用强化学习无师自通！

https://mp.weixin.qq.com/s/qYWsFBKNCKCGUmizX_1sVg

AlphaGo 教学工具终于上线了！

https://mp.weixin.qq.com/s/JxbIeDk8_wnYu_ewUHp29g

深度学习与围棋实战书籍《Deep Learning and the Game of Go》

https://mp.weixin.qq.com/s/gsRnbknytz2FY2dWgdWEYg

精通国际象棋的AI研究员：AlphaZero真的是一次突破吗？

https://zhuanlan.zhihu.com/p/31809930

浅述：从Minimax到AlphaZero，完全信息博弈之路（1）

https://zhuanlan.zhihu.com/p/32073374

浅述：从Minimax到AlphaZero，完全信息博弈之路（2）

https://zhuanlan.zhihu.com/p/32089487

AlphaZero实战：从零学下五子棋

http://mp.weixin.qq.com/s/72riTTC3w0q9oF5H-51kXA

手把手教你搭建AlphaZero（使用Python和Keras）

https://mp.weixin.qq.com/s/Qw2tT7H1PwDvPgOYy8YUsQ

AlphaGo Zero代码迟迟不开源，TF等不及自己推了一个

https://mp.weixin.qq.com/s/Vq-osjgNXJQu5avGkxQdsw

手把手：AlphaGo有啥了不起，我也能教你做一个

https://mp.weixin.qq.com/s/ajajJ9yJZsOy4Vc0ULBxXg

国际象棋版AlphaZero出来了诶，还开源了Keras实现

https://zhuanlan.zhihu.com/p/41814142

从源码解密AlphaGo Zero背后基本原理

https://www.ifanr.com/630602

AlphaGo的棋局，与人工智能有关，与人生无关

https://mp.weixin.qq.com/s/J0w6kzzdKTbsaiZitbQdoA

达观数据：一文详解AlphaGo原理

https://mp.weixin.qq.com/s/BBQ54HHrFiqxXkC-EI6ELw

Science封面：AlphaZero达成终极进化体，史上最强棋类AI降临！

https://mp.weixin.qq.com/s/Pgw_xaCNl_kCPCg8NFzUBQ

人类没法下了！DeepMind贝叶斯优化调参AlphaGo，自弈胜率大涨16.5%

https://mp.weixin.qq.com/s/eE3oL6c5zHmTglHE-dgBvg

详解AlphaGo到AlphaGo Zero！

https://mp.weixin.qq.com/s/aAF0Gr7yEPkHRLASecJipw

百度正用谷歌AlphaGo，解决一个比围棋更难的问题

https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef

The 3 Tricks That Made AlphaGo Zero Work

https://web.stanford.edu/~surag/posts/alphazero.html

A Simple Alpha(Go) Zero Tutorial

# MARL

Multi-agent Reinforcement Learning(MARL)，一般被称为多智能体强化学习。与之相对的则是Single-agent Reinforcement Learning(SARL)。

以下主要参考了如下文章：

https://zhuanlan.zhihu.com/p/39037444

多智能体强化学习笔记 01

https://zhuanlan.zhihu.com/p/43579796

多智能体强化学习笔记 02

论文：

《Multi-agent reinforcement learning: An overview》

《Game Theory and Multi-agent Reinforcement Learning》

MARL应用了大量博弈论的知识，可参见《强化学习（十）》。

----

我们使用如下示例，探讨一下**什么是MARL？**

![](/images/img3/MARL.png)

上图是两个智能体1、2将金条（object）搬运回家的例子。这根金条需要两个人一人抬着一边才能扛回家。

要想把金条扛回家，小红和小蓝必须先绕过障碍物，然后每个人到达金条的一边，扛起金条后，两人还得绕开家门口的障碍物，这样才能将金条扛回家。

从上面的问题可以看出，MARL至少应该包括如下几个要素：

- 在多智能体系统中至少有两个智能体。

- 智能体之间存在着一定的关系，如合作关系（如本例），竞争关系（如多人游戏），或者同时存在竞争与合作的关系。

- 每个智能体最终所获得的回报不仅仅与自身的动作有关系，还跟对方的动作有关系。如本例中每个智能体要想获得回报，必须是金条被两个智能体一起搬回家。

SARL用MDP来描述，而MARL则需要用Markov game（马尔科夫博弈，又称随机博弈（stochastic game））来描述。

类似于MDP，Markov game可形式化表示为：

$$(n,S,A_1,\dots,A_n,T,\gamma,R_1,\dots,R_n)$$

n为玩家的个数，在本例中n为2。

S为系统状态，一般指多智能体的联合状态，即每个智能体的联合状态。

T为状态转移函数，指给定玩家当前状态和联合行为时，下一状态的概率分布。

R为回报函数。它描述了多智能体之间的关系。当每个智能体的回报函数一致时，则表示智能体之间是合作关系；当回报函数相反时，则表示智能体之间是竞争关系，当回报函数介于两者之间，则是混合关系。

MARL的结构一般如下图所示。

![](/images/img3/MARL_2.png)

**静态博弈**：static/stateless game是指没有状态s，不存在动力学使状态能够转移的博弈。例如一个矩阵博弈。

**阶段博弈**：stage game，是随机博弈的组成成分，状态s是固定的，相当于一个状态固定的静态博弈，随机博弈中的Q值函数就是该阶段博弈的奖励函数。若干状态的阶段博弈组成一个随机博弈。

**重复博弈**：智能体重复访问同一个状态的阶段博弈，并且在访问同一个状态的阶段博弈的过程中收集其他智能体的信息与奖励值，并学习更好的Q值函数与策略。

----

MARL的目标是找到每一个状态的纳什均衡策略，然后将这些策略联合起来。

<div style="text-align: center;">
<table>
  <tr>
    <td colspan="2" rowspan="2"></td>
    <td colspan="3">Game setting</td>
  </tr>
  <tr>
    <td>Stateless Games</td>
    <td>Team Markov Games</td>
    <td>General Markov Games</td>
  </tr>
  <tr>
    <td rowspan="2">Information<br/>Requirement</td>
    <td>Independent Learners</td>
    <td>Stateless Q-learning<br/>Learning Automata<br/>IGA<br/>FMQ<br/>Commitment Sequences<br/>Lenient Q-learners</td>
    <td>Policy Search<br/>Policy Gradient</td>
    <td>MG-ILA<br/>(WoLF-)PG<br/>Learning of Coordination<br/>Independent RL<br/>CQ-learning</td>
  </tr>
  <tr>
    <td>Joint Action Learners</td>
    <td></td>
    <td>Distributed- Q<br/>Sparse Tabular Q<br/>Utile Coordination</td>
    <td>Nash-Q<br/>Friend-or-Foe Q<br/>Asymmetric Q<br/>Joint Action Learning<br/>Correlated-Q</td>
  </tr>
</table>
</div>

参考：

https://zhuanlan.zhihu.com/c_1061939147282915328

一个MARL方面的专栏

https://github.com/LantaoYu/MARL-Papers

Paper list of multi-agent reinforcement learning (MARL)

https://blog.csdn.net/qq_38163755/article/details/98057751

Game Theory and Multi-agent Reinforcement Learning笔记1
