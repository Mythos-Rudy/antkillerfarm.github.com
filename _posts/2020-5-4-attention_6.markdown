---
layout: post
title:  Attention（六）——Attention进阶
category: Attention 
---

* toc
{:toc}

# BERT进阶

https://mp.weixin.qq.com/s/pYSs6NhIAB6DuwNnKZhkZQ

Bert改进：如何融入知识

https://mp.weixin.qq.com/s/in5SDWlQg8ts4E8DTmHxMQ

BERT在推荐系统领域可能会有什么作为？

https://mp.weixin.qq.com/s/kJhOrz0VaYc-k-6XJS02ag

8篇论文梳理BERT相关模型进展与反思

https://mp.weixin.qq.com/s/-eBL9gFbAGFtmqkLMAoUTw

文本+视觉，多篇Visual/Video BERT论文介绍

https://mp.weixin.qq.com/s/hI9XAiqKaHLq-Z9JkaWA_A

解决自然语言歧义问题，斯坦福教授、IJCAI卓越研究奖得主提出SenseBERT模型

https://mp.weixin.qq.com/s/55B0ToIKDusiPI5farR19w

NLP这两年：15个预训练模型对比分析与剖析

https://mp.weixin.qq.com/s/SPfa17p3QetZXCC01DwmQA

解密BERT

https://zhuanlan.zhihu.com/p/72805778

BERT的演进和应用

https://mp.weixin.qq.com/s/9YuBY0wLLVQ8ZrT9fiNICA

语音版BERT？滴滴提出无监督预训练模型，中文识别性能提升10%以上

https://mp.weixin.qq.com/s/OXkXjPHhaMXsKw2YevV6sw

邱锡鹏：从Transformer到BERT--自然语言处理中的表示学习进展

https://mp.weixin.qq.com/s/6i9eQISKsWU0jawKzWg8nQ

超越bert，最新预训练模型ELECTRA论文阅读笔记

https://mp.weixin.qq.com/s/dV4RkxZOC9o2BxNi0GljKQ

谷歌最强NLP模型BERT官方中文版来了！多语言模型支持100种语言

https://mp.weixin.qq.com/s/fz-bQMAi5bs2_bvRhf3ERg

从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史

https://mp.weixin.qq.com/s/k_33UK1RkMyHn6TSudU6Kg

详解谷歌最强NLP模型BERT

https://mp.weixin.qq.com/s/d2MZQbamdo0EC_MVtf-HZA

BERT详解：开创性自然语言处理框架的全面指南

https://mp.weixin.qq.com/s/pD4it8vQ-aE474uSMQG0YQ

两行代码玩转Google BERT句向量词向量

https://mp.weixin.qq.com/s/osmUZxAAX3x-oTHYJbzemA

谷歌BERT模型fine-tune终极实践教程

https://mp.weixin.qq.com/s/XmeDjHSFI0UsQmKeOgwnyA

小数据福音！BERT在极小数据下带来显著提升的开源实现

https://mp.weixin.qq.com/s/HXYDO5PM8UIoXgEPGe8p-w

图解当前最强语言模型BERT：NLP是如何攻克迁移学习的？

https://mp.weixin.qq.com/s/zz3j9HEuzw5e92MQXxSQsA

遗珠之作？谷歌Quoc Le这篇NLP预训练模型论文值得一看

https://mp.weixin.qq.com/s/IN4YfoZnlBozwEFdhSvLZg

用可视化解构BERT，我们从上亿参数中提取出了6种直观模式

https://mp.weixin.qq.com/s/nIT3GIU0dUIYyGChxsiOWw

Google BERT应用之《红楼梦》对话人物提取

https://mp.weixin.qq.com/s/dcp_ANYijRmicMYX7OpJmA

如何用最强模型BERT做NLP迁移学习？

https://mp.weixin.qq.com/s/DR4SkgOfUT7KYiaXm5NynQ

跨语言版BERT：Facebook提出跨语言预训练模型XLM

https://mp.weixin.qq.com/s/epjjHmlmMFhWtRO_cCUITA

用BERT进行多标签文本分类

https://mp.weixin.qq.com/s/Wk6gvOS_Qnud6ib1esMFXA

加入Transformer-XL，这个PyTorch包能调用各种NLP预训练模型！

https://mp.weixin.qq.com/s/GqqU3Ixht1BzMnQeRYQEqQ

谷歌NLP深度学习模型BERT特征的可解释性表现怎么样？

https://mp.weixin.qq.com/s/2f91Ksj19rk_emoFpEmPfA

从BERT看大规模数据的无监督利用

https://mp.weixin.qq.com/s/hF4EcKqmaTm_gemxX7Kftg

BERT的嵌入层是如何实现的？

https://mp.weixin.qq.com/s/CdjNQKSNuklVUsXe4InSoA

FastBERT：放飞BERT的推理速度

https://zhuanlan.zhihu.com/p/132361501

BERT是如何分词的

https://mp.weixin.qq.com/s/Tld9V1jdmWs06zNxiJNkZg

BART&MASS自然语言生成任务上的进步

https://mp.weixin.qq.com/s/G995ulqe6Ifxml_AJqapAw

BERT在小米NLP业务中的实战探索

https://www.cnblogs.com/gczr/p/12874409.html

Sentence-BERT: 一种能快速计算句子相似度的孪生网络

https://mp.weixin.qq.com/s/0hUNG6tC-hlfyTJtuzwU5w

NLP中的Mask全解

https://mp.weixin.qq.com/s/cyNcVNImoCOmTrsS0QVq4w

用Siamese和Dual BERT来做多源文本分类

https://mp.weixin.qq.com/s/uv74FKtUNtgjIBQZbsX7Qw

你finetune BERT的姿势可能不对哦？

https://mp.weixin.qq.com/s/BvM5zx-3XrsZj8BQ5WEa4A

一文带你了解MultiBERT

https://mp.weixin.qq.com/s/mFRhp9pJRa9yHwqc98FMbg

BERT在美团搜索核心排序的探索和实践

https://mp.weixin.qq.com/s/MPGF3tkNn3PBA_7S-fo9eg

谷歌新模型突破BERT局限：NLP版“芝麻街”新成员Big Bird长这样

https://zhuanlan.zhihu.com/p/165893466

BERT及其变种

https://mp.weixin.qq.com/s/5HZULHPI3-HJypvAMXEOcQ

MT-BERT在文本检索任务中的实践

# Transformer进阶

https://mp.weixin.qq.com/s/-Y7Qy-5aJNJ5bx8QJf3k2w

Transformer及其变种

https://mp.weixin.qq.com/s/M0zLw9hA5xzontKB7Zj23Q

Memory Transformer，一种简单明了的Transformer改造方案

https://mp.weixin.qq.com/s/FJeZ8X9gtyciqCTs9zvlLA

Transformer是CNN是GNN是RNN，Attention is all you need！

https://mp.weixin.qq.com/s/d1qqRw7sWyLdoyfnqMBdJQ

深度自适应Transformer

https://mp.weixin.qq.com/s/IOc-gxOa6a415Hf1VBmiQw

“Linformer”拍了拍“被吊打Transformers的后浪们”

https://mp.weixin.qq.com/s/UowNtBm_hqnes-Lz3POXGQ

Transformers中的Beam Search高效实现

# Attention进阶

https://zhuanlan.zhihu.com/p/40920384

真正的完全图解Seq2Seq Attention模型

https://mp.weixin.qq.com/s/ZSzHOu6uowRSoWrqB7vOaQ

深度学习注意力机制-Attention in Deep learning-附101页PPT

https://mp.weixin.qq.com/s/FlA1YrR0sLQGJoJZnSXpRw

DeepMind：深度学习注意力与记忆机制，附70页ppt

https://mp.weixin.qq.com/s/_mBa-GTdILrvluJtegz8fw

南洋理工大学：注意力神经网络，Attention Neural Networks，78页ppt

https://mp.weixin.qq.com/s/pKxqPB9qIGmE_PslPa6EyA

长文详解Attention的前世今生

https://mp.weixin.qq.com/s/2kFZAmb_WnTNYUXT_jVMqg

Attention注意力机制的前世今生

https://mp.weixin.qq.com/s/eq1iTyKguQm5t6Xoc7KUgw

一文搞懂NLP中的Attention机制

https://zhuanlan.zhihu.com/p/106662375

More About Attention

https://mp.weixin.qq.com/s/y_hIhdJ1EN7D3p2PVaoZwA

阿里北大提出新attention建模框架，一个模型预测多种行为

https://mp.weixin.qq.com/s/Yq3S4WrsQRQC06GvRgGjTQ

打入神经网络思维内部

https://mp.weixin.qq.com/s/MJ1578NdTKbjU-j3Uuo9Ww

基于文档级问答任务的新注意力模型

https://mp.weixin.qq.com/s/C4f0N_bVWU9YPY34t-HAEA

UNC&Adobe提出模块化注意力模型MAttNet，解决指示表达的理解问题

https://mp.weixin.qq.com/s/V3brXuey7Gear0f_KAdq2A

基于注意力机制的交易上下文感知推荐，悉尼科技大学和电子科技大学最新工作

http://mp.weixin.qq.com/s/Bt6EMD4opHCnRoHKYitsUA

结合人类视觉注意力进行图像分类

https://mp.weixin.qq.com/s/POYTh4Jf7HttxoLhrHZQhw

基于双向注意力机制视觉问答pyTorch实现

https://mp.weixin.qq.com/s/2gxp7A38epQWoy7wK8Nl6A

谷歌翻译最新突破，“关注机制”让机器读懂词与词的联系

https://zhuanlan.zhihu.com/p/25928551

用深度学习（CNN RNN Attention）解决大规模文本分类问题-综述和实践

http://blog.csdn.net/leo_xu06/article/details/53491400

视觉注意力的循环神经网络模型

https://mp.weixin.qq.com/s/l4HN0_VzaiO-DwtNp9cLVA

循环注意力区域实现图像多标签分类

https://mp.weixin.qq.com/s/zhZLK4pgJzQXN49YkYnSjA

自适应注意力机制在Image Caption中的应用

https://mp.weixin.qq.com/s/uvr-G5-_lKpyfyn5g7ES0w

基于注意力机制，机器之心带你理解与训练神经机器翻译系统

https://mp.weixin.qq.com/s/ANpBFnsLXTIiW6WHzGrv2g

自注意力机制学习句子embedding

https://mp.weixin.qq.com/s/49fQX8yiOIwDyof3PD01rA

CMU&谷歌大脑提出新型问答模型QANet：仅使用卷积和自注意力，性能大大优于RNN

https://mp.weixin.qq.com/s/c64XucML13OwI26_UE9xDQ

滴滴披露语音识别新进展：基于Attention显著提升中文识别率

https://mp.weixin.qq.com/s/7OYY3L7gL4wVv_EjoosOHA

如何增强Attention Model的推理能力

https://mp.weixin.qq.com/s/9Kt6_DfeYRnhsb10aCSFGw

FAGAN：完全注意力机制（Full Attention）GAN，Self-attention+GAN

https://mp.weixin.qq.com/s/lZOIK5BRXZrmL_Z9crl6sA

机器翻译新突破！“普适注意力”模型：概念简单参数少，性能大增

https://mp.weixin.qq.com/s/JoTzaInn_uAA9oZgMcfskw

计算机视觉技术self-attention最新进展

https://mp.weixin.qq.com/s/jRfOzKO6OlQLokIzipbqUQ

为什么使用自注意力机制？

https://mp.weixin.qq.com/s/_3pA8FZwzegSpyz_cK63BQ

Self-Attention GAN中的self-attention机制

https://mp.weixin.qq.com/s/h7sLwVXb_UI8jvJU-oe3Cg

Google AI提出“透明注意力”机制，实现更深层NMT模型

https://mp.weixin.qq.com/s/1LYz5SH5rVnPPJ0tZvRQAA

从各种注意力机制窥探深度学习在NLP中的神威

https://zhuanlan.zhihu.com/p/32928645

计算机视觉中的注意力机制

https://zhuanlan.zhihu.com/p/56501461

计算机视觉中的注意力机制

https://zhuanlan.zhihu.com/p/33078323

数字串识别：基于位置的硬性注意力机制

https://zhuanlan.zhihu.com/p/32971586

图像描述：基于项的注意力机制

https://zhuanlan.zhihu.com/p/33158614

图像识别：基于位置的柔性注意力机制

https://mp.weixin.qq.com/s/tVKEJ9rqlMaZ9bx6ngIelw

自注意力机制在计算机视觉中的应用

https://mp.weixin.qq.com/s/-gAISWjSiG6ccPuOPAEg3A

五张动图，看清神经机器翻译里的Attention！

https://mp.weixin.qq.com/s/Qs6tm50YvzHaJv2rh60WMw

撩一发深度文本分类之RNN via Attention

https://mp.weixin.qq.com/s/MMIZGHTKM5FrvNE6ucQRYQ

33页最新《自然语言处理中神经注意力机制综述》论文

https://mp.weixin.qq.com/s/Q0Ft5bWTuiZUIQSTk7X6ZQ

图解神经机器翻译中的注意力机制

https://mp.weixin.qq.com/s/D7GQ8DRzss9ppP6pyAs1qA

从0到1再读注意力机制

https://mp.weixin.qq.com/s/K_VRt0B9-Xw7YJndmb4WZg

Attention！注意力机制模型最新综述

https://mp.weixin.qq.com/s/hzwp5oGspdtDyNBmq8sMsw

HAN：基于双层注意力机制的异质图深度神经网络

https://mp.weixin.qq.com/s/SBrLPZjx2RdBwZpPQQ5DXQ

HAN：异构图注意力网络

https://mp.weixin.qq.com/s/aixpv9t1PLPRWUP6PvZ0EQ

用自注意力增强卷积：这是新老两代神经网络的对话

https://mp.weixin.qq.com/s/i3Xd_IB7R0-QPztn-pgpng

遍地开花的Attention，你真的懂吗？

https://mp.weixin.qq.com/s/rrbwItXt-1EaGiqtDEGvog

为节约而生：从标准Attention到稀疏Attention

https://mp.weixin.qq.com/s/G4mFW8cn-ho3KGmbw5sSTw

计算机视觉中注意力机制原理及其模型发展和应用

https://mp.weixin.qq.com/s/MzHmvbwxFCaFjmMkjfjeSg

遍地开花的Attention，你真的懂吗？

https://mp.weixin.qq.com/s/7ETHeN2xV_hEwkDxrhJyNg

用Attention玩转CV，一文总览自注意力语义分割进展

https://mp.weixin.qq.com/s/e_LEhLf2Rh-1zkEBmqS4nA

NLP这两年：15个预训练模型对比分析与剖析
