---
layout: post
title:  强化学习（十三）——模仿学习
category: RL 
---

* toc
{:toc}

# Exploration & Exploitation（续）

## 一些MAB变种

我们前面的讨论默认了环境是不会变化的。而一些MAB问题，这个假设可能不成立，这就好比如果一位玩家发现某个机器的$$p_i$$很高，一直摇之后赌场可能人为降低这台机器吐钱的概率。在这种情况下，MAB问题的环境就是随着时间/玩家的行为会发生变化。

如果概率事先设定好，并且在玩家开始有动作之后也无法更改，我们叫做oblivious adversary setting；如果这个对手在玩家有动作之后还能随时更改自己的设定，那就叫做adaptive adversary setting。

另外一类变种，叫做contextual MAB(cMAB)。几乎所有在线广告推送（dynamic ad display）都可以看成是cMAB问题。在这类问题中，每个arm的回报会和当前时段出现的顾客的特征（也就是这里说的context）有关。

参考：

https://mp.weixin.qq.com/s/8Ks1uayLw6nfKs4-boiMpA

多任务学习时转角遇到Bandit老虎机

# Inverse Reinforcement Learning

逆强化学习问题定义：

- 已知：

智能体在变化的环境中的行为

（optional）智能体传感器的数据

（optional）环境的模型

- 求：

最优化的Reward function

逆强化学习研究意义：

- 对动物人类行为用强化学习建模（生物学、计量经济学...）模仿学习。

- reward function在强化学习里面非常非常重要，是对行为的抽象精简的描述，因此IRL (Inverse Reinforcement Learning)可能是一种很高效的模仿学习范式。

通俗的说法就是：**我们用优化控制或强化学习得到的策略能用来解释人类的行为吗？**

比如让一个人去拿桌子上的一个橘子，那手的轨迹一定不是一条从起点到目标的直线，而是有一些弯曲的轨迹，也就是带有偏差的较优行为，但是这种偏差其实并不重要，只要最后拿到橘子就行了，也就是说：

1.有过程中一些偏差是不重要的，但另一些偏差就比较重要了（如最后没拿到）。

2.每次拿橘子的动作也是不一样的，因此人的动作带有一定的随机性。

3.我们可以认为人类行为轨迹分布是以最优策略为峰的随机分布。

![](/images/img3/GAN_vs_IRL.png)

参考：

https://zhuanlan.zhihu.com/p/61674994

Algorithms for Inverse Reinforcement Learning

https://zhuanlan.zhihu.com/p/32502503

CS 294：IRL

# 模仿学习

https://zhuanlan.zhihu.com/p/27935902

机器人学习Robot Learning之模仿学习Imitation Learning的发展

https://zhuanlan.zhihu.com/p/25688750

模仿学习（Imitation Learning）完全介绍

https://mp.weixin.qq.com/s/naq73D27vsCOUBperKto8A

从监督式到DAgger，综述论文描绘模仿学习全貌

https://mp.weixin.qq.com/s/LNNqp2KsEAljG26hY43mUw

ICML2018 模仿学习教程

https://mp.weixin.qq.com/s/f9vSgH1HQwGXBDb0UGHQyQ

深度学习进阶之无人车行为克隆

https://mp.weixin.qq.com/s/To3pnx1hVq_4p7UnQVMw9A

斯坦福大学&DeepMind联合提出机器人控制新方法，RL+IL端到端地学习视觉运动策略

https://mp.weixin.qq.com/s/O0Q1XoTA-7Yshr1ZqOZ90w

加州理工：什么是模仿学习, 这62页ppt带你了解进展

https://mp.weixin.qq.com/s/b1hXMbDKP_QZcBlmY7VtKQ

最新《Imitation Learning》教程，63页ppt，微软Kamil Ciosek

# RL与神经科学

Pavlov Model（1901）

Rescorla-Wagner Model（1972）

Thorndike’s Puzzle Box（1910）

参考：

https://zhuanlan.zhihu.com/p/24437724

学习理论之Rescorla-Wagner模型

https://zhuanlan.zhihu.com/p/104261195

生物，AI，心理：目前的大脑/认知/意识/AGI/DRL模型

# RL参考资源

https://mp.weixin.qq.com/s/sIS9VvZ3yTtn6puJScuHig

强化学习70年演进：从精确动态规划到基于模型

https://mp.weixin.qq.com/s/Fn1s9Ia8L1ckgn6iP24FhQ

如何让神经网络具有好奇心

https://mp.weixin.qq.com/s/PBf-YrkhwhPYXuiOGyahxQ

强化学习遭遇瓶颈！分层RL将成为突破的希望

https://zhuanlan.zhihu.com/p/58815288

强化学习之值函数学习

https://mp.weixin.qq.com/s/8Cqknze_iosz6Z6cqnuK5w

谷歌提出强化学习新算法SimPLe，模拟策略学习效率提高2倍

https://mp.weixin.qq.com/s/hKGS4Ek5prwTRJoMCaxQLA

强化学习Exploration漫游

https://zhuanlan.zhihu.com/p/65116688

值分布强化学习（01）

https://zhuanlan.zhihu.com/p/65364484

值分布强化学习（02）

https://zhuanlan.zhihu.com/p/62363784

强化学习之策略搜索

https://mp.weixin.qq.com/s/j9Cs5M9gyITu2u_XDkKm-g

Policy Gradient——一种不以loss来反向传播的策略梯度方法

https://mp.weixin.qq.com/s/x6gKTuYIx8y25KX-fCc5bA

蒙特卡洛梯度估计方法（MCGE）简述

https://mp.weixin.qq.com/s/jUKTsJdlBLPWbiBa6TlPPA

基于模型的强化学习—LQR与iLQR

https://mp.weixin.qq.com/s/tv8Tq9hIsll9_aPGUKI8qQ

强化学习《奖励函数设计: Reward Shaping》详细解读

https://zhuanlan.zhihu.com/p/25319023

强化学习（Reinforcement Learning）知识整理

# DRL参考资源+

https://mp.weixin.qq.com/s/1b3_AiFhwXqxb7FozdRYIQ

最in强化学习+NLP技术分享会

https://mp.weixin.qq.com/s/RKQb7-mQ-ELRRq18db02Pg

DeepMind大突破！AI模拟大脑导航功能，学会像动物一样“抄近路”

https://mp.weixin.qq.com/s/qBwszD9rn4gKazXdwqexSQ

MIT提出使用“深度强化学习”帮助智能体在运动中做出“动作决策”

https://mp.weixin.qq.com/s/i-udn1M4kiJpF8U7u5Uepg

专家解读DeepMind最新论文：深度学习模型复现大脑网格细胞

https://mp.weixin.qq.com/s/AS1VFjBFnSk19QJ28tBVWA

NIPS 2017斯坦福赛题大公开：强化学习模拟人类肌肉骨骼模型

https://mp.weixin.qq.com/s/TWjFWe6-dZWDoTi5gN1BxA

深度强化学习在指代消解中的一种尝试

https://mp.weixin.qq.com/s/YnMgJDAh3XhyyNdI8RXmtw

腾讯知文等提出新型生成式摘要模型：结合主题信息和强化训练生成更优摘要

https://mp.weixin.qq.com/s/VrVdsxn94ux_46mIyS8f0w

谷歌大脑实现更宽广的智能体视野，在Atari2600上可持续超越人类玩家！

https://mp.weixin.qq.com/s/P5EysBHBaR6L3IfeSgo6fw

强化学习20分钟，剑桥博士教汽车学会自动驾驶！

https://mp.weixin.qq.com/s/ij3bf61Pu7lrX0WijhbDeA

骑驴找马：利用深度强化学习模型定位新物体

https://mp.weixin.qq.com/s/iYxijHlE3sLJgKnwwd8Tgg

使用深度强化学习和贝叶斯优化获得巨额利润

https://mp.weixin.qq.com/s/_QkxCrQlyRM10eZK8aNCKA

强化学习在携程酒店推荐排序中的应用探索

https://mp.weixin.qq.com/s/fWySZWsYEKBRwYaFL3J2Xg

强化学习大规模应用还远吗？Youtube推荐已强势上线

https://mp.weixin.qq.com/s/nHBczPlffhZrJy4G4oJ1Ag

让神经网络懂得黄金法则

https://mp.weixin.qq.com/s/0dUlVC9I8qmv3f2BB0IFew

强化学习介绍及自动驾驶汽车应用

https://mp.weixin.qq.com/s/_Di73PkEWJV1-OLLHfz7yQ

组合在线学习：实时反馈玩转组合优化

https://mp.weixin.qq.com/s/_Ze8C8UQV9UZXnSX6qq2GA

强化学习玩王者荣耀

https://mp.weixin.qq.com/s/NkrPfitZ6o75XaGxGC0eZw

谷歌发布离线强化学习新范式，克服RL智能体只能在线训练难题，训练集相当于200多个ImageNet

https://mp.weixin.qq.com/s/nEw0Vem-aL9D6w-1lEAsBQ

字节跳动基于深度强化学习的广告推荐模型DEAR

https://mp.weixin.qq.com/s/xG7IboDg6tn13rRIgKSICg

强化学习中从仿真器到现实环境的迁移

https://mp.weixin.qq.com/s/CcJrsbYO70GwOcedUXnHdg

除了网红，强化学习也能带货？

https://mp.weixin.qq.com/s/u44neSzF2Iqyv-i1Fc4lsQ

脱胎换骨的生成模式：强化学习重排

https://mp.weixin.qq.com/s/nk-X88bF6LiAywnqRz3REQ

Youtube推荐RL首弹，基于Top-K的Off-Policy矫正解决推荐中的信息茧房困境

https://mp.weixin.qq.com/s/EPLvdQFiT1MiGgG7u4Qoqg

基于强化学习的无地图机器人导航，Reinforcement Learning Based MRN

https://mp.weixin.qq.com/s/X8STHGKlJYN2Qizz4t8Llg

Accelerating DRL via Human knowledge

https://zhuanlan.zhihu.com/p/145983063

大规模深度强化学习的发展

https://zhuanlan.zhihu.com/p/53326459

深度强化学习中的好奇心

https://zhuanlan.zhihu.com/p/79712897

动态环境下基于DRL的无人车自适应路径规划方法

https://mp.weixin.qq.com/s/LdkPnm8vo8oeYzIC0Imlvw

俞扬：强化学习真实环境不好用？那就模拟器来凑！

https://mp.weixin.qq.com/s/GcjoZfasWNZWlTC_xP1_wg

通用强化学习用算法发现算法：DeepMind 数据驱动“价值函数”自我更新，14款Atari游戏完虐人类！

https://mp.weixin.qq.com/s/IFIXvZ_9oEzHJI34_dFI8g

训练DQN模型，loss出现nan，要怎么解决？

https://mp.weixin.qq.com/s/RE43jNFKbOj0DcLpGnAY7g

Distributional Soft Actor-Critic (DSAC)强化学习算法的设计与验证

https://mp.weixin.qq.com/s/Sxrp3EZ8LCA3d06Zm5meKQ

《深度强化学习中的迁移学习》2020综述论文，22页pdf

https://mp.weixin.qq.com/s/ylavFA_MXLUhIBLCqxAjLQ

阿里强化学习重排实践

# Attention进阶++

https://mp.weixin.qq.com/s/fbAAA7fO2voP_v-NsavQew

深度学习中的注意力机制（三）

https://mp.weixin.qq.com/s/y_hIhdJ1EN7D3p2PVaoZwA

阿里北大提出新attention建模框架，一个模型预测多种行为

https://mp.weixin.qq.com/s/Yq3S4WrsQRQC06GvRgGjTQ

打入神经网络思维内部

https://mp.weixin.qq.com/s/MJ1578NdTKbjU-j3Uuo9Ww

基于文档级问答任务的新注意力模型

https://mp.weixin.qq.com/s/_3pA8FZwzegSpyz_cK63BQ

Self-Attention GAN中的self-attention机制

https://mp.weixin.qq.com/s/h7sLwVXb_UI8jvJU-oe3Cg

Google AI提出“透明注意力”机制，实现更深层NMT模型

https://mp.weixin.qq.com/s/1LYz5SH5rVnPPJ0tZvRQAA

从各种注意力机制窥探深度学习在NLP中的神威

https://zhuanlan.zhihu.com/p/33078323

数字串识别：基于位置的硬性注意力机制

https://mp.weixin.qq.com/s/-gAISWjSiG6ccPuOPAEg3A

五张动图，看清神经机器翻译里的Attention！

https://mp.weixin.qq.com/s/aixpv9t1PLPRWUP6PvZ0EQ

用自注意力增强卷积：这是新老两代神经网络的对话

https://mp.weixin.qq.com/s/i3Xd_IB7R0-QPztn-pgpng

遍地开花的Attention，你真的懂吗？

https://mp.weixin.qq.com/s/rrbwItXt-1EaGiqtDEGvog

为节约而生：从标准Attention到稀疏Attention

https://mp.weixin.qq.com/s/MzHmvbwxFCaFjmMkjfjeSg

遍地开花的Attention，你真的懂吗？

https://mp.weixin.qq.com/s/e_LEhLf2Rh-1zkEBmqS4nA

NLP这两年：15个预训练模型对比分析与剖析

https://mp.weixin.qq.com/s/LAInpFPa-3R1rfv6idILnw

注意力机制发展如何了，如何学习它在各类任务中的应用？
