---
layout: post
title:  名校机器学习相关课程（二）
category: resource 
---

# 名校机器学习相关课程

https://sites.google.com/view/berkeley-cs294-158-sp19/home

UCB CS294-158: Deep Unsupervised Learning

https://github.com/mbinary/USTC-CS-Courses-Resource

中国科学技术大学计算机学院课程资源

https://mp.weixin.qq.com/s/iQpERylxzfQ0P51AQBj8bA

概率论导论-哈佛和斯坦福大学合著630页新书

https://people.csail.mit.edu/madry/6.883/

6.883 Science of Deep Learning: Bridging Theory and Practice

https://fullstackdeeplearning.com/march2019

UCB: Full Stack Deep Learning。这个课程是专门讲述如何部署DL工程到实际产品中。

http://web.stanford.edu/class/cs246/

CS246: Mining Massive Data Sets

https://mp.weixin.qq.com/s/P7szDhvn5QNRKc7Iuq3Q6g

UAI 2019教程《深度学习数学基础》

https://mp.weixin.qq.com/s/By6xBnUgCKkK3b7pU7E1mA

宾夕法尼亚大学：面向计算机视觉、机器人和机器学习的线性代数 - 附749页书籍PDF

https://mp.weixin.qq.com/s/i79W79Z3ujRnHRBl81kvvA

最新版《机器学习数学基础》发布，417页PDF免费下载

https://mp.weixin.qq.com/s/HBUI9-jMa3otnB-Qq_eg5A

悉尼科大徐亦达教授：机器学习讲义，32份主题推介

# Actor-Critic

## 概述（续）

基于Actor-Critic策略梯度学习分为两部分内容：

1.Critic：更新action-value函数的参数w。

2.Actor：按照Critic得到的价值，引导策略函数参数$$\theta$$的更新。

$$\nabla_\theta J(\theta)\approx E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

$$\Delta \theta = \alpha \nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)$$

可以看出，Critic做的事情其实是我们已经见过的：策略评估，他要告诉个体，在由参数$$\theta$$确定的策略$$\pi_\theta$$到底表现得怎么样。

## Compatible Function Approximation

近似策略梯度的方法引入了Bias，从而得到的策略梯度不一定能最后找到较好的解决方案，例如当近似价值函数引起状态重名的特征时。

幸运的是，如果我们小心设计近似函数，是可以避免引入bias的。该近似函数需要满足下面两个条件:

- 近似价值函数的梯度完全等同于策略函数对数的梯度，即不存在重名情况：

$$\nabla_w Q_w(s,a)=\nabla_\theta \log \pi_\theta(s,a)$$

- 价值函数参数w使得均方差最小：

$$\epsilon = E_{\pi_\theta}[(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2]$$

符合这两个条件，则认为策略梯度是准确的，即：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

## Reducing Variance Using Baseline

除了bias之外，variance也是需要考虑的方面。

我们从策略梯度里抽出一个基准函数B(s)，要求这一函数仅与状态有关，而与行为无关，因而不改变梯度本身。

$$E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)B(s)] = \sum_{s\in S} d^{\pi_\theta}(s)\sum_{a\in A}\nabla_\theta \pi_\theta(s,a)B(s)\\= \sum_{s\in S} d^{\pi_\theta}(s)B(s) \nabla_\theta \sum_{a\in A} \pi_\theta(s,a) = 0$$

由于B(s)与行为无关，可以将其从针对行为a的求和中提出来，同时我们也可以把梯度从求和符号中提出来，而最后一个求和项：策略函数针对所有行为的求和，根据策略函数的定义，这里的结果肯定是1。而常数的梯度是0，因此总的结果等于0。

原则上，和行为无关的函数都可以作为B(s)。一个很好的B(s)就是基于当前状态的状态价值函数：$$V^{\pi_\theta}(s)$$。

所以，我们可以用一个advantage function来改写策略梯度：

$$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$$

上式的现实意义在于评估当个体采取行为a离开s状态时，究竟比该状态s总体平均价值要好多少？

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]$$

advantage function可以明显减少状态价值的variance，因此，现在Critic的任务就改为估计advantage function。

在这种情况下，我们需要两个近似函数也就是两套参数，一套用来近似状态价值函数，一套用来近似行为价值函数。即：

$$V_v(s)\approx V^{\pi_\theta}(s)$$

$$Q_w(s,a)\approx Q^{\pi_\theta}(s,a)$$

$$A(s,a) = Q_w(s,a) - V_v(s)$$

不过实际操作时，我们并不需要计算两个近似函数。这里以TD学习为例说明一下。

根据定义，TD误差$$\delta^{\pi_\theta}$$可以根据真实的状态价值函数$$V^{\pi_\theta}(s)$$算出：

$$\delta^{\pi_\theta} = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)$$

因为：

$$\begin{align}
E_{\pi_\theta}[\delta^{\pi_\theta} | s,a] &= E_{\pi_\theta}[r + \gamma V^{\pi_\theta}(s') | s,a] - V^{\pi_\theta}(s)\\
&= Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s) \\
&= A^{\pi_\theta}(s,a)
\end{align}$$

可见$$\delta^{\pi_\theta}$$就是$$A^{\pi_\theta}(s,a)$$的一个无偏估计。

因此，我们就可以使用TD误差来计算策略梯度：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\delta^{\pi_\theta}]$$

实际运用时，我们使用一个近似的TD误差，即用状态函数的近似函数来代替实际的状态函数：

$$\delta_v = r + \gamma V_v(s') - V_v(s)$$

这也就是说，我们只需要一套参数描述状态价值函数，而不再需要行为价值函数了。

上述方法不仅可用于TD方法，还可用于MC方法等，以下不加讨论的给出如下结论：

$$\begin{align}
\nabla_\theta J(\theta) &= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{v_t}] & \text{REINFORCE}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{Q_w(s,a)}] & \text{Q Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{A_w(s,a)}] & \text{Advantage Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{\delta}] & \text{TD Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{\delta e}] & \text{TD(}\lambda\text{) Actor-Critic}
\end{align}$$

## 参考

https://zhuanlan.zhihu.com/p/51652845

强化学习这都学不会的话，咳咳，你过来下！

https://zhuanlan.zhihu.com/p/70360272

最前沿：深度解读Soft Actor-Critic算法

https://mp.weixin.qq.com/s/ce9W3FbLdsqAEyvw6px_RA

Actor Critic——一个融合基于策略梯度和基于值优点的强化学习算法

https://mp.weixin.qq.com/s/c4xoy4CJ_hsVdmGe1n3rTQ

A3C——一种异步强化学习方法

# Integrating Learning and Planning

前面的章节主要介绍了Model-Free RL，下面将讲一下Model-Based RL，主要包括如下内容：

- 如何从经历中直接学习Model。

- 如何基于模型来进行Planning（规划）。

- 如何将“学习”和“规划”整合起来。

![](/images/img3/Model_Free_RL.png) | ![](/images/img3/Model_RL.png)

上面两图形象的说明了Model-Free RL（左图）和Model-Based RL（右图）的差别。

## Model-Based RL

![](/images/img3/Model_RL_2.png)

上图比较好的说明了模型学习在整个RL学习中的位置和作用。

我们首先看一下Model的定义：Model是一个参数化（参数为$$\eta$$）的MDP<S, A, P, R>，其中假定：状态空间Ｓ和行为空间Ａ是已知的，则$$M=<P_\eta, R_\eta>$$，其中$$P_\eta \approx P, R_\eta \approx R$$。则：

$$S_{t+1} \sim P_\eta (S_{t+1} | S_t, A_t)$$

$$R_{t+1} = R_\eta (R_{t+1} | S_t, A_t)$$

通常我们需要如下的假设，即状态转移函数和奖励函数是条件独立的：

$$P[S_{t+1}, R_{t+1} | S_t, A_t] = P[S_{t+1} | S_t, A_t] P[R_{t+1} | S_t, A_t]$$

## Model Learning

所谓Model Learning是指：从experience$$\{S_1, A_1, R_2, \dots, S_T\}$$，学习

$$S_1, A_1\to R_2,S_2$$

$$S_2, A_2\to R_3,S_3$$

$$\dots$$

$$S_{T-1}, A_{T-1}\to R_T,S_T$$

这实际上是一个监督学习的问题。其中，$$s,a\to r$$是一个回归问题(regression problem)，而$$s,a\to s'$$是一个密度估计问题(density estimation problem)。

选择一个损失函数，比如均方差，KL散度等，优化参数$$\eta$$来最小化经验损失(empirical loss)。所有监督学习相关的算法都可以用来解决上述两个问题。

根据使用的算法不同，可以有如下多种模型：查表式(Table lookup Model)、线性期望模型(Linear Expectation Model)、线性高斯模型(Linear Gaussian Model)、高斯决策模型(Gaussian Process Model)、和深信度神经网络模型(Deep Belief Network Model)等。

这里主要以查表模型来解释模型的构建。

## Table Lookup Model

查表模型适用于MDP的P，R都为已知的情况。我们通过visit得到各状态行为的转移概率和奖励，把这些数据存入表中，使用时直接检索。状态转移概率和奖励计算方法如下：

$$\hat{P}^a_{s,s'}=\frac{1}{N(s,a)}\sum_{t=1}^T 1(S_t,A_t,S_{t+1}=s,a,s')$$

$$\hat{R}^a_{s}=\frac{1}{N(s,a)}\sum_{t=1}^T 1(S_t,A_t=s,a)R_t$$

其中的$$N(s,a)$$表示state action pair$$<s,a>$$在visit中出现的次数。

这里也可以采用如下变种：

- 在每个time-step：t，记录experience tuple：$$<S_t, A_t, R_{t+1}, S_{t+1}>$$。

- 从模型采样构建虚拟经历时，从tuples中随机选择符合$$<s,a,\cdot,\cdot>$$的一个tuple。

这种方法，不再以Episode为最小学习单位，而是以时间步（time-step）为单位，一次学习一个状态转换。

## Planning with a Model

规划的过程相当于求解一个MDP的过程，即给定一个模型$$M_\eta = <P_\eta, R_\eta>$$，求解MDP$$<S, A, P_\eta, R_\eta>$$，找到基于该模型的最优价值函数或最优策略，也就是在给定的状态s下确定最优的行为a。

我们可以使用之前介绍过的价值迭代，策略迭代方法；也可以使用树搜索方法（Tree Search）。

## Sample-Based Planning


