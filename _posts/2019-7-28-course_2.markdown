---
layout: post
title:  名校机器学习相关课程（二）
category: resource 
---

# 名校机器学习相关课程

https://sites.google.com/view/berkeley-cs294-158-sp19/home

UCB CS294-158: Deep Unsupervised Learning

https://github.com/mbinary/USTC-CS-Courses-Resource

中国科学技术大学计算机学院课程资源

https://mp.weixin.qq.com/s/iQpERylxzfQ0P51AQBj8bA

概率论导论-哈佛和斯坦福大学合著630页新书

https://people.csail.mit.edu/madry/6.883/

6.883 Science of Deep Learning: Bridging Theory and Practice

https://fullstackdeeplearning.com/march2019

UCB: Full Stack Deep Learning。这个课程是专门讲述如何部署DL工程到实际产品中。

http://web.stanford.edu/class/cs246/

CS246: Mining Massive Data Sets

https://mp.weixin.qq.com/s/P7szDhvn5QNRKc7Iuq3Q6g

UAI 2019教程《深度学习数学基础》

https://mp.weixin.qq.com/s/By6xBnUgCKkK3b7pU7E1mA

宾夕法尼亚大学：面向计算机视觉、机器人和机器学习的线性代数 - 附749页书籍PDF

https://mp.weixin.qq.com/s/i79W79Z3ujRnHRBl81kvvA

最新版《机器学习数学基础》发布，417页PDF免费下载

https://mp.weixin.qq.com/s/HBUI9-jMa3otnB-Qq_eg5A

悉尼科大徐亦达教授：机器学习讲义，32份主题推介

https://mp.weixin.qq.com/s/KCFJdk8fZ1rmRI9bZSQykQ

微软研究院新版书籍《数据科学基础》，附479页PDF下载

# 策略梯度

## Policy Gradient Theorem（续）

>在监督学习里，当前状态、行为的好坏由监督信息告知；而在强化学习里，则需要通过价值函数来估计当前状态行为的好坏。

我们将Policy Gradient应用到MC算法上，则得到如下流程：

>Initialise $$\theta$$ arbitrarily   
>>for each episode $$\{s_1,a_1,r_2,\dots,s_{T−1},a_{T−1},r_T\}\sim \pi_\theta$$ {   
>>>for t = 1 to $$T−1$$ {   
>>>$$\theta \leftarrow \theta + \alpha\nabla_\theta \log \pi_\theta (s_t , a_t )v_t$$   
>>>}   
>>
>>}   
>
>return $$\theta$$

在基于策略的学习算法中，算法挑选策略的时候不需使用$$\epsilon$$-贪婪搜索，策略是直接根据参数$$\theta$$得到的。同时在对策略参数更新时有一个学习率$$\alpha$$，它体现了在梯度方向上更新参数θ的步长（step size），一般的我们在更新参数时是按梯度方向只更新由$$\alpha$$确定的一定量。

打个比方，当前策略在更新时提示梯度方向倾向于选择“向左”的行为，那么在更新策略参数时，可以朝着向左的方向更新一定的值，如果这个$$\alpha$$取值增大，则导致决策朝着更容易选择“向左”的行为倾斜，这其实就相当于没有探索的贪婪决策行为。而只要学习在持续，就有可能因为梯度变化而尝试更多的行为，这一过程中参数$$\alpha$$控制了策略更新的平滑度。

如果基于价值函数制定策略，则使用查表（table look-up)的方式可以保证收敛到全局最优解。即使使用的是直接基于策略的学习方法；但是,如果使用的是通用化的近似函数表示方法，比如神经网络等，则无论是基于价值函数，还是基于策略，都可能陷入局部最优解。

## 参考

https://zhuanlan.zhihu.com/p/54825295

基于值和策略的强化学习入坑

https://blog.csdn.net/gsww404/article/details/80705950

策略梯度（Policy Gradient）

# Actor-Critic

## 概述

MC策略梯度方法使用了收获作为状态价值的估计，它虽然是无偏的，但是噪声却比较大，也就是变异性（方差）较高。如果我们能够相对准确地估计状态价值，用它来指导策略更新，那么是不是会有更好的学习效果呢？这就是Actor-Critic策略梯度的主要思想。

![](/images/img3/Actor-critic.png)

Actor-Critic的字面意思是“演员-评论”，相当于演员在演戏的同时，有评论家指点，继而演员演得越来越好。即使用Critic来估计行为价值：

$$Q_w(s,a)\approx Q^{\pi_\theta}(s,a)$$

基于Actor-Critic策略梯度学习分为两部分内容：

1.Critic：更新action-value函数的参数w。

2.Actor：按照Critic得到的价值，引导策略函数参数$$\theta$$的更新。

$$\nabla_\theta J(\theta)\approx E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

$$\Delta \theta = \alpha \nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)$$

可以看出，Critic做的事情其实是我们已经见过的：策略评估，他要告诉个体，在由参数$$\theta$$确定的策略$$\pi_\theta$$到底表现得怎么样。

## Compatible Function Approximation

近似策略梯度的方法引入了Bias，从而得到的策略梯度不一定能最后找到较好的解决方案，例如当近似价值函数引起状态重名的特征时。

幸运的是，如果我们小心设计近似函数，是可以避免引入bias的。该近似函数需要满足下面两个条件:

- 近似价值函数的梯度完全等同于策略函数对数的梯度，即不存在重名情况：

$$\nabla_w Q_w(s,a)=\nabla_\theta \log \pi_\theta(s,a)$$

- 价值函数参数w使得均方差最小：

$$\epsilon = E_{\pi_\theta}[(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2]$$

符合这两个条件，则认为策略梯度是准确的，即：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

## Reducing Variance Using Baseline

除了bias之外，variance也是需要考虑的方面。

我们从策略梯度里抽出一个基准函数B(s)，要求这一函数仅与状态有关，而与行为无关，因而不改变梯度本身。

$$E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)B(s)] = \sum_{s\in S} d^{\pi_\theta}(s)\sum_{a\in A}\nabla_\theta \pi_\theta(s,a)B(s)\\= \sum_{s\in S} d^{\pi_\theta}(s)B(s) \nabla_\theta \sum_{a\in A} \pi_\theta(s,a) = 0$$

由于B(s)与行为无关，可以将其从针对行为a的求和中提出来，同时我们也可以把梯度从求和符号中提出来，而最后一个求和项：策略函数针对所有行为的求和，根据策略函数的定义，这里的结果肯定是1。而常数的梯度是0，因此总的结果等于0。

原则上，和行为无关的函数都可以作为B(s)。一个很好的B(s)就是基于当前状态的状态价值函数：$$V^{\pi_\theta}(s)$$。

所以，我们可以用一个advantage function来改写策略梯度：

$$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$$

上式的现实意义在于评估当个体采取行为a离开s状态时，究竟比该状态s总体平均价值要好多少？

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]$$

advantage function可以明显减少状态价值的variance，因此，现在Critic的任务就改为估计advantage function。

在这种情况下，我们需要两个近似函数也就是两套参数，一套用来近似状态价值函数，一套用来近似行为价值函数。即：

$$V_v(s)\approx V^{\pi_\theta}(s)$$

$$Q_w(s,a)\approx Q^{\pi_\theta}(s,a)$$

$$A(s,a) = Q_w(s,a) - V_v(s)$$

不过实际操作时，我们并不需要计算两个近似函数。这里以TD学习为例说明一下。

根据定义，TD误差$$\delta^{\pi_\theta}$$可以根据真实的状态价值函数$$V^{\pi_\theta}(s)$$算出：

$$\delta^{\pi_\theta} = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)$$

因为：

$$\begin{align}
E_{\pi_\theta}[\delta^{\pi_\theta} | s,a] &= E_{\pi_\theta}[r + \gamma V^{\pi_\theta}(s') | s,a] - V^{\pi_\theta}(s)\\
&= Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s) \\
&= A^{\pi_\theta}(s,a)
\end{align}$$

可见$$\delta^{\pi_\theta}$$就是$$A^{\pi_\theta}(s,a)$$的一个无偏估计。

因此，我们就可以使用TD误差来计算策略梯度：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\delta^{\pi_\theta}]$$

实际运用时，我们使用一个近似的TD误差，即用状态函数的近似函数来代替实际的状态函数：

$$\delta_v = r + \gamma V_v(s') - V_v(s)$$

这也就是说，我们只需要一套参数描述状态价值函数，而不再需要行为价值函数了。

上述方法不仅可用于TD方法，还可用于MC方法等，以下不加讨论的给出如下结论：

$$\begin{align}
\nabla_\theta J(\theta) &= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{v_t}] & \text{REINFORCE}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{Q_w(s,a)}] & \text{Q Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{A_w(s,a)}] & \text{Advantage Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{\delta}] & \text{TD Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{\delta e}] & \text{TD(}\lambda\text{) Actor-Critic}
\end{align}$$

## 参考

https://zhuanlan.zhihu.com/p/51652845

强化学习这都学不会的话，咳咳，你过来下！

https://zhuanlan.zhihu.com/p/70360272

最前沿：深度解读Soft Actor-Critic算法

https://mp.weixin.qq.com/s/ce9W3FbLdsqAEyvw6px_RA

Actor Critic——一个融合基于策略梯度和基于值优点的强化学习算法

https://mp.weixin.qq.com/s/c4xoy4CJ_hsVdmGe1n3rTQ

A3C——一种异步强化学习方法

https://mp.weixin.qq.com/s/5kI72vg4JNAZWD93EYAUWA

直观的强化学习算法(A2C)

# Integrating Learning and Planning

前面的章节主要介绍了Model-Free RL，下面将讲一下Model-Based RL，主要包括如下内容：

- 如何从经历中直接学习Model。

- 如何基于模型来进行Planning（规划）。

- 如何将“学习”和“规划”整合起来。

![](/images/img3/Model_Free_RL.png) | ![](/images/img3/Model_RL.png)

上面两图形象的说明了Model-Free RL（左图）和Model-Based RL（右图）的差别。

## Model-Based RL

![](/images/img3/Model_RL_2.png)

上图比较好的说明了模型学习在整个RL学习中的位置和作用。

我们首先看一下Model的定义：Model是一个参数化（参数为$$\eta$$）的MDP<S, A, P, R>，其中假定：状态空间Ｓ和行为空间Ａ是已知的，则$$M=<P_\eta, R_\eta>$$，其中$$P_\eta \approx P, R_\eta \approx R$$。则：

$$S_{t+1} \sim P_\eta (S_{t+1} | S_t, A_t)$$

$$R_{t+1} = R_\eta (R_{t+1} | S_t, A_t)$$

通常我们需要如下的假设，即状态转移函数和奖励函数是条件独立的：

$$P[S_{t+1}, R_{t+1} | S_t, A_t] = P[S_{t+1} | S_t, A_t] P[R_{t+1} | S_t, A_t]$$
