---
layout: post
title:  名校机器学习相关课程（二）
category: resource 
---

# 名校机器学习相关课程

https://sites.google.com/view/berkeley-cs294-158-sp19/home

UCB CS294-158: Deep Unsupervised Learning

https://github.com/mbinary/USTC-CS-Courses-Resource

中国科学技术大学计算机学院课程资源

https://mp.weixin.qq.com/s/iQpERylxzfQ0P51AQBj8bA

概率论导论-哈佛和斯坦福大学合著630页新书

https://people.csail.mit.edu/madry/6.883/

6.883 Science of Deep Learning: Bridging Theory and Practice

https://fullstackdeeplearning.com/march2019

UCB: Full Stack Deep Learning。这个课程是专门讲述如何部署DL工程到实际产品中。

http://web.stanford.edu/class/cs246/

CS246: Mining Massive Data Sets

https://mp.weixin.qq.com/s/P7szDhvn5QNRKc7Iuq3Q6g

UAI 2019教程《深度学习数学基础》

https://mp.weixin.qq.com/s/By6xBnUgCKkK3b7pU7E1mA

宾夕法尼亚大学：面向计算机视觉、机器人和机器学习的线性代数 - 附749页书籍PDF

https://mp.weixin.qq.com/s/i79W79Z3ujRnHRBl81kvvA

最新版《机器学习数学基础》发布，417页PDF免费下载

https://mp.weixin.qq.com/s/HBUI9-jMa3otnB-Qq_eg5A

悉尼科大徐亦达教授：机器学习讲义，32份主题推介

# Actor-Critic

## 概述（续）

基于Actor-Critic策略梯度学习分为两部分内容：

1.Critic：更新action-value函数的参数w。

2.Actor：按照Critic得到的价值，引导策略函数参数$$\theta$$的更新。

$$\nabla_\theta J(\theta)\approx E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

$$\Delta \theta = \alpha \nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)$$

可以看出，Critic做的事情其实是我们已经见过的：策略评估，他要告诉个体，在由参数$$\theta$$确定的策略$$\pi_\theta$$到底表现得怎么样。

## Compatible Function Approximation

近似策略梯度的方法引入了Bias，从而得到的策略梯度不一定能最后找到较好的解决方案，例如当近似价值函数引起状态重名的特征时。

幸运的是，如果我们小心设计近似函数，是可以避免引入bias的。该近似函数需要满足下面两个条件:

- 近似价值函数的梯度完全等同于策略函数对数的梯度，即不存在重名情况：

$$\nabla_w Q_w(s,a)=\nabla_\theta \log \pi_\theta(s,a)$$

- 价值函数参数w使得均方差最小：

$$\epsilon = E_{\pi_\theta}[(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2]$$

符合这两个条件，则认为策略梯度是准确的，即：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

## Reducing Variance Using Baseline

除了bias之外，variance也是需要考虑的方面。

我们从策略梯度里抽出一个基准函数B(s)，要求这一函数仅与状态有关，而与行为无关，因而不改变梯度本身。

$$E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)B(s)] = \sum_{s\in S} d^{\pi_\theta}(s)\sum_{a\in A}\nabla_\theta \pi_\theta(s,a)B(s)\\= \sum_{s\in S} d^{\pi_\theta}(s)B(s) \nabla_\theta \sum_{a\in A} \pi_\theta(s,a) = 0$$

由于B(s)与行为无关，可以将其从针对行为a的求和中提出来，同时我们也可以把梯度从求和符号中提出来，而最后一个求和项：策略函数针对所有行为的求和，根据策略函数的定义，这里的结果肯定是1。而常数的梯度是0，因此总的结果等于0。

原则上，和行为无关的函数都可以作为B(s)。一个很好的B(s)就是基于当前状态的状态价值函数：$$V^{\pi_\theta}(s)$$。

所以，我们可以用一个advantage function来改写策略梯度：

$$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$$

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]$$

advantage function可以明显减少状态价值的variance，因此，现在Critic的任务就改为估计advantage function。

在这种情况下，我们需要两个近似函数也就是两套参数，一套用来近似状态价值函数，一套用来近似行为价值函数。即：

$$V_v(s)\approx V^{\pi_\theta}(s)$$

$$Q_w(s,a)\approx Q^{\pi_\theta}(s,a)$$

$$A(s,a) = Q_w(s,a) - V_v(s)$$



## 参考

https://zhuanlan.zhihu.com/p/51652845

强化学习这都学不会的话，咳咳，你过来下！

https://zhuanlan.zhihu.com/p/70360272

最前沿：深度解读Soft Actor-Critic算法

https://mp.weixin.qq.com/s/ce9W3FbLdsqAEyvw6px_RA

Actor Critic——一个融合基于策略梯度和基于值优点的强化学习算法

https://mp.weixin.qq.com/s/c4xoy4CJ_hsVdmGe1n3rTQ

A3C——一种异步强化学习方法

# 模仿学习

https://zhuanlan.zhihu.com/p/27935902

机器人学习Robot Learning之模仿学习Imitation Learning的发展

https://zhuanlan.zhihu.com/p/25688750

模仿学习（Imitation Learning）完全介绍

https://mp.weixin.qq.com/s/naq73D27vsCOUBperKto8A

从监督式到DAgger，综述论文描绘模仿学习全貌

https://mp.weixin.qq.com/s/LNNqp2KsEAljG26hY43mUw

ICML2018 模仿学习教程

https://mp.weixin.qq.com/s/f9vSgH1HQwGXBDb0UGHQyQ

深度学习进阶之无人车行为克隆

# RL与神经科学

Pavlov Model（1901）

Rescorla-Wagner Model（1972）

Thorndike’s Puzzle Box（1910）

参考：

https://zhuanlan.zhihu.com/p/24437724

学习理论之Rescorla-Wagner模型

# RL参考资源

https://mp.weixin.qq.com/s/Fn1s9Ia8L1ckgn6iP24FhQ

如何让神经网络具有好奇心

https://mp.weixin.qq.com/s/PBf-YrkhwhPYXuiOGyahxQ

强化学习遭遇瓶颈！分层RL将成为突破的希望

https://zhuanlan.zhihu.com/p/58815288

强化学习之值函数学习

https://mp.weixin.qq.com/s/8Cqknze_iosz6Z6cqnuK5w

谷歌提出强化学习新算法SimPLe，模拟策略学习效率提高2倍

https://mp.weixin.qq.com/s/hKGS4Ek5prwTRJoMCaxQLA

强化学习Exploration漫游

https://zhuanlan.zhihu.com/p/65116688

值分布强化学习（01）

https://zhuanlan.zhihu.com/p/65364484

值分布强化学习（02）

https://zhuanlan.zhihu.com/p/62363784

强化学习之策略搜索

https://mp.weixin.qq.com/s/j9Cs5M9gyITu2u_XDkKm-g

Policy Gradient——一种不以loss来反向传播的策略梯度方法
