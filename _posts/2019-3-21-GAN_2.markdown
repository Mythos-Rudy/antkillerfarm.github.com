---
layout: post
title:  GAN（二）——GAN进阶, DCGAN, WGAN-GP
category: GAN & VAE 
---

# 概况

## WGAN（续）

论文：

《Wasserstein GAN》

此外，还有使用最小二乘代替JS散度的GAN——LSGAN。

参考：

https://zhuanlan.zhihu.com/p/25071913

令人拍案叫绝的Wasserstein GAN

https://zhuanlan.zhihu.com/p/25204020

条条大路通罗马LS-GAN：把GAN建立在Lipschitz密度上

## 其他细节

1.GAN的目的是在高维非凸的参数空间中找到纳什均衡点，GAN的纳什均衡点是一个鞍点。而SGD只会找到局部极小值，所以GAN中的优化器不常用SGD。

## 参考

https://mp.weixin.qq.com/s/xa3F3kCprE6DEQclas4umg

GAN的数学原理

http://www.jianshu.com/p/e2d2d7cbbe49

50行代码实现GAN

https://mp.weixin.qq.com/s/pgWysIGObceGVrxs85kyew

白话生成对抗网络GAN，50行代码玩转GAN模型！

https://mp.weixin.qq.com/s/YnOF9CCUFvtaiTY8HXYOuw

深入浅出：GAN原理与应用入门介绍

http://blog.csdn.net/u011534057/article/category/6396518

GAN系列blog

https://mp.weixin.qq.com/s/4CypEZscTfmUzOk-p_rZog

生成对抗网络初学入门：一文读懂GAN的基本原理

http://mp.weixin.qq.com/s/bzwG0QxnP2drqS4RwcZlBg

微软详解：到底什么是生成式对抗网络GAN？

https://mp.weixin.qq.com/s/GobKiuxgZv0-ufSRBpTcIA

Ian Goodfellow ICCV2017演讲：解读GAN的原理与应用

https://mp.weixin.qq.com/s/NDZPPA-0FhqSzRndQOhNEw

Google GAN之父Ian Goodfellow 最新演讲：生成对抗网络介绍

https://mp.weixin.qq.com/s/TIgRVbnZYtrGUCDNLcL1uw

GAN的入门与实践

https://mp.weixin.qq.com/s/er-VG1P8iNIcQew2gXZqGw

一文读懂生成对抗网络

https://mp.weixin.qq.com/s/BNWPTPl_vowAlQUQ7__wvQ

有三说GANs

https://zhuanlan.zhihu.com/p/24897387

GAN的基本原理、应用和走向

https://mp.weixin.qq.com/s/YUMIL-f019vKpQ84mKS-8g

这篇TensorFlow实例教程文章告诉你GANs为何引爆机器学习？

https://zhuanlan.zhihu.com/p/27012520

从头开始GAN

https://mp.weixin.qq.com/s/uyn41vKKoptXPZXBP2vVDQ

生成对抗网络（GAN）之MNIST数据生成

http://mp.weixin.qq.com/s/21CN4hAA6p7ZjWsO1sT2rA

一文看懂生成式对抗网络GANs：介绍指南及前景展望

https://mp.weixin.qq.com/s/Lll9e5bCVwxdD0-Q2wamKg

一文详解生成对抗网络(GAN)的原理

https://mp.weixin.qq.com/s/Qso0pv0NjtNrYhZR-sV2xg

通俗了解对抗生成网络(GAN)核心思想

https://mp.weixin.qq.com/s/YLys6L9WT7eCC-xGr1j0Iw

带多分类判别器的GAN模型

https://mp.weixin.qq.com/s/lqQeCpLQVqSdJPWx0oxs2g

例解生成对抗网络

https://mp.weixin.qq.com/s/LAS0KgPiVekGdQXbqlw1cQ

深度学习的三大生成模型：VAE、GAN、GAN的变种

https://mp.weixin.qq.com/s/mPtv1fQd0NBgdY2b_ALNTQ

机器之心GitHub项目：GAN完整理论推导与实现，Perfect！

https://mp.weixin.qq.com/s/uUSq3irEIcBM35JCYGDPfw

生成对抗网络综述：从架构到训练技巧，看这篇论文就够了

https://mp.weixin.qq.com/s/d_W0O7LNqlBuZV87Ou9uqw

训练GAN的16个技巧

https://mp.weixin.qq.com/s/uJlgx9Bq-XI49l8wwmdIsw

用GAN让晴天下大雨，小猫变狮子，黑夜转白天

https://mp.weixin.qq.com/s/hjsFBVE3_IiKTZSesa44ug

GAN系列学习(1)——前生今世

https://mp.weixin.qq.com/s/JRyQ5vp_zDwcG3X15e32Gw

GAN系列学习(2)——前生今世

https://mp.weixin.qq.com/s/FL63vEAhp8mElI5RFxnbSQ

GAN开山之作及最新综述

https://mp.weixin.qq.com/s/A66WeHH77IOCv61RHiDE0w

生成式对抗网络（GAN）如何快速理解？这里有一篇最直观的解读

# GAN进阶

## Structured Learning

机器学习模型所预测的对象，不仅有值（例如线性回归模型），或者类别（例如逻辑回归模型），也可以是一个复杂的结构，例如Sequence、List、Tree、Image、Bounding Box等。

输出结果是复杂结构的ML，通常被称作Structured Learning。

Structured Learning有以下特点：

1.在分类任务中，每一个类别都有一定的样例Sample，但是在结构化预测中，如果把每一个output（例如一个sequence）当做一个类别来看的话，输出空间就非常大了，因为测试数据的输出很有可能都是训练数据没有见过的（毕竟完全相同的sequence会比较少）。

2.模型需要有planning的能力，以组合各种有依赖的component，从而达到全局最优。

参考：

https://www.cnblogs.com/bluemapleman/p/9277175.html

结构化学习（Structured Learning）

## 传统GAN的局限

**1.没有用户控制（user control）能力。**

在传统的GAN里，输入一个随机噪声，就会输出一幅随机图像。

但用户是有想法的，我们想输出的图像是我们想要的那种图像，和我们的输入是对应的、有关联的。比如输入一只喵的草图，输出同一形态的喵的真实图片。

**2. 低分辨率（Low resolution）和低质量（Low quality）问题。**

## GAN的发展

最早的GAN出现在2014年6月，但直到2015年底，也只有5个变种，发展并不迅速。

2016年，GAN开始发力，年底时已有52个变种。2017年6月底，更达到142个变种。

参考：

https://github.com/hindupuravinash/the-gan-zoo

GAN的各种变种

GAN的变种（或者说改进）主要分为两大类：

**1.修改Loss。**这一类的典型就是前述的WGAN。Sebastian Nowozin提出的f-GAN，对这一类改进进行了总结。

参见：

https://mp.weixin.qq.com/s/f93aCYrxlBFhRn0bgPDiTg

微软剑桥研究院153页最新GAN教程

>Sebastian Nowozin，微软剑桥研究院首席研究员。

**2.修改网络结构。**

![](/images/article/GAN_structure.png)

上图的源地址：

https://github.com/hwalsuklee/tensorflow-generative-model-collections

![](/images/img2/GAN.jpg)

![](/images/img2/GAN_2.jpg)

《Generating Reasonable and Diversified Story Ending Using Sequence to Sequence Model with Adversarial Training》，这篇论文提出的Seq2Seq+RL+GAN的组合模型，也算是让我开眼界了。

![](/images/img2/Seq2Seq_RL_GAN.png)

## 参考

https://zhuanlan.zhihu.com/p/58812258

GAN万字长文综述

https://mp.weixin.qq.com/s/gH6b5zgvWArOSfKBSIG1Ww

必读！生成对抗网络GAN论文TOP 10

https://zhuanlan.zhihu.com/p/34016536

历史最全GAN网络及其各种变体整理

https://mp.weixin.qq.com/s/rE_RYklz8G_Etg5Gstlhew

十款神奇的GAN，总有一个适合你！

https://mp.weixin.qq.com/s/N7YU-YeXiVX7gSB-mzYgnw

生成式对抗网络GAN的研究进展与展望

https://mp.weixin.qq.com/s/IjlIT-3FVY7IfYzNDtkkgg

谷歌大脑发布GAN全景图：看百家争鸣的生成对抗网络

https://mp.weixin.qq.com/s/Y4Ags-yupq__gQZmparhsg

COLING 2018⽤对抗增强的端到端模型⽣成合理且多样的故事结尾

https://mp.weixin.qq.com/s/df51nYaA-uz7vd4WbBuE8g

GAN货：生成对抗网络知识资料全集

https://mp.weixin.qq.com/s/QRmy8f88eJcdp1xCxCI8bg

历数GAN的5大基本结构

https://mp.weixin.qq.com/s/LHCEh4BPZ_qbSKaar19nNg

十种主流GANs，我该如何选择？

https://mp.weixin.qq.com/s/RAlQVWMBYeddG2Mvu2bF4w

生成对抗网络（GANs）最新家谱：为你揭秘GANs的前世今生

https://mp.weixin.qq.com/s/NPUJ89nddF1WHfFv2nn-Hg

GANs有嘻哈：一次学完10个GANs明星模型

## GAN的理论解释

顾险峰教授对GAN提出了自己的理论解释。

>顾险峰，1989年考入清华大学计算机科学与技术系。1992年获得清华大学特等奖学金。后于美国哈佛大学获得计算机博士学位，师从国际著名微分几何大师丘成桐先生。目前为美国纽约州立大学石溪分校计算机系终身教授。

参考：

https://mp.weixin.qq.com/s/7O0AKIUVYK7HRyvdRbUVkg

虚构的对抗，GAN with the wind

https://mp.weixin.qq.com/s/trvMOTXNs7L6fSmTkZXwsA

看穿机器学习（W-GAN模型）的黑箱

https://mp.weixin.qq.com/s/thcxsBVttSIEzVNLQlAVCA

看穿机器学习的黑箱（II）

https://mp.weixin.qq.com/s/Jx0o17CwlIVcRV22PXk4wQ

看穿机器学习的黑箱（III）

https://mp.weixin.qq.com/s/ecqPzcSa75U9n4BcvnNJ_Q

GAN模式崩溃的理论解释

## GAN的评估指标

尽管可用的GAN模型非常多，但对它们的评估仍然主要是定性评估，通常需要借助人工检验生成图像的视觉保真度来进行。此类评估非常耗时，且主观性较强、具备一定误导性。鉴于定性评估的内在缺陷，恰当的定量评估指标对于GAN的发展和更好模型的设计至关重要。

参考：

https://mp.weixin.qq.com/s/EPwsQ_005CYNlCK_82SYWQ

六种GAN评估指标的综合评估实验，迈向定量评估GAN的重要一步

# DCGAN

论文：

《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》

![](/images/img2/DCGAN.png)

DCGAN的改进主要是：

1.使用步长卷积代替上采样层，卷积在提取图像特征上具有很好的作用，并且使用卷积代替全连接层。

2.上图是G网络的结构图，而D网络和G网络的结构基本是对称的。

3.在判别器中使用leakrelu激活函数，而不是RELU，防止梯度稀疏，生成器中仍然采用relu，但是输出层采用tanh。

参考：

https://mp.weixin.qq.com/s/Dz5c32SnM8-Pdjb9oWxZ_Q

想实现DCGAN？从制作一张门票谈起！

https://mp.weixin.qq.com/s/ouLWl623r_YaZdIdpqSWcw

深度卷积对抗生成网络(DCGAN)实战

# WGAN-GP

论文：

《Improved Training of Wasserstein GANs》

![](/images/img2/WGAN_GP.png)

在某些情况下，weight clipping会导致weight集中在两端，这样整个网络就退化成二值网络了。为了改善这一点，WGAN-GP提出了一种叫做gradient penalty的办法，来取代weight clipping。

weight clipping对于超出limit的值，采用了简单粗暴clipping方式。而gradient penalty则偏于软性的“惩罚”，也就是说：对于超出limit的值，允许其存在，但要惩罚一下，使之靠近limit。距离越远，惩罚力度越大。

Lipschitz约束只关心所有的梯度都小于C，但C的值是多少，其实并不care。gradient penalty虽然并不能把所有值都约束在limit之内，但是梯度总归不会是无穷大，因此还是满足Lipschitz约束的。

参考：

https://mp.weixin.qq.com/s/aSQ2-QxbToGF0ROyjxw2yw

萌物生成器：如何使用四种GAN制造猫图

https://mp.weixin.qq.com/s/h7lrJYQ_RqJDako8UoYK-A

六种改进均未超越原版：谷歌新研究对GAN现状提出质疑
