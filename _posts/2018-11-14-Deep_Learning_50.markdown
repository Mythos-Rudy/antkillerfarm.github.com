---
layout: post
title:  深度学习（五十）——GAN进阶（2）, 图像超分辨率进阶, 深度推理, DNC, Transformer, DMN
category: DL 
---

# GAN进阶

https://mp.weixin.qq.com/s/QacQCrjh3KmrQSMp-G_rEg

贝叶斯生成对抗网络

https://mp.weixin.qq.com/s/c84LMFnIhoDeolc1B4MIVA

AI以假乱真怎么办？TequilaGAN教你轻松辨真伪

https://mp.weixin.qq.com/s/fgL6FtjeF-EgG5jjAGDR7A

GANimation让图片秒变GIF表情包，秒杀StarGAN

https://mp.weixin.qq.com/s/NPUJ89nddF1WHfFv2nn-Hg

GANs有嘻哈：一次学完10个GANs明星模型

https://mp.weixin.qq.com/s/yShYrMFKox30jXajXXQPGw

如何让GAN生成更高质量图像？斯坦福大学给你答案

https://mp.weixin.qq.com/s/qiLFQowjH67XECXBlppUDg

对抗深度学习:鱼(模型准确性)与熊掌(模型鲁棒性)能否兼得？

https://mp.weixin.qq.com/s/1SpHGtjkSfDEFCZuudCm0Q

基于GAN和VAE的跨模态图像生成

https://mp.weixin.qq.com/s/02amaVnLFxeLDBsjG-iN1Q

UBC&腾讯AI Lab提出首个模块化GAN架构，搞定任意图像PS组合

https://mp.weixin.qq.com/s/pf0fNSoNDaI9bZvohRX28A

不再使用人眼评估，你训练的GAN还OK吗？ 

https://mp.weixin.qq.com/s/KeXXi5kwvWAfArv13f6VPg

给Cycle-GAN加上时间约束，CMU等提出新型视频转换方法Recycle-GAN

https://mp.weixin.qq.com/s/IzVTkH7fEiS4gAUIyA_IrA

谷歌GAN 实验室来了！迄今最强可视化工具，在浏览器运行GAN

https://mp.weixin.qq.com/s/1jBpz55pPgM8oxlUZLSWXA

ICML2018对抗生成网络论文评述

https://mp.weixin.qq.com/s/uQpmP7pJ8wnEgyJrvvHgwg

基于解剖结构的面部表情生成

https://mp.weixin.qq.com/s/cLCtxS1frJYvC5tLsQAWrQ

用神经网络生成音乐

https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247485368&idx=1&sn=1da8bd2490fa2fe16dbe25aea79dcd63

交互式GAN Lab让生成对抗网络轻松实现可视化！

https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247491630&idx=1&sn=394ffec2969cff23f63022526684f259

杜伦大学提出GANomaly：无需负例样本实现异常检测

https://mp.weixin.qq.com/s/BEuA5icaQwRJGsJJLnAFVg

用机器学习生成图片：GAN的局限性以及如何GAN的更爽

https://zhuanlan.zhihu.com/p/41114883

手机照片脑补成超大画幅，这个GAN想象力惊人

https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650749368&idx=3&sn=fc20d9e6682c74227282df3133cea06c

基于IR-transformer、IRGAN模型，解读搜狗语义匹配技术

https://mp.weixin.qq.com/s/bKve_tZi9usz4oX0T3S15A

悉尼大学陶大程：遗传对抗生成网络有效解决GAN两大痛点

https://mp.weixin.qq.com/s/UO0pNLcwYN5tE5x_4azVJA

LSGAN：最小二乘生成对抗网络

https://zhuanlan.zhihu.com/p/46629127

生成对抗网络-GAN---一个好老师的重要性

https://mp.weixin.qq.com/s/Sp0EYvaq-1u0mtnrrmFNCQ

为什么说GANs是一个绝妙的艺术创作工具？

https://mp.weixin.qq.com/s/uHEAtuY1_KZdUAdDAwFi_A

以为GAN只能“炮制假图”？它还有这7种另类用途

https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492415&idx=1&sn=a359e72ee99555f7a2fb4e21b2ad51db

InfoGAN：一种无监督生成方法

https://mp.weixin.qq.com/s/Yf5quOXmzJAy0GnJnvam5g

台湾学者研发新型二元神经元GAN！有望用于AI作曲

https://mp.weixin.qq.com/s/8aL7COItG7lS4q5-3IZCmQ

定制人脸图像没那么难！使用TL-GAN模型轻松变脸

https://mp.weixin.qq.com/s/9t0GvQW-cmakM0E9dWxBcg

旧照片着色修复神器！自注意力GAN效果惊艳

https://mp.weixin.qq.com/s/cUFQ6EADa39h2eFoa_Dh0A

最高76%破解成功率！GAN已经能造出“万能指纹”，你的手机还安全吗？

https://mp.weixin.qq.com/s/_tABIMkWX8L5xQFmvPI7rw

有效稳定对抗模型训练过程，伯克利提出变分判别器瓶颈

https://zhuanlan.zhihu.com/p/50790727

SeqGAN: Sequence GAN with Policy Gradient

https://mp.weixin.qq.com/s/bH5yYbwq6NGQJ84xUDhoxg

生成对抗网络在图像翻译上的应用

# 图像超分辨率进阶

https://mp.weixin.qq.com/s/xpvGz1HVo9eLNDMv9v7vqg

NTIRE2017夺冠论文：用于单一图像超分辨率的增强型深度残差网络

https://www.zhihu.com/question/25401250

如何通过多帧影像进行超分辨率重构？

https://www.zhihu.com/question/38637977

超分辨率重建还有什么可以研究的吗？

https://zhuanlan.zhihu.com/p/25912465

胎儿MRI高分辨率重建技术：现状与趋势

https://mp.weixin.qq.com/s/i-im1sy6MNWP1Fmi5oWMZg

华为推出新型HiSR：移动端的超分辨率算法

https://mp.weixin.qq.com/s/h4Xzt-aS1_-5zjTB0ypTLg

普通视频转高清：10个基于深度学习的超分辨率神经网络

https://mp.weixin.qq.com/s/WmqagSGRy98USgnz21W3Pg

WDSR

https://mp.weixin.qq.com/s/AxHTaT-G5_Y6Iw_3aIIxCg

超分辨率技术如何发展？这6篇ECCV 18论文带你一次尽览

https://mp.weixin.qq.com/s/zWoQCKbZNz2td3cZxEsqKQ

腾讯优图提出SRN-DeblurNet：高效高质量去除复杂图像模糊

# 深度推理

https://mp.weixin.qq.com/s/RS39wqZDf6gEhOY6gcOVSA

74页教程融合逻辑推理和深度学习

https://mp.weixin.qq.com/s/NcUHJ89DPVdB7cJttsh-xw

基于神经网络的知识推理

https://mp.weixin.qq.com/s/ouHPvm4vQKga5sZfG_CHBw

DeepMind用深度学习模仿大脑推理，预测编码智能推进一大步！

https://zhuanlan.zhihu.com/p/28654835

视觉推理（Visual Reasoning），神经网络也可以有逻辑

https://mp.weixin.qq.com/s/HP8KLbo26W5UscCGavKDRw

IBM Watson提出人机推理网络HuMaINs，结合人机两者优势

https://mp.weixin.qq.com/s/210l9K94KMqtJtqbrZ-mgg

打开黑箱重要一步，MIT提出TbD-net，弥合视觉推理模型的性能与可解释性鸿沟

https://mp.weixin.qq.com/s/7NpJJn4k5oGPOB04S9Jyqw

DeepMind新论文，关联推理为什么是智能最重要的特征

http://mp.weixin.qq.com/s/6I0Z_yY7UT_7qOKOWcd7Mw

李飞飞发表研究新成果：视觉推理的推断和执行程序！

http://mp.weixin.qq.com/s/9mtgdNynv92FC2dA8-5KJA

VAE和Adam发明人博士论文：变分推理和深度学习

https://mp.weixin.qq.com/s/sbV5SL6fAGad5KlBoqUKFQ

斯坦福大学教授Christopher Manning提出全可微神经网络架构MAC：可用于机器推理

https://mp.weixin.qq.com/s/5vDSrWeUvlBuZgmX0R9pBQ

斯坦福“黑盒学习”研究：使用神经变分推理的无向图模型，可替代“采样”

https://mp.weixin.qq.com/s/XESTrLMERQPR9eXcFA56gg

神经符号系统：让机器善解人意

https://mp.weixin.qq.com/s/GYe1psxy1KMCbV7f3K8f4Q

神经规则引擎：让符号规则学会变通

https://mp.weixin.qq.com/s/-qc2MENppQmdClaIKV6Lww

深度推理学习中的图网络与关系表征

# LSM

liquid state machine (LSM)

http://www.docin.com/p-390935406.html

基于液体状态机的脑运动神经系统的建模研究

# DNC

https://zhuanlan.zhihu.com/p/27773709

浅析至强RNN可微分神经计算机(DNC)

https://zhuanlan.zhihu.com/p/27964341

浅析至强RNN可微分神经计算机(DNC)-2

https://zhuanlan.zhihu.com/p/28209628

DNC-3滚动分类的模式识别

https://zhuanlan.zhihu.com/p/28433712

DNC4广义线性回归

# Transformer

之前的文章已经介绍了Attention和《Attention is All You Need》。但实际上，《Attention is All You Need》不仅提出了两种Attention模块，而且还提出了如下图所示的Transformer模型。该模型主要用于NMT领域，由于Attention不依赖上一刻的数据，同时精度也不弱于LSTM，因此有很好并行计算特性，在工业界得到了广泛应用。阿里巴巴和搜狗目前的NMT方案都是基于Transformer模型的。

![](/images/img2/Transformer.png)

$$FFN(x) = \max(0,xW_1 + b_1)W_2 + b_2$$

代码：

https://github.com/Kyubyong/transformer

参考：

http://jalammar.github.io/illustrated-transformer/

The Illustrated Transformer

http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)

https://zhuanlan.zhihu.com/p/39034683

Attention is all you need模型笔记

https://zhuanlan.zhihu.com/p/40920384

真正的完全图解Seq2Seq Attention模型

https://mp.weixin.qq.com/s/HquT_mKm7x_rbDGz4Voqpw

阿里巴巴最新实践：TVM+TensorFlow提高神经机器翻译性能

https://mp.weixin.qq.com/s/S_xhaDrOaPe38ZvDLWl4dg

从技术到产品，搜狗为我们解读了神经机器翻译的现状

https://mp.weixin.qq.com/s/vzjKU_0qhapWKOYZ4Rnj-Q

谷歌的机器翻译模型Transformer，现在可以用来做任何事了

https://mp.weixin.qq.com/s/lgGDTCF3qg84njv2IeHC9A

大规模集成Transformer模型，阿里达摩院如何打造WMT 2018机器翻译获胜系统

https://mp.weixin.qq.com/s/_UC2jlOfb34tfB_tsEXjMg

谷歌全新神经网络架构Transformer：基于自注意力机制，擅长自然语言理解

https://mp.weixin.qq.com/s/w3IKoygTLDsAxk1MB5JrGg

详细讲解Transformer新型神经网络在机器翻译中的应用

https://mp.weixin.qq.com/s/HzzDG8PpDlyilQjr2PH6PA

Transformer注解及PyTorch实现（上）

https://mp.weixin.qq.com/s/YDaSv5oHLEtyJrp4Y5e64A

Transformer注解及PyTorch实现（下）

https://mp.weixin.qq.com/s/j0KRAOf8Sd0_tTlRadnw9Q

利用篇章信息提升机器翻译质量

# DMN

Question answering是自然语言处理领域的一个复杂问题。它需要对文本的理解力和推理能力。大部分NLP问题都可以转化为一个QA问题。Dynamic Memory Networks可以用来处理QA问题。DMN的输入包含事实输入，问题输入，经过内部处理形成片段记忆，最终产生问题的答案。

DMN可进行端到端的训练，并在多种任务上取得了state-of-the-art的效果：包括QA（Facebook 的 bAbI 数据集），情感分析文本分类（Stanford Sentiment Treebank）和词性标注（WSJ-PTB）。

![](/images/article/DMN.png)

参考：

http://blog.csdn.net/javafreely/article/details/71994247

动态记忆网络
