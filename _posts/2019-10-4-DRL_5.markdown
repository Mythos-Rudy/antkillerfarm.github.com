---
layout: post
title:  深度强化学习（五）——AlphaGo
category: DRL 
---

# OpenAI（续）

## RND

OpenAI最近开发了RND（Random Network Distillation），一种基于预测的强化学习算法，用于鼓励强化学习代理通过好奇心来探索他们所处环境。在游戏任务Montezuma's Revenge上首次超过人类的平均表现。

blog：

https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/

Reinforcement Learning with Prediction-Based Rewards

代码：

https://github.com/openai/random-network-distillation

# AlphaGo

## 概述

AlphaGo算是这波AI浪潮的里程碑事件了。如果说AlexNet让学术界重新认识了DL的话，AlphaGo则让大众都认识到了DL的威力。我也是在AlphaGo的感召之下，投身ML/DL领域的（2016.7）。因此，了解AlphaGo的原理，就成为了我一直以来的目标。岂料直到三年多之后（2019.11），我才能真正看懂AlphaGo。

在讲解原理之前，我们首先回顾一下AlphaGo的各个重要事件。

2015.10 AlphaGo Fan 5:0 击败樊麾。

2016.3 AlphaGo Lee 4:1 击败李世石。

樊麾讲解AlphaGo与李世石的五番棋：

https://deepmind.com/research/alphago/alphago-games-simplified-chinese/

2017.1 AlphaGo Master 60:0 击败各大高手。

2017.5 AlphaGo Ke 3:0 击败柯洁。

2017.10 AlphaGo Zero。

2017.12 AlphaZero：

训练4小时打败了国际象棋的最强程序Stockfish！

训练2小时打败了日本将棋的最强程序Elmo！

训练8小时打败了与李世石对战的AlphaGo Lee！

这里包含了AlphaGo一系列战役的棋谱：

http://www.alphago-games.com/

## DarkForestGo

DarkForestGo是田渊栋2015年11月的作品，虽然棋力和稍后的AlphaGo相去甚远，但毕竟也算是用到了RL和DNN了。

论文：

《Better Computer Go Player with Neural Network and Long-term Prediction》

代码：

https://github.com/facebookresearch/darkforestGo

DarkForest中的一些规则借鉴了开源围棋软件Pachi：

http://pachi.or.cz/

以下是作者本人的讲解：

https://zhuanlan.zhihu.com/p/20607684

AlphaGo的分析

![](/images/img3/DarkForestGo.png)

上图是DarkForest的网络结构图。其中的细节，我们将在讲解AlphaGo的时候，再细说。

## AlphaGo

论文：

《Mastering the game of Go with deep neural networks and tree search》

AlphaGo主要由几个部分组成：

1. 走棋网络（Policy Network），给定当前局面，预测/采样下一步的走棋。

2. 快速走子（Fast rollout），目标和1一样，但在适当牺牲走棋质量的条件下，速度要比1快1000倍。

3. 估值网络（Value Network），给定当前局面，估计是白胜还是黑胜。

4. 蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS)，把以上这三个部分连起来，形成一个完整的系统。

以下是详细的解说。

### Policy Network

![](/images/img3/AlphaGo.png)

上图是AlphaGo的Policy Network的网络结构图。

从结构来看，它与DarkForestGo是十分类似的：

1. 都是1层5x5的conv+k层3x3的conv。

2. 两者的input plane都是手工构建的特征。

3. 由于棋子的精确位置很重要，这些CNN中都没有pooling。

它们的差异在于：

1.DarkForestGo训练时，会预测三步而非一步，提高了策略输出的质量。

Policy Network摆脱了之前的基于规则的围棋软件，长于局部，但大局较差的弱点，它的大局观非常强，不会陷入局部战斗中。例如，DarkForestGo的走棋网络直接放上KGS就有3d的水平。

它的缺点是：会不顾大小无谓争劫，会无谓脱先，不顾局部死活，对杀出错，等等。有点像高手不经认真思考的随手棋——只有“棋感”，而没有计算。（其实更类似于计算力衰退的老棋手，比如聂棋圣。）

### Value Network

AlphaGo的Value Network也是一个和Policy Network几乎一样的深度卷积网络。

### Fast rollout

有了走棋网络，为什么还要做快速走子呢？

- 走棋网络的运行速度是比较慢的（3毫秒），而快速走子能做到几微秒级别，差了1000倍。

- 快速走子可以用来评估盘面。由于天文数字般的可能局面数，围棋的搜索是毫无希望走到底的，搜索到一定程度就要对现有局面做个估分。在没有估值网络的时候，不像国象可以通过算棋子的分数来对盘面做比较精确的估值，围棋盘面的估计得要通过模拟走子来进行，从当前盘面一路走到底，不考虑岔路地算出胜负，然后把胜负值作为当前盘面价值的一个估计。显然，如果一步棋在快速走子之后，生成的N个结果中的胜率较大的话，那它本身是步好棋的概率也较大。

为了速度快，Fast rollout没有使用神经网络，而是使用传统的局部特征匹配（local pattern matching）加线性回归（logistic regression）的方法。

这种方法虽然没有NN这么强，但还是比更为传统的基于规则的方案适应性好。毕竟规则是死的，而传统的机器学习，再怎么说也是可以自动学习规则的。当然了，这更比随机走子的效率高了。

由于Fast rollout既可以提供策略，又有一定的价值评估的手段，因此单独使用它，比单独使用Policy Network或者Value Network都要好。相当于是一个劣化版本的AlphaGo。

### MCTS

AlphaGo的MCTS使用的是传统的UCT算法，没太多好讲的。一个细节是Game Tree的结点并不是立即展开，而是要等到路过该结点的次数超过一定阈值，才进行展开，从而大大减小了搜索空间。

### 其他关键点

![](/images/img3/AlphaGo_2.png)

AlphaGo不是一个纯粹的DRL，它还是使用了人类棋谱的先验数据。

- 首先，从人类棋谱中学习rollout策略，并初始化Policy Network。

- 然后，使用自我博弈的方式，训练Policy Network和Value Network。

由于很多人类的棋局都是因为中间偶然的失误导致了全盘覆灭（所谓“一着不慎满盘皆输”），其中的偶然性非常大，局部的优劣往往和棋局的最终结果无关，因此Value Network并没有用人类棋谱来训练。

AlphaGo每更新一个“小版本”后，都要将这个版本和迄今最好的版本对比，如果新的版本胜率超过55%，才会用来取代以前最好的版本。这样做的显然的好处是防止AlphaGo自我博弈得“走火入魔”，陷入局部最优。

## AlphaGo Zero

论文：

《Mastering the game of Go without human knowledge》

![](/images/img3/AlphaGo_Zero.png)

AlphaGo Zero对AlphaGo进行了全面提升：

- input plane去掉了手工特征，基本全由历史信息组成。

- Policy Network和Value Network不再是两个同构的独立网络，而是合并成了一个网络，只是该网络有两个输出——Policy Head和Value Head。

- 骨干结构采用了Resnet，层数大大增加。

- 完全采用自我博弈，去掉了人类棋谱。

- 取消了Fast rollout。AlphaGo Zero的实践表明，如果有足够好的Value函数的话，MCTS的采样效率要远远高于传统的alpha-beta剪枝。因此，rollout也不是必须的。

>稍后的AlphaZero的实践表明：AlphaZero搜索80000个节点的棋力，已经超过了Stockfish搜索70000000个节点的棋力。

- Policy Gradient vs. Policy Iteration

AlphaGo依赖快速走子的结果，获得最终的结果信息。因此，它的奖励来源比较单一：只有对局的最终结果。这种做法实际上就是通常说的Policy Gradient。

但正如之前指出的：棋下输了，不意味着每步棋都是臭棋。因此，只使用最终结果，既会导致奖励稀疏，也不利于实时评估走子的价值。

AlphaGo Zero转而采用Policy Iteration方法，实时对盘面进行估计，不再依赖终局结果。

![](/images/img3/AlphaGo_Zero_2.jpg)

## AlphaZero

论文：

《Mastering Chess and Shogi by Self-Play with aGeneral Reinforcement Learning Algorithm》

AlphaZero相对于AlphaGo Zero的改进不算大，毕竟也就只差2个月。它的贡献在于，证明了DRL对于很多棋类都是有效的。

## MuZero

MuZero是DeepMind 2019年11月的作品。

论文：

《Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model》

MuZero在不具备任何底层动态知识的情况下，通过结合基于树的搜索和学得模型，在Atari 2600游戏中达到了SOTA表现，在国际象棋、日本将棋和围棋的精确规划任务中可以匹敌AlphaZero，甚至超过了提前得知规则的围棋版AlphaZero。

![](/images/img3/MuZero.png)

参考：

https://mp.weixin.qq.com/s/BLQF8WNsIe4a_rDeGWo_Vg

通用AlphaGo诞生？DeepMind的MuZero在多种棋类游戏中超越人类

## Leela Zero

Leela Zero是比利时人Gian-Carlo Pascutto开源的围棋AI。它的算法与AlphaGo Zero相同。而训练采用GTP协议，集合全球算力，进行分布式训练。

官网：

http://zero.sjeng.org/

代码：

https://github.com/gcp/leela-zero

>十多年前，当我还是一个中二青年的时候，就幻想有朝一日能够拿围棋世界冠军。当然，就算再中二，我自己也明白靠实力那是不可能的，当时做梦的法宝是制造一个AI，然后碾压一下所谓的国手。   
>按照当时(2000年前后)人们的预计，这个AI在2030年之前，都不可能造出来，然而，最终的结果实际上只花了一半左右的时间。   
>再之后，随着AI围棋的平民化，我的中二梦终于也有人将之付诸实现了：   
>https://mp.weixin.qq.com/s/npt2zZrKwPnNdY-hsa2RjQ   
>AI再乱围棋圈：“食言之战”柯洁落败；首例素人作弊引风波

这次作弊风波所使用的AI就是Leela Zero，可见目前（2018.5）它的棋力已经超过了顶尖棋手。

## ELF OpenGo

ELF OpenGo是Facebook开源的围棋AI，它是FB的AI游戏框架ELF的一部分。

官网：

https://github.com/pytorch/ELF

参考：

https://mp.weixin.qq.com/s/lOAx3suLIS-pEWyi8xZl6Q

“全民体验”AlphaZero：FAIR田渊栋首次开源超级围棋AI

## PhoenixGo

PhoenixGo是腾讯微信团队的AlphaGo Zero复刻版。

官网：

https://github.com/Tencent/PhoenixGo

参考：

https://mp.weixin.qq.com/s/tJDmxsuS1QigYS75ZIdzRA

微信团队开源围棋AI技术PhoenixGo，复现AlphaGo Zero论文

## 参考

https://blog.csdn.net/amds123/article/details/70666594

论文笔记：Mastering the game of Go with deep neural networks and tree search

https://mp.weixin.qq.com/s/Sfv-jzQAkN0PsZOGZUQhkQ

AlphaGo Zero横空出世，DeepMind Nature论文解密不使用人类知识掌握围棋
