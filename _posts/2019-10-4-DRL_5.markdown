---
layout: post
title:  深度强化学习（五）——AlphaGo（1）
category: DRL 
---

# OpenAI

## Gym（续）

官网：

https://gym.openai.com/

{% highlight bash %}
sudo apt install libffi-dev swig
git clone https://github.com/openai/gym 
cd gym
pip install -e . # minimal install
pip install -e .[all] # all install
{% endhighlight %}

这里选择minimal install就可以了，all install需要安装MuJoCo，而后者是收费软件。

和Gym配套的还有一个算法库：

https://github.com/openai/baselines

当然，看名字也知道这只是一个简单的算法库。

参考：

http://tech.163.com/16/0510/09/BMMOPSCR00094OE0.html

马斯克的AI野心——OpenAI Gym系统深度解析

https://mp.weixin.qq.com/s/KK1gwDW2EyptZOiuFjyAlw

OpenAI发布强化学习环境Gym Retro：支持千种游戏

https://blog.csdn.net/jinzhuojun/article/details/77144590

常用增强学习实验环境 I (MuJoCo, OpenAI Gym, rllab, DeepMind Lab, TORCS, PySC2)

https://blog.csdn.net/jinzhuojun/article/details/78508203

常用增强学习实验环境 II (ViZDoom, Roboschool, TensorFlow Agents, ELF, Coach等)

https://mp.weixin.qq.com/s/0oVG7zMi08dzMQrk43T3mw

像训练Dota2一样训练真实机器人？Gibson Environment环境了解一下

https://mp.weixin.qq.com/s/_A0q8DFAsIclaofVgZfjMA

定制股票交易OpenAI Gym强化学习环境

https://blog.csdn.net/gsww404/article/details/80627892

OpenAI-baselines的使用方法

## RND

OpenAI最近开发了RND（Random Network Distillation），一种基于预测的强化学习算法，用于鼓励强化学习代理通过好奇心来探索他们所处环境。在游戏任务Montezuma's Revenge上首次超过人类的平均表现。

blog：

https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/

Reinforcement Learning with Prediction-Based Rewards

代码：

https://github.com/openai/random-network-distillation

# AlphaGo

## 概述

AlphaGo算是这波AI浪潮的里程碑事件了。如果说AlexNet让学术界重新认识了DL的话，AlphaGo则让大众都认识到了DL的威力。我也是在AlphaGo的感召之下，投身ML/DL领域的（2016.7）。因此，了解AlphaGo的原理，就成为了我一直以来的目标。岂料直到三年多之后（2019.11），我才能真正看懂AlphaGo。

在讲解原理之前，我们首先回顾一下AlphaGo的各个重要事件。

2015.10 AlphaGo Fan 5:0 击败樊麾。

2016.3 AlphaGo Lee 4:1 击败李世石。

樊麾讲解AlphaGo与李世石的五番棋：

https://deepmind.com/research/alphago/alphago-games-simplified-chinese/

2017.1 AlphaGo Master 60:0 击败各大高手。

2017.5 AlphaGo Ke 3:0 击败柯洁。

2017.10 AlphaGo Zero。

2017.12 AlphaZero：

训练4小时打败了国际象棋的最强程序Stockfish！

训练2小时打败了日本将棋的最强程序Elmo！

训练8小时打败了与李世石对战的AlphaGo Lee！

这里包含了AlphaGo一系列战役的棋谱：

http://www.alphago-games.com/

## DarkForestGo

DarkForestGo是田渊栋2015年11月的作品，虽然棋力和稍后的AlphaGo相去甚远，但毕竟也算是用到了RL和DNN了。

论文：

《Better Computer Go Player with Neural Network and Long-term Prediction》

代码：

https://github.com/facebookresearch/darkforestGo

DarkForest中的一些规则借鉴了开源围棋软件Pachi：

http://pachi.or.cz/

以下是作者本人的讲解：

https://zhuanlan.zhihu.com/p/20607684

AlphaGo的分析

![](/images/img3/DarkForestGo.png)

上图是DarkForest的网络结构图。其中的细节，我们将在讲解AlphaGo的时候，再细说。

## AlphaGo

论文：

《Mastering the game of Go with deep neural networks and tree search》

AlphaGo主要由几个部分组成：

1. 走棋网络（Policy Network），给定当前局面，预测/采样下一步的走棋。

2. 快速走子（Fast rollout），目标和1一样，但在适当牺牲走棋质量的条件下，速度要比1快1000倍。

3. 估值网络（Value Network），给定当前局面，估计是白胜还是黑胜。

4. 蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS)，把以上这三个部分连起来，形成一个完整的系统。

以下是详细的解说。

### Policy Network

![](/images/img3/AlphaGo.png)

上图是AlphaGo的Policy Network的网络结构图。

从结构来看，它与DarkForestGo是十分类似的：

1. 都是1层5x5的conv+k层3x3的conv。

2. 两者的input plane都是手工构建的特征。

3. 由于棋子的精确位置很重要，这些CNN中都没有pooling。

它们的差异在于：

1.DarkForestGo训练时，会预测三步而非一步，提高了策略输出的质量。

Policy Network摆脱了之前的基于规则的围棋软件，长于局部，但大局较差的弱点，它的大局观非常强，不会陷入局部战斗中。例如，DarkForestGo的走棋网络直接放上KGS就有3d的水平。

它的缺点是：会不顾大小无谓争劫，会无谓脱先，不顾局部死活，对杀出错，等等。有点像高手不经认真思考的随手棋——只有“棋感”，而没有计算。（其实更类似于计算力衰退的老棋手，比如聂棋圣。）

### Value Network

AlphaGo的Value Network也是一个和Policy Network几乎一样的深度卷积网络。

### Fast rollout

有了走棋网络，为什么还要做快速走子呢？

- 走棋网络的运行速度是比较慢的（3毫秒），而快速走子能做到几微秒级别，差了1000倍。

- 快速走子可以用来评估盘面。由于天文数字般的可能局面数，围棋的搜索是毫无希望走到底的，搜索到一定程度就要对现有局面做个估分。在没有估值网络的时候，不像国象可以通过算棋子的分数来对盘面做比较精确的估值，围棋盘面的估计得要通过模拟走子来进行，从当前盘面一路走到底，不考虑岔路地算出胜负，然后把胜负值作为当前盘面价值的一个估计。显然，如果一步棋在快速走子之后，生成的N个结果中的胜率较大的话，那它本身是步好棋的概率也较大。

为了速度快，Fast rollout没有使用神经网络，而是使用传统的局部特征匹配（local pattern matching）加线性回归（logistic regression）的方法。

这种方法虽然没有NN这么强，但还是比更为传统的基于规则的方案适应性好。毕竟规则是死的，而传统的机器学习，再怎么说也是可以自动学习规则的。当然了，这更比随机走子的效率高了。

由于Fast rollout既可以提供策略，又有一定的价值评估的手段，因此单独使用它，比单独使用Policy Network或者Value Network都要好。相当于是一个劣化版本的AlphaGo。

### MCTS

AlphaGo的MCTS使用的是传统的UCT算法，没太多好讲的。一个细节是Game Tree的结点并不是立即展开，而是要等到路过该结点的次数超过一定阈值，才进行展开，从而大大减小了搜索空间。

### 其他关键点

![](/images/img3/AlphaGo_2.png)

AlphaGo不是一个纯粹的DRL，它还是使用了人类棋谱的先验数据。

- 首先，从人类棋谱中学习rollout策略，并初始化Policy Network。

- 然后，使用自我博弈的方式，训练Policy Network和Value Network。

由于很多人类的棋局都是因为中间偶然的失误导致了全盘覆灭（所谓“一着不慎满盘皆输”），其中的偶然性非常大，局部的优劣往往和棋局的最终结果无关，因此Value Network并没有用人类棋谱来训练。

AlphaGo每更新一个“小版本”后，都要将这个版本和迄今最好的版本对比，如果新的版本胜率超过55%，才会用来取代以前最好的版本。这样做的显然的好处是防止AlphaGo自我博弈得“走火入魔”，陷入局部最优。

## AlphaGo Zero

论文：

《Mastering the game of Go without human knowledge》

![](/images/img3/AlphaGo_Zero.png)

AlphaGo Zero对AlphaGo进行了全面提升：

- input plane去掉了手工特征，基本全由历史信息组成。

- Policy Network和Value Network不再是两个同构的独立网络，而是合并成了一个网络，只是该网络有两个输出——Policy Head和Value Head。

- 骨干结构采用了Resnet，层数大大增加。

- 完全采用自我博弈，去掉了人类棋谱。

- 取消了Fast rollout。AlphaGo Zero的实践表明，如果有足够好的Value函数的话，MCTS的采样效率要远远高于传统的alpha-beta剪枝。因此，rollout也不是必须的。

>稍后的AlphaZero的实践表明：AlphaZero搜索80000个节点的棋力，已经超过了Stockfish搜索70000000个节点的棋力。

- Policy Gradient vs. Policy Iteration

AlphaGo依赖快速走子的结果，获得最终的结果信息。因此，它的奖励来源比较单一：只有对局的最终结果。这种做法实际上就是通常说的Policy Gradient。

但正如之前指出的：棋下输了，不意味着每步棋都是臭棋。因此，只使用最终结果，既会导致奖励稀疏，也不利于实时评估走子的价值。

AlphaGo Zero转而采用Policy Iteration方法，实时对盘面进行估计，不再依赖终局结果。

![](/images/img3/AlphaGo_Zero_2.jpg)

## AlphaZero

论文：

《Mastering Chess and Shogi by Self-Play with aGeneral Reinforcement Learning Algorithm》

AlphaZero相对于AlphaGo Zero的改进不算大，毕竟也就只差2个月。它的贡献在于，证明了DRL对于很多棋类都是有效的。

## MuZero

MuZero是DeepMind 2019年11月的作品。

论文：

《Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model》

参考代码：

https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning

### 基本结构

MuZero在不具备任何底层动态知识的情况下，通过结合基于树的搜索和学得模型，在Atari 2600游戏中达到了SOTA表现，在国际象棋、日本将棋和围棋的精确规划任务中可以匹敌AlphaZero，甚至超过了提前得知规则的围棋版AlphaZero。

传统方法的局限：

- Model-based RL在Atari 2600游戏上表现不佳。这类游戏的Model很难刻画，规则比较抽象。

- Model-Free RL在棋类游戏上表现不佳。棋类的规则十分明确。

![](/images/img3/MuZero.png)

上图是MuZero和AlphaZero的网络结构对比图。从中可以看出：

1.AlphaZero只有一个网络。（虽然有两个用途：Policy和Value）

2.MuZero有三个网络：

- Prediction Network。这个和AlphaZero相同。

- Dynamics Network。

- Representation Network。
