---
layout: post
title:  GAN（三）——DCGAN, WGAN-GP, CGAN, BEGAN & EBGAN, Stack GAN, GAN Ensemble, Pix2Pix
category: GAN & VAE 
---

# DCGAN

论文：

《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》

![](/images/img2/DCGAN.png)

DCGAN的改进主要是：

1.使用步长卷积代替上采样层，卷积在提取图像特征上具有很好的作用，并且使用卷积代替全连接层。

2.不使用pooling层。

3.上图是G网络的结构图，而D网络和G网络的结构基本是对称的。

4.在判别器中使用leakrelu激活函数，而不是RELU，防止梯度稀疏，生成器中仍然采用relu，但是输出层采用tanh。

参考：

https://mp.weixin.qq.com/s/Dz5c32SnM8-Pdjb9oWxZ_Q

想实现DCGAN？从制作一张门票谈起！

https://mp.weixin.qq.com/s/ouLWl623r_YaZdIdpqSWcw

深度卷积对抗生成网络(DCGAN)实战

# WGAN-GP

论文：

《Improved Training of Wasserstein GANs》

![](/images/img2/WGAN_GP.png)

在某些情况下，weight clipping会导致weight集中在两端，这样整个网络就退化成二值网络了。为了改善这一点，WGAN-GP提出了一种叫做gradient penalty的办法，来取代weight clipping。

weight clipping对于超出limit的值，采用了简单粗暴clipping方式。而gradient penalty则偏于软性的“惩罚”，也就是说：对于超出limit的值，允许其存在，但要惩罚一下，使之靠近limit。距离越远，惩罚力度越大。

Lipschitz约束只关心所有的梯度都小于C，但C的值是多少，其实并不care。gradient penalty虽然并不能把所有值都约束在limit之内，但是梯度总归不会是无穷大，因此还是满足Lipschitz约束的。

参考：

https://mp.weixin.qq.com/s/aSQ2-QxbToGF0ROyjxw2yw

萌物生成器：如何使用四种GAN制造猫图

https://mp.weixin.qq.com/s/h7lrJYQ_RqJDako8UoYK-A

六种改进均未超越原版：谷歌新研究对GAN现状提出质疑

# CGAN

论文：

《Conditional Generative Adversarial Nets》

GAN不仅可用于无监督学习，也可用于有监督学习。Conditional GAN中的Condition实际上就是监督学习中的类别信息。

GAN首先是个生成模型，类别信息对于GAN的意义在于：**我不仅可以生成和数据集中样本类似的fake data，而且还可以指定它的类别。**

以MNIST数据集为例，GAN能生成数字，但生成之前，无法知道是哪个数字，而CGAN则可以按需生成。

类别信息和随机噪声的融合，可以采用组合编码（combined embedding）的方式，也可以采用hadamard product之类的互相关操作。

从数学角度看，GAN拟合的是$$p(data)$$，而CGAN拟合的是条件概率$$p(data \mid c)$$。

CGAN的D网络，除了预测数据的真假之外，还要预测数据的类别。

![](/images/img3/CGAN.png)

上图展示了监督学习（图左二）、GAN（图左三）和CGAN（图左四）的效果：

1.监督学习一般采用MSE loss，它学习到的往往是若干训练图片的平均值，所以生成的图片比较模糊。

2.GAN生成的图片细节较多，但是只保留了训练数据的语义信息，而忽略了具体内容。这里的语义信息，指的是高层级的抽象，比如都是狗图片（语义相同），内容却可以千差万别。

3.CGAN较好的解决了监督学习和GAN的结合问题，即保留了语义，也保留了内容。

# EBGAN & BEGAN

论文：

《Energy-based Generative Adversarial Network》

《BEGAN: Boundary Equilibrium GenerativeAdversarial Networks》

这两篇论文的思路（包括名字）都差不多，都是从AutoEncoder中获取的灵感，这里就放在一起讲了。

AutoEncoder包含两个部分：

**1.Encoder。**将图片编码为tensor。

**2.Decoder。**将tensor解码为图片。

对于原始的GAN来说，G网络和Decoder的结构类似，而D网络和Encoder的结构类似。所以整个网络D(G(z))就像是一个反着接的AutoEncoder。

AutoEncoder：Encoder->Decoder

GAN：Decoder->Encoder

对于生成模型来说，细节是很重要的，然而原始的GAN，通常只能生成模糊的图片。如何提高图片的分辨率呢？

EBGAN将D网络的结构由Encoder改为了Encoder->Decoder（也就是一个AE），即输入和输出都是图片。将Loss由KL散度改为了MSE。

这样的做法可以类比CV中的分类网络和语义分割网络。由于Loss和图片的每个像素都直接相关，因此更利于生成细节信息。

类似的，这里的AE，也可以换成其他Encoder->Decoder网络，例如U-NET。

EBGAN的最大特点就是判别器一开始就非常强(因为有pretrain)，因此生成器在一开始就能获得比较大的“能量驱动”(energy based)，使得在一开始生成器就进步非常快。所以如果我们比较看中训练效率，希望在短期内获得一个比较不错的生成器的话，就可以考虑EBGAN。

BEGAN则借鉴了EBGAN和WGAN各自的一些优点。

# Stack GAN

论文：

《StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks》

早期以DCGAN为代表的网络生成的图片分辨率太低，质量不够好，都不超过100×100，在32×32或者64×64左右。这是因为难以一次性学习到生成高分辨率的样本，收敛过程容易不稳定。因此采用级联结构，逐次提升分辨率的Stack GAN应运而生。

![](/images/img2/Stack_GAN.png)

上图是Stack GAN的网络结构图。其中的Conditioning Augmentation，显然使用了和VAE类似的添加随机噪声的技术。

类似想法的还有LAPGAN。

论文：

《Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks》

![](/images/img3/LAPGAN.png)

上图是LAPGAN的网络结构图。顾名思义，这是使用了Laplacian Pyramid来不断提升生成图片的分辨率。

参考：

https://www.cnblogs.com/wangxiaocvpr/p/5966776.html

LAPGAN论文笔记

https://zhuanlan.zhihu.com/p/30532830

眼见已不为实，迄今最真实的GAN：Progressive Growing of GANs

# GAN Ensemble

除了级联结构之外，以下结构用的也比较多。

## 多判别器单生成器

论文：

《Generative Multi-Adversarial Networks》

![](/images/img3/boosting_GAN.png)

采用多个判别器的好处带来了类似于boosting的优势，训练一个过于好的判别器，会损坏生成器的性能，这是GAN面临的一个大难题。如果能够训练多个没有那么强的判别器，然后进行boosting，可以取得不错的效果，甚至连dropout技术都可以应用进来。

多个判别器还可以相互进行分工，比如在图像分类中，一个进行粗粒度的分类，一个进行细粒度的分类。在语音任务中，各自用于不同声道的处理。

## 单判别器多生成器

论文：

《Multi-Agent Diverse Generative Adversarial Networks》

![](/images/img3/MA_GAN.png)

一般来说，生成器相比判别器要完成的任务更难，因为它要完成数据概率密度的拟合，而判别器只需要进行判别，导致影响GAN性能的一个问题就是模式坍塌，即生成高度相似的样本。采用多个生成器单个判别器的方法，可以有效地缓解这个问题。

从上图结构可以看出，多个生成器采用同样的结构，在网络的浅层还共享权重。

**mode collapse(模式坍塌)**，即生成高度相似的样本。

![](/images/img2/mode_collapse.jpg)

假设某个data distribution有两个模式（如上图所示），generator可能就学到一个mode，另一个mode则完全没学到。

## 增加分类器

论文：

《Triple Generative Adversarial Nets》

![](/images/img3/Triple_GAN.png)

在利用GAN进行半监督的图像分类任务时，判别器需要同时担任两个角色，即判别生成的假样本，以及预测类别，这对判别器提出了较高的要求。通过增加一个分类器可以分担判别器的工作量，即将捕捉样本和标签的条件分布这一任务交给生成器和分类器，而判别器只专注于区分真实样本和生成的样本。

# Pix2Pix

论文：

《Image-to-image translation with conditional adversarial networks》

代码：

https://github.com/phillipi/pix2pix

torch版本

https://github.com/affinelayer/pix2pix-tensorflow

tensorflow版本

https://github.com/williamFalcon/pix2pix-keras

keras版本

![](/images/img2/pix2pix.png)

Pix2Pix对于user control的要求比一般的CGAN更高，这里的监督信息不再是一个类别，而是一张图片。上图就是一个使用Pix2Pix对素描图上色的示例。其中的素描图就相当于CGAN中的类别信息。

Pix2Pix相对于传统GAN的改进在于：

**1.D网络的输入同时包括生成的图片X和它的素描图Y，X和Y使用Concat操作进行融合。**例如，假设两者都是3通道的RGB颜色图，则D网络的Input就是一个6通道的tensor，即所谓的Depth-wise concatenation。

**2.G网络使用dropout来提供随机性。**作者在实践中发现，传统的噪声向量在这个模型的学习过程中，被模型忽略掉了，起不到相应的作用。

**3.G网络使用U-NET。**实践表明，U-NET比AE的效果要好。

**4.L1损失函数的加入来保证输入和输出之间的一致性。**

**5.使用PatchGAN来保证局部精准。**

一般的GAN的D网络，只需要输出一个true or fasle的矢量，这代表对整张图像的评价。而PatchGAN输出的是一个NxN的矩阵，这个NxN的矩阵的每一个元素，对应着原图中的一个Patch。

![](/images/img3/Patch_GAN.png)

判别器对每一个patch做真假判别，将一张图片所有patch的结果取平均作为最终的判别器输出。这一步在具体实现上，就是使用Conv来代替D网络最后一层的FC。由于Conv操作不在乎输入尺寸，因此可以用不同尺寸的图片训练网络。

![](/images/img3/Patch_GAN_2.png)

这是使用不同大小的patch的效果。可以看出patch size越大，则效果越好，但是计算量也越大。

参考：

https://blog.csdn.net/stdcoutzyx/article/details/78820728

Pix2Pix-基于GAN的图像翻译

https://zhuanlan.zhihu.com/p/38411618

pix2pix

https://www.jianshu.com/p/8c7a7cb7198c

pix2pix

https://blog.csdn.net/gdymind/article/details/82696481

一文读懂GAN, pix2pix, CycleGAN和pix2pixHD

https://mp.weixin.qq.com/s/_PlISSOaowgvVW5msa7GlQ

条件GAN高分辨率图像合成与语义编辑pix2pixHD

https://mp.weixin.qq.com/s/PoSA6JXYE_OexEoJYzaX4A

利用条件GANs的pix2pix进化版：高分辨率图像合成和语义操作

https://blog.csdn.net/xiaoxifei/article/details/86506955

关于PatchGAN的理解
