---
layout: post
title:  深度强化学习（七）——MARL, AlphaStar, Dota 2, DRL参考资源（1）
category: DRL 
---

# MARL

Multi-agent Reinforcement Learning(MARL)，一般被称为多智能体强化学习。与之相对的则是Single-agent Reinforcement Learning(SARL)。

书籍：

http://www.masfoundations.org/download.html

《MULTIAGENT SYSTEMS Algorithmic, Game-Theoretic, and Logical Foundations》

以下主要参考了如下文章：

https://zhuanlan.zhihu.com/p/39037444

多智能体强化学习笔记 01

https://zhuanlan.zhihu.com/p/43579796

多智能体强化学习笔记 02

论文：

《Multi-agent reinforcement learning: An overview》

《Game Theory and Multi-agent Reinforcement Learning》

MARL应用了大量博弈论的知识，可参见《强化学习（十）》。

----

我们使用如下示例，探讨一下**什么是MARL？**

![](/images/img3/MARL.png)

上图是两个智能体1、2将金条（object）搬运回家的例子。这根金条需要两个人一人抬着一边才能扛回家。

要想把金条扛回家，小红和小蓝必须先绕过障碍物，然后每个人到达金条的一边，扛起金条后，两人还得绕开家门口的障碍物，这样才能将金条扛回家。

从上面的问题可以看出，MARL至少应该包括如下几个要素：

- 在多智能体系统中至少有两个智能体。

- 智能体之间存在着一定的关系，如合作关系（如本例），竞争关系（如多人游戏），或者同时存在竞争与合作的关系。

- 每个智能体最终所获得的回报不仅仅与自身的动作有关系，还跟对方的动作有关系。如本例中每个智能体要想获得回报，必须是金条被两个智能体一起搬回家。

SARL用MDP来描述，而MARL则需要用Markov game（马尔科夫博弈，又称随机博弈（stochastic game））来描述。

类似于MDP，Markov game可形式化表示为：

$$(n,S,A_1,\dots,A_n,T,\gamma,R_1,\dots,R_n)$$

n为玩家的个数，在本例中n为2。

S为系统状态，一般指多智能体的联合状态，即每个智能体的联合状态。

T为状态转移函数，指给定玩家当前状态和联合行为时，下一状态的概率分布。

R为回报函数。它描述了多智能体之间的关系。当每个智能体的回报函数一致时，则表示智能体之间是合作关系；当回报函数相反时，则表示智能体之间是竞争关系，当回报函数介于两者之间，则是混合关系。

MARL的结构一般如下图所示。

![](/images/img3/MARL_2.png)

**静态博弈**：static/stateless game是指没有状态s，不存在动力学使状态能够转移的博弈。例如一个矩阵博弈。

**阶段博弈**：stage game，是随机博弈的组成成分，状态s是固定的，相当于一个状态固定的静态博弈，随机博弈中的Q值函数就是该阶段博弈的奖励函数。若干状态的阶段博弈组成一个随机博弈。

**重复博弈**：智能体重复访问同一个状态的阶段博弈，并且在访问同一个状态的阶段博弈的过程中收集其他智能体的信息与奖励值，并学习更好的Q值函数与策略。

----

MARL的目标是找到每一个状态的纳什均衡策略，然后将这些策略联合起来。

<div style="text-align: center;">
<table>
  <tr>
    <td colspan="2" rowspan="2"></td>
    <td colspan="3">Game setting</td>
  </tr>
  <tr>
    <td>Stateless Games</td>
    <td>Team Markov Games</td>
    <td>General Markov Games</td>
  </tr>
  <tr>
    <td rowspan="2">Information<br/>Requirement</td>
    <td>Independent Learners</td>
    <td>Stateless Q-learning<br/>Learning Automata<br/>IGA<br/>FMQ<br/>Commitment Sequences<br/>Lenient Q-learners</td>
    <td>Policy Search<br/>Policy Gradient</td>
    <td>MG-ILA<br/>(WoLF-)PG<br/>Learning of Coordination<br/>Independent RL<br/>CQ-learning</td>
  </tr>
  <tr>
    <td>Joint Action Learners</td>
    <td></td>
    <td>Distributed- Q<br/>Sparse Tabular Q<br/>Utile Coordination</td>
    <td>Nash-Q<br/>Friend-or-Foe Q<br/>Asymmetric Q<br/>Joint Action Learning<br/>Correlated-Q</td>
  </tr>
</table>
</div>

参考：

https://zhuanlan.zhihu.com/c_1061939147282915328

一个MARL方面的专栏

https://www.zhihu.com/people/yao-xing-hu/posts

一个MARL方面的专栏

https://github.com/LantaoYu/MARL-Papers

Paper list of multi-agent reinforcement learning (MARL)

https://blog.csdn.net/qq_38163755/article/details/98057751

Game Theory and Multi-agent Reinforcement Learning笔记1

https://blog.csdn.net/qq_38163755/article/details/98223987

Game Theory and Multi-agent Reinforcement Learning笔记2

https://mp.weixin.qq.com/s/bQx-ydEAfijgK1Ii5ilPFQ

UCL汪军团队新方法提高群体智能，解决大规模AI合作竞争

https://mp.weixin.qq.com/s/4lj_G2z0Lbfk5mOgvxLC6w

对抗式协作：一个框架解决多个无监督学习视觉问题

https://mp.weixin.qq.com/s/0v57oHMEDcJuUivs8D5pnQ

善于单挑却难以协作，构建多智能体AI系统为何如此之难？

https://mp.weixin.qq.com/s/tME5GsKEXveVcgWb-Zbx_A

乔明达ICML 2018论文提出协作学习的鲁棒性方法

https://mp.weixin.qq.com/s/0Xuxr7CXqsxrssW2I6NCBQ

通用智能化：BAIR简述人类-机器人协作新技术

https://mp.weixin.qq.com/s/GNbCu1lbOmwJDCU3vgMbtQ

OpenAI发布多智能体深度强化学习新算法LOLA

https://mp.weixin.qq.com/s/5Go20MyBxdVI1r5SkwA6lw

面向星际争霸：DeepMind提出多智能体强化学习新方法

https://mp.weixin.qq.com/s/TlcVxjhHXXGSpIvptC4H8g

使用Gym和CNN构建多智能体自动驾驶马里奥赛车

https://mp.weixin.qq.com/s/zRcq1LMtK72Cl4T3877tgQ

牛津教授吐槽DeepMind心智神经网络，还推荐了这些多智能体学习论文

https://mp.weixin.qq.com/s/qU6XD45RGeMI-T7GI0dg2Q

OpenAI竞争性自我对抗训练：简单环境下获得复杂的智能体

https://mp.weixin.qq.com/s/cUFSaHWGKhyZhEvRdX09hw

谷歌大脑QT-Opt算法，机器人探囊取物成功率96%

https://mp.weixin.qq.com/s/xxB1lsiQxSfaHPKwZUI9bw

拔掉机器人的一条腿，它还能学走路？——三次元里优化的DRL策略

https://mp.weixin.qq.com/s/e4hriDvTRLkg-mIifVWayw

如何解决深度学习中的多体问题

https://zhuanlan.zhihu.com/p/61060358

Paper survey: Multi-Agent Reinforcement Learning

https://zhang-yi-chi.github.io/2019/08/15/cooperative-multi-agent-rl/

Cooperative Multi-Agent Reinforcement Learning

# AlphaStar

AlphaStar是DeepMind在解决了围棋问题之后，在RTS游戏领域的尝试。

里程碑事件：

2018.12 5:0击败TLO，5:0击败MaNa。

2019.1 表演赛不敌MaNa。

论文：

《Grandmaster level in StarCraft II using multi-agent reinforcement learning》

参考：

https://mp.weixin.qq.com/s/_Y0bCjTu9UrHfnen15htqQ

AlphaStar称霸星际争霸2！AI史诗级胜利，DeepMind再度碾压人类

https://mp.weixin.qq.com/s/axr5VFbHQmYo0shW9ilBaQ

DeepMind回应一切：AlphaStar两百年相当于人类多长时间？

https://www.zhihu.com/question/310011363

如何评价DeepMind在北京时间19年1月25日2点的《星际争霸 2》项目演示？

https://mp.weixin.qq.com/s/k0l2uoik-Z9aA9zax7AoZg

中科院自动化所深度解析：Deepmind AlphaStar如何战胜人类职业玩家

https://zhuanlan.zhihu.com/p/55781614

AlphaStar背后的机器学习原理

https://mp.weixin.qq.com/s/XljE82cJZfFOgf2KrXWSKA

DeepMind首个战胜星际2职业玩家的AI为何无敌？新视角揭秘AI里程碑

https://mp.weixin.qq.com/s/_t2vLFfIG6VYAdWDalpIXQ

说人话教AI打游戏，Facebook开源迷你版星际争霸，成果登上NeurIPS 2019

https://mp.weixin.qq.com/s/X7QQn6W2dB3y_Ljii-sJ3w

Alphastar再登Nature！星际争霸任一种族，战网狂虐99.8%人类玩家

https://zhuanlan.zhihu.com/p/102749648

基于多智能体强化学习主宰星际争霸游戏

https://zhuanlan.zhihu.com/p/92543229

AlphaStar

# Dota 2

Dota 2的AI项目主要是OpenAI来搞，算是DRL在MOBA游戏上的应用。

里程碑事件：

2017.8 1vs1击败Dendi。2：0。稍后，OpenAI将该版本放在线上，被人找出破绽，惨遭调戏。

2018.6 OpenAI Five击败半职业玩家。

2018.7 OpenAI Five在Ti8表演赛被paiN Gaming和中国退役大神队击败。

2019.4 OpenAI Five击败Ti8冠军OG。

2019.12 OpenAI开发了更强的Rerun。Rerun对OpenAI Five的胜率为98%。

论文:

《Dota 2 with Large Scale Deep Reinforcement Learning》

参考：

https://mp.weixin.qq.com/s/rPsxvzOLuoo_IRtgH94Ecw

OpenAI自学习多智能体5v5团队战击败人类玩家

https://mp.weixin.qq.com/s/VPVzXzBYD_nYO1SjTMNGVA

Dota2团战AI击败人类最全解析：能团又能gank，AI一日人间180年

https://mp.weixin.qq.com/s/5Vg9RFvyNv6T7QkIfPm1aQ

DOTA2中打败Dendi的AI如何炼出？

https://mp.weixin.qq.com/s/e4lK3UoQq-8j1YIWKALBLg

嵌入技术在Dota2人工智能战队OpenAI Five中的应用

https://mp.weixin.qq.com/s/xsoS3tvOh78rkbI0UUIkPQ

2:0！Dota2世界冠军OG被OpenAI碾压，全程人类只推掉两座外塔

https://mp.weixin.qq.com/s/X73jL5T_uZ87F3iE-uTnpw

Dota2冠军OG如何被AI碾压？OpenAI累积三年的完整论文终于放出

# DRL参考资源

https://mp.weixin.qq.com/s/ruAAQEjPIDPWPiuPepKU6Q

Deep Reinforcement Learning: An Overview

https://mp.weixin.qq.com/s/FjmlXqlpaRVLdAbjqspOJA

这是一份你必须学习的强化学习算法清单

https://mp.weixin.qq.com/s/lfXQqllfFPtuNIrQZiD-NQ

深度强化学习十大原则

https://mp.weixin.qq.com/s/I8IwPCY6-zocJKFXMr6rUg

深度强化学习的18个关键问题

https://mp.weixin.qq.com/s/_lmz0l1vP_CQ6p6DdFnHWA

谷歌大脑工程师的深度强化学习劝退文

https://zhuanlan.zhihu.com/p/39999667

强化学习路在何方？

https://mp.weixin.qq.com/s/Ns5AUGfDoTeTwnBD-pYdLA

详解强化学习当前进展及未来方向

https://mp.weixin.qq.com/s/oA98YyLqn1B22QZ5b_iDVA

IJCAI 2018：聚焦强化学习的学习效率

https://mp.weixin.qq.com/s/FtHJCXniVne2TGKfgCeS9w

Pieter Abbeel：深度强化学习加速方法

https://mp.weixin.qq.com/s/xlnwB9e1ks-4M4djnyIAyQ

解读72篇DeepMind深度强化学习论文

https://mp.weixin.qq.com/s/VYIyWRykREjOyLu4YDhLeA

61篇NIPS2019深度强化学习论文及部分解读

https://mp.weixin.qq.com/s/e6QOz-MQSn7n53EPtpw64w

DeepMind强化学习综述：她可以很快，但快从慢中来

https://zhuanlan.zhihu.com/p/72642285

基于模型的强化学习论文合集

https://zhuanlan.zhihu.com/p/77976582

强化学习并行训练论文合集

https://mp.weixin.qq.com/s/sHP03DAeZvdBxwVPuSNRPg

如何丝滑地入门神经网络？写个AI赛车游戏，只训练4代就能安全驾驶
