---
layout: post
title:  GAN（四）——CycleGAN, StarGAN, InfoGAN
category: GAN & VAE 
---

# CycleGAN

Style Transfer的基本内容可参见《深度学习（十五）》。

除了基于纹理的Style Transfer之外，基于GAN的Style Transfer是另外一个流派。

论文：

《Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks》

![](/images/img3/CycleGAN.png)

Pix2Pix要求训练数据的X和Y是成对的，因此它只适合于图片翻译任务。对于一般的Style Transfer来说，训练数据显然是不配对的，那么又该怎么办呢？

![](/images/img3/CycleGAN_2.png)

首先，简单的按照CGAN的做法，肯定是行不通的。最终生成的图片，实际上只是Y的复刻，一点X的影子都没有。

![](/images/img3/CycleGAN_3.png)

上图是CycleGAN的网络结构图。它的要点如下：

**1**.既然G网络缺乏配对的生成数据，那么不妨采用类似AE的手法来处理。

1）我们首先使用$$G_{X\to Y}$$将X变成Domain Y的数据$$Y'$$。

2）再使用$$G_{Y\to X}$$将$$Y'$$变成Domain X的数据$$X'$$。

3）X和$$X'$$显然是配对的，因此也就是可以比较的。

**2**.D网络不需要配对数据即可训练。

**3**.为了增加训练稳定性，我们可以交替训练两个网络：$$X\to Y'\to X'$$和$$Y\to X'\to Y'$$。其中的$$G_{X\to Y}$$和$$G_{Y\to X}$$在两个网络中的参数是相同的。

CycleGAN不仅可用于Style Transfer，还可用于其他用途。

![](/images/img3/CycleGAN_3.png)

上图是CycleGAN用于Steganography（隐写术）的示例。

值得注意的是，CycleGAN的idea并非该文作者独有，**同期（2017.3）的DualGAN和DiscoGAN采用了完全相同做法。**

DualGAN论文：

《DualGAN: Unsupervised Dual Learning for Image-to-Image Translation》

DiscoGAN论文：

《Learning to Discover Cross-Domain Relationswith Generative Adversarial Networks》

参考：

https://zhuanlan.zhihu.com/p/28342644

CycleGAN的原理与实验详解

https://mp.weixin.qq.com/s/7GHBH79kWIpEBLYX-VEd7A

CycleGAN：图片风格，想换就换

https://mp.weixin.qq.com/s/sxa0BfXtylHXzjq0YBn-Kg

伯克利图像迁移cycleGAN，猫狗互换效果感人

https://mp.weixin.qq.com/s/tzPCU1bxQ7NWtQ7o2PjF0g

BAIR提出MC-GAN，使用GAN实现字体风格迁移

https://mp.weixin.qq.com/s/A2VhfO3CkyQGCs5GqBWzOg

实景照片秒变新海诚风格漫画：清华大学提出CartoonGAN

https://mp.weixin.qq.com/s/rt4uuqh8IrZTjsYXEZvxKQ

阿里提出全新风格化自编码GAN，图像生成更逼真！

https://mp.weixin.qq.com/s/3lrCRiq5zuX9yWxiQRQE9A

CartoonGAN不靠手绘，也能成为漫画达人

https://mp.weixin.qq.com/s/i3lNcbOblgVKUTBtwKIFwg

AI秒造全球房源：StyleGAN快速生成假房子，连图说都配好了！

https://mp.weixin.qq.com/s/PhaDqX3OT-mqyFHohDMfdA

从Pix2Code到CycleGAN：2017年深度学习重大研究进展全解读

https://mp.weixin.qq.com/s/dwEHorYSJuX9JapIYLHiXg

BicycleGAN：图像转换多样化，大幅提升pix2pix生成图像效果

# StarGAN

论文：

《StarGAN: Unified Generative Adversarial Networksfor Multi-Domain Image-to-Image Translation》

![](/images/img3/StarGAN.png)

CycleGAN的局限在于：对于两个Domain之间的变换，需要两个G网络。可以想象，当Domain的数量上升时，所需G网络的个数将呈指数级增长。如上图左半部分所示。

StarGAN给出的办法是：所有的Domain共享一个G网络。如上图右半部分所示。

具体的操作如下图所示：

![](/images/img3/StarGAN_2.png)

1.D网络除了常规的Real/Fake判别器之外，还有一个Domain分类器。

2.G网络的结构和CycleGAN类似，也包括了两部分：$$G_{X\to Y}$$和$$G_{Y\to X}。区别在于：后者的两个G网络是不同的网络，而前者是同一个网络。

3.既然G网络只有一个，那么如何完成Domain变换呢？答案就是：把目标Domain的信息也输进G网络中。

![](/images/img3/StarGAN_3.png)

StarGAN不仅可在同一数据集中进行Domain变换，还可在不同数据集之间进行Domain变换。上图展示的是StarGAN在CelebA和RaFD数据集上的训练过程：

1.两个数据集的标签不是完全相同的。（实际上是完全不同，囧）

2.对标签进行编码。例如图中使用的Onehot编码。

3.**利用Mask区分是哪个数据集。**这一步是关键。

![](/images/img3/GAN_5.png)

除此之外，同期的Couple GAN也采用了类似的设计。上图是Couple GAN的网络结构图，其中的虚线表示网络的参数是共享的。

参考：

https://mp.weixin.qq.com/s/rPDvLnG4MBDRUMCWs2fjcQ

最新StarGAN对抗生成网络实现多领域图像变换

# InfoGAN

论文：

《InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets》

CGAN虽然已经有了些user control的能力，然而还不够强。比如人脸图片，user control的点（例如肤色、发型、表情等）就非常多，简单的标签不足以表达这么多含义。InfoGAN主要就是用来提升user control能力的。

![](/images/img3/InfoGAN.png)

上图是InfoGAN的网络结构图。相比CGAN，它的改进在于：

1.将控制变量嵌入网络中。

2.Discriminator和Classifier共享大多数参数，仅最后一层不同。

参考：

https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492415&idx=1&sn=a359e72ee99555f7a2fb4e21b2ad51db

InfoGAN：一种无监督生成方法

# ProGAN

论文：

《》

参考：

https://mp.weixin.qq.com/s/X8osUSPROJqGVTvw0gieDQ

T2T：利用StackGAN和ProGAN从文本生成人脸

# BigGAN

论文：

《Large Scale GAN Training for High Fidelity Natural Image Synthesis》

参考：

https://mp.weixin.qq.com/s/b3EVdPGY2jxliwdbbB1kcQ

“史上最强GAN图像生成器”BigGAN的demo出了！

https://mp.weixin.qq.com/s/kSyXd5dgdEcqupDeouhObQ

BigGAN论文解读

https://mp.weixin.qq.com/s/akLvNQZMNTaVbkbUrZY4tw

史上最强图像生成器BigGAN变身DeepGAN？四倍深度实现更强效果

# S2GAN

论文：

《》

# SAGAN

论文：

《》
