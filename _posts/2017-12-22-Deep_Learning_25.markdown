---
layout: post
title:  深度学习（二十五）——多任务学习
category: DL 
---

* toc
{:toc}

# 多任务学习

## 教程

http://cs330.stanford.edu/

CS 330: Deep Multi-Task and Meta Learning

## 论文

《Multi-Task Learning for Dense Prediction Tasks: A Survey》

《Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations》

## 概述

![](/images/img4/MTL.png)

![](/images/img4/MTL_2.png)

![](/images/img4/Single_MTL.png)

![](/images/img4/Multi_MTL.png)

## 参考

https://mp.weixin.qq.com/s/guAgXdhZSbEAkERSB1sLRA

多任务学习-Multitask Learning概述

https://zhuanlan.zhihu.com/p/355147999

《Multitask Learning》-多任务学习发展的关键节点

https://mp.weixin.qq.com/s/A-CVKTz_moaFzTYywSt2gg

张宇 杨强：多任务学习概述

https://zhuanlan.zhihu.com/p/148132708

最新《深度多任务学习》综述论文

https://zhuanlan.zhihu.com/p/46044947

深度神经网络中多任务学习简介

https://zhuanlan.zhihu.com/p/67524006

多任务学习综述-A Survey on Multi-Task Learning

https://mp.weixin.qq.com/s/QXOy2jo4RhCZrD5bSVzBOQ

共享相关任务表征，一文读懂深度神经网络多任务学习

https://zhuanlan.zhihu.com/p/59413549

Multi-task Learning(Review)多任务学习概述

https://mp.weixin.qq.com/s/X6FwTgr282hbqgOz3oBX-w

多任务学习概述论文：从定义和方法到应用和原理分析

https://blog.csdn.net/CoderPai/article/details/80080455

多任务学习与深度学习

https://mp.weixin.qq.com/s/NpO1UP_mzyaeqW26xLY1Xg

CVPR 2018最佳论文作者亲笔解读：研究视觉任务关联性的Taskonomy

https://mp.weixin.qq.com/s/DUSa3SW1AgvJ0szL030NHQ

一个AI玩57个游戏，DeepMind离真正“万能”的AGI不远了！

https://mp.weixin.qq.com/s/P81I5vl99mV-4StNHmd_6A

作为多目标优化的多任务学习：寻找帕累托最优解

https://mp.weixin.qq.com/s/cyX_FzHF-6df9_AMUDu1aQ

One Model To Learn Them All

https://mp.weixin.qq.com/s/oY2yGrTmIpr3H2qvzYKInA

谷歌一个模型解决所有问题《One Model to Learn Them All》论文深度解读

https://mp.weixin.qq.com/s/VMHyEOn8uyRYt39QXVWUww

西湖大学张岳：自然语言处理中的多任务联合学习（384页PPT）

https://mp.weixin.qq.com/s/5zo-2WB9v2hOvKAC7ZhKuQ

基于学习的多任务框架L2MT，为多任务问题选择最优模型

https://mp.weixin.qq.com/s/MPhKUosKZbLtVjJ1XYGXYA

如何利用深度学习模型实现多任务学习？这里有三点经验

https://mp.weixin.qq.com/s/Oopgglg2G7TwnXeN2DtZhA

多任务+注意力机制的学习

https://mp.weixin.qq.com/s/vMgHCQ03Gt5v6GdgW-pY9A

一个神经网络实现4大图像任务，GitHub已开源

https://mp.weixin.qq.com/s/Zui8FFn1FDP_UoAGMH0L7Q

知其然，知其所以然：基于多任务学习的可解释推荐系统

https://mp.weixin.qq.com/s/8Ablt7Sa86DXTB1dE_RqaA

港中文开源基于PyTorch的多任务人脸识别框架

https://www.zhihu.com/question/345173757

多任务学习成功的原因是引入了别的数据库还是多任务框架本身呢？

https://zhuanlan.zhihu.com/p/52566508

深度神经网络中的多任务学习汇总

https://mp.weixin.qq.com/s/DUsXj46TCZ9w_Dxgw4ZH8g

用于多任务CNN的随机滤波分组，性能超现有基准方法

https://mp.weixin.qq.com/s/-SHLp26oGDDp9HG-23cetg

多任务学习在推荐算法中的应用

https://mp.weixin.qq.com/s/avWy3-mju0d0QjufX_bi4Q

Multi-task Learning的三个实用小知识

https://mp.weixin.qq.com/s/hLJB8yH8V0Ncug77jHU1Bw

模型独立学习：多任务学习与迁移学习

https://zhuanlan.zhihu.com/p/138597214

Multi-task Learning and Beyond: 过去，现在与未来

https://mp.weixin.qq.com/s/gIEJoE9B8mmNx6u6bqLspQ

最新《深度多任务学习》综述论文，22页pdf

https://mp.weixin.qq.com/s/ifTNRW0W7-P_LyfNldtavQ

多任务学习方法在推荐中的演变

https://zhuanlan.zhihu.com/p/269492239

多任务学习优化（Optimization in Multi-task learning）

https://www.cnblogs.com/RyanXing/p/10730829.html

Cross-stitch Networks for Multi-task Learning

https://mp.weixin.qq.com/s/T_UsCkj9hnV7tgOTcaf6Wg

Multi-Task多任务学习， 那些你不知道的事

https://mp.weixin.qq.com/s/vFHYk7202bSZ214Za_SOtQ

2021年浅谈多任务学习

https://mp.weixin.qq.com/s/pvQHUXo83hMfH8kCumXSQA

如何利用多任务学习提升模型性能？

https://mp.weixin.qq.com/s/dcv2pqccrAtg2nNaHTTU2Q

一文"看透"多任务学习

# Attention进阶+

https://mp.weixin.qq.com/s/wrmjMLPuvpLIcF5VQBqZxg

最新“注意力机制Attention”大综述论文，66页pdf

https://mp.weixin.qq.com/s/rrbwItXt-1EaGiqtDEGvog

为节约而生：从标准Attention到稀疏Attention

https://mp.weixin.qq.com/s/MzHmvbwxFCaFjmMkjfjeSg

遍地开花的Attention，你真的懂吗？

https://mp.weixin.qq.com/s/e_LEhLf2Rh-1zkEBmqS4nA

NLP这两年：15个预训练模型对比分析与剖析

https://mp.weixin.qq.com/s/LAInpFPa-3R1rfv6idILnw

注意力机制发展如何了，如何学习它在各类任务中的应用？

https://zhuanlan.zhihu.com/p/40920384

真正的完全图解Seq2Seq Attention模型

https://mp.weixin.qq.com/s/ZSzHOu6uowRSoWrqB7vOaQ

深度学习注意力机制-Attention in Deep learning-附101页PPT

https://mp.weixin.qq.com/s/FlA1YrR0sLQGJoJZnSXpRw

DeepMind：深度学习注意力与记忆机制，附70页ppt

https://mp.weixin.qq.com/s/_mBa-GTdILrvluJtegz8fw

南洋理工大学：注意力神经网络，Attention Neural Networks，78页ppt

https://mp.weixin.qq.com/s/pKxqPB9qIGmE_PslPa6EyA

长文详解Attention的前世今生

https://mp.weixin.qq.com/s/2kFZAmb_WnTNYUXT_jVMqg

Attention注意力机制的前世今生

https://mp.weixin.qq.com/s/eq1iTyKguQm5t6Xoc7KUgw

一文搞懂NLP中的Attention机制

https://zhuanlan.zhihu.com/p/106662375

More About Attention

https://mp.weixin.qq.com/s/MFxQSUMFNRXjdLQwzveO4w

深度学习中的注意力机制

https://mp.weixin.qq.com/s/vkyPwsaxH-SvHqQx09VhVw

深度学习中的注意力机制（二）

https://mp.weixin.qq.com/s/fbAAA7fO2voP_v-NsavQew

深度学习中的注意力机制（三）

https://mp.weixin.qq.com/s/CftkSOmAx0UTtCixdxj6_A

深度学习中的注意力机制（完结篇）

https://mp.weixin.qq.com/s/Qs6tm50YvzHaJv2rh60WMw

撩一发深度文本分类之RNN via Attention

https://mp.weixin.qq.com/s/MMIZGHTKM5FrvNE6ucQRYQ

33页最新《自然语言处理中神经注意力机制综述》论文

https://mp.weixin.qq.com/s/Q0Ft5bWTuiZUIQSTk7X6ZQ

图解神经机器翻译中的注意力机制

https://mp.weixin.qq.com/s/D7GQ8DRzss9ppP6pyAs1qA

从0到1再读注意力机制

https://mp.weixin.qq.com/s/K_VRt0B9-Xw7YJndmb4WZg

Attention！注意力机制模型最新综述

https://mp.weixin.qq.com/s/hzwp5oGspdtDyNBmq8sMsw

HAN：基于双层注意力机制的异质图深度神经网络

https://mp.weixin.qq.com/s/SBrLPZjx2RdBwZpPQQ5DXQ

HAN：异构图注意力网络

https://mp.weixin.qq.com/s/0EDN-ILeL_diZ1G11ikwjw

T5模型：NLP Text-to-Text预训练模型超大规模探索

https://zhuanlan.zhihu.com/p/89719631

T5: Text-to-Text Transfer Transformer阅读笔记

https://mp.weixin.qq.com/s/X1mLXPzJU7k_ANMzvVPxjA

BERT、RoBERTa、DistilBERT与XLNet，我们到底该如何选择？

https://mp.weixin.qq.com/s/GGRORF5EfJ5xzMLwAsJt5w

从词袋到Transfomer，NLP十年突破史

https://zhuanlan.zhihu.com/p/125145283

Rethink深度学习中的Attention机制

https://mp.weixin.qq.com/s/fxEg8UOa3MeJ6qx5SjEHog

NLP领域中各式各样Attention知识系统性的梳理和总结

https://mp.weixin.qq.com/s/_5YaZdYa8bTFiAzHyrMFBg

理解卷积神经网络中的自注意力机制

https://mp.weixin.qq.com/s/y_hIhdJ1EN7D3p2PVaoZwA

阿里北大提出新attention建模框架，一个模型预测多种行为

https://mp.weixin.qq.com/s/Yq3S4WrsQRQC06GvRgGjTQ

打入神经网络思维内部

https://mp.weixin.qq.com/s/MJ1578NdTKbjU-j3Uuo9Ww

基于文档级问答任务的新注意力模型

https://mp.weixin.qq.com/s/_3pA8FZwzegSpyz_cK63BQ

Self-Attention GAN中的self-attention机制

https://mp.weixin.qq.com/s/h7sLwVXb_UI8jvJU-oe3Cg

Google AI提出“透明注意力”机制，实现更深层NMT模型

https://mp.weixin.qq.com/s/1LYz5SH5rVnPPJ0tZvRQAA

从各种注意力机制窥探深度学习在NLP中的神威

https://zhuanlan.zhihu.com/p/33078323

数字串识别：基于位置的硬性注意力机制

https://mp.weixin.qq.com/s/-gAISWjSiG6ccPuOPAEg3A

五张动图，看清神经机器翻译里的Attention！

https://mp.weixin.qq.com/s/aixpv9t1PLPRWUP6PvZ0EQ

用自注意力增强卷积：这是新老两代神经网络的对话

https://mp.weixin.qq.com/s/i3Xd_IB7R0-QPztn-pgpng

遍地开花的Attention，你真的懂吗？

https://zhuanlan.zhihu.com/p/151640509

注意力机制在推荐系统中的应用

https://mp.weixin.qq.com/s/-SU5cNbklI31WLmTawZJIQ

自注意模型学不好？这个方法帮你解决！

https://mp.weixin.qq.com/s/K5EbO0djcXHN4K5LQiMh5g

Triplet Attention机制让Channel和Spatial交互更加丰富

https://mp.weixin.qq.com/s/C4f0N_bVWU9YPY34t-HAEA

UNC&Adobe提出模块化注意力模型MAttNet，解决指示表达的理解问题

https://mp.weixin.qq.com/s/V3brXuey7Gear0f_KAdq2A

基于注意力机制的交易上下文感知推荐，悉尼科技大学和电子科技大学最新工作

https://mp.weixin.qq.com/s/2gxp7A38epQWoy7wK8Nl6A

谷歌翻译最新突破，“关注机制”让机器读懂词与词的联系

https://zhuanlan.zhihu.com/p/25928551

用深度学习（CNN RNN Attention）解决大规模文本分类问题-综述和实践

https://mp.weixin.qq.com/s/l4HN0_VzaiO-DwtNp9cLVA

循环注意力区域实现图像多标签分类

https://mp.weixin.qq.com/s/zhZLK4pgJzQXN49YkYnSjA

自适应注意力机制在Image Caption中的应用

https://mp.weixin.qq.com/s/uvr-G5-_lKpyfyn5g7ES0w

基于注意力机制，机器之心带你理解与训练神经机器翻译系统

https://mp.weixin.qq.com/s/ANpBFnsLXTIiW6WHzGrv2g

自注意力机制学习句子embedding

https://mp.weixin.qq.com/s/49fQX8yiOIwDyof3PD01rA

CMU&谷歌大脑提出新型问答模型QANet：仅使用卷积和自注意力，性能大大优于RNN

https://mp.weixin.qq.com/s/c64XucML13OwI26_UE9xDQ

滴滴披露语音识别新进展：基于Attention显著提升中文识别率

https://mp.weixin.qq.com/s/7OYY3L7gL4wVv_EjoosOHA

如何增强Attention Model的推理能力

https://mp.weixin.qq.com/s/9Kt6_DfeYRnhsb10aCSFGw

FAGAN：完全注意力机制（Full Attention）GAN，Self-attention+GAN

https://mp.weixin.qq.com/s/lZOIK5BRXZrmL_Z9crl6sA

机器翻译新突破！“普适注意力”模型：概念简单参数少，性能大增

https://mp.weixin.qq.com/s/jRfOzKO6OlQLokIzipbqUQ

为什么使用自注意力机制？

https://zhuanlan.zhihu.com/p/339123850

关于attention机制的一些细节的思考

https://mp.weixin.qq.com/s/n4mzHSweOT-vDWBGs0XFbw

卷积神经网络中的自我注意
