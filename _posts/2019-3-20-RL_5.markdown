---
layout: post
title:  强化学习（五）——Model-Free Control
category: RL 
---

* toc
{:toc}

# Temporal-Difference Learning（续）

## bootstrapping

在前面的章节，我们一直提到bootstrapping这个名词，然而却没有解释它的含义，现在是时候了。

统计学中，bootstrapping可以指依赖于重置随机抽样的一切试验。bootstrapping可以用于计算样本估计的准确性。对于一个采样，我们只能计算出某个统计量(例如均值)的一个取值，无法知道均值统计量的分布情况。但是通过自助法(自举法)我们可以模拟出均值统计量的近似分布。有了分布很多事情就可以做了（比如说有你推出的结果来进而推测实际总体的情况）。

bootstrapping方法的实现很简单，假设抽取的样本大小为n，在原样本中有放回的抽样，抽取n次。每抽一次形成一个新的样本，重复操作，形成很多新样本，通过这些样本就可以计算出样本的一个分布。新样本的数量通常是1000-10000。如果计算成本很小，或者对精度要求比较高，就增加新样本的数量。

优点：简单易于操作。

缺点：bootstrapping的运用基于很多统计学假设，因此假设的成立与否会影响采样的准确性。

**但是，这不是bootstrapping在RL中的含义！**

Finally, we note one last special property of DP methods. All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea **bootstrapping**.

上面这段是Sutton给bootstrapping的定义，其实也不是太好懂。那么bootstrapping到底是什么意思呢？

$$V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$$

上式是TD的更新公式，从中可以看出TD target：$$R_{t+1}+\gamma V(S_{t+1})$$中已经包含了V(s)，也就是说它是用其它V(s)更新当前V(s)。这种特性就是**bootstrapping**。

$$V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))$$

而MC的target：$$G_t$$就和V(s)无关。

参考：

https://datascience.stackexchange.com/questions/26938/what-exactly-is-bootstrapping-in-reinforcement-learning

What exactly is bootstrapping in reinforcement learning?

## TD(n)

先前所介绍的TD算法实际上都是TD(0)算法，括号内的数字0表示的是在当前状态下往前多看1步，要是往前多看2步更新状态价值会怎样？这就引入了n-step的概念。

在当前状态往前行动n步，计算n步的return，同样TD target 也由2部分组成，已走的步数使用确定的即时reward，剩下的使用估计的状态价值替代。这就是TD(n)算法。

显然，MC实际上就是TD($$n=\infty$$)。

定义n-步收获：

$$G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})$$

TD(n)的更新公式：

$$V(S_t)\leftarrow V(S_t)+\alpha(G_t^{(n)}-V(S_t))$$

## Large Random Walk

既然存在n-步预测，那么n=？时效果最好呢，下面的例子试图回答这个问题：

这个示例研究了使用多个不同步数预测联合不同步长(step-size，公式里的系数$$\alpha$$）时，分别在在线（On-line）和离线（Off-line）状态时状态函数均方差的差别。所有研究使用了10个Episode。离线与在线的区别在于，离线是在经历所有10个Episode后进行状态价值更新；而在线则至多经历一个Episode就更新一次状态价值。

![](/images/img2/LRW.png)

结果如上图所示，离线和在线之间曲线的形态差别不明显；从步数上来看，步数越长，越接近MC算法，均方差越大。对于这个大规模随机行走示例，在线计算比较好的步数是3-5步，离线计算比较好的是6-8步。但是不同的问题其对应的比较高效的步数不是一成不变的。因此选择多少步数作为一个较优的计算参数也是一个问题。

## TD($$\lambda$$)

一种简单的方法是计算Averaging n-Step Returns，例如：

$$G=\frac{1}{2}G^{(2)}+\frac{1}{2}G^{(4)}$$

有没有更有效的方法呢？这里我们引入了一个新的参数：$$\lambda$$。通过引入这个新的参数，可以做到在不增加计算复杂度的情况下综合考虑所有步数的预测。

$$\lambda$$收获：

$$G_t^{\lambda}=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}$$

$$\lambda$$预测：

$$V(S_t)\leftarrow V(S_t)+\alpha(G_t^{\lambda}-V(S_t))$$

![](/images/img2/TD_2.png)

这张图还是比较好理解的，例如对于n=3的3-步收获，赋予其在收获中的权重如左侧阴影部分面积，对于终止状态的T-步收获，赋予其T以后的所有阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。

TD($$\lambda$$)的设计使得Episode中，后一个状态的状态价值与之前所有状态的状态价值有关，同时也可以说成是：一个状态价值参与决定了后续所有状态的状态价值。但是每个状态的价值对于后续状态价值的影响权重是不同的。我们可以从两个方向来理解TD($$\lambda$$)：

### 前向认识TD($$\lambda$$)

引入了$$\lambda$$之后，会发现要更新一个状态的状态价值，必须要走完整个Episode获得每一个状态的即时奖励以及最终状态获得的即时奖励。这和MC算法的要求一样，因此TD($$\lambda$$)算法有着和MC方法一样的劣势。λ取值区间为[0,1]，当$$\lambda$$=1时对应的就是MC算法。这给实际计算带来了不便。

### 反向认识TD($$\lambda$$)

TD($$\lambda$$)从另一方面提供了一个单步更新的机制，通过下面的示例来说明。

![](/images/img2/TD_3.png)

老鼠在连续接受了3次响铃和1次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？

这里有两种启发方法：

**频率启发 Frequency heuristic**：将原因归因于出现频率最高的状态。

**就近启发 Recency heuristic**：将原因归因于较近的几次状态。

给每一个状态引入一个数值：效用追踪（Eligibility Traces, ET），来结合上述两种启发：

$$
\begin{array}\\
E_0(s)=0\\
E_t(s)=\gamma\lambda E_{t-1}(s)+1(S_t=s)
\end{array}
$$

![](/images/img2/TD_4.png)

上图是$$E_t(s)$$函数的一个可能的曲线图。该图横坐标是时间，横坐标下有竖线的位置代表当前进入了状态s，纵坐标是效用追踪值E 。可以看出当某一状态连续出现，E值会在一定衰减的基础上有一个单位数值的提高，此时将增加该状态对于最终收获贡献的比重，因而在更新该状态价值的时候可以较多地考虑最终收获的影响。同时如果该状态距离最终状态较远，则其对最终收获的贡献越小。

特别的，E值并不需要等到完整的Episode结束才能计算出来，它可以每经过一个时刻就得到更新。E值存在饱和现象，有一个瞬时最高上限：

$$E_\max=1/(1-\gamma\lambda)$$

结合之前提到的TD error和ET，则更新公式可改为：

$$V(S_t)\leftarrow V(S_t)+\alpha\delta_tE_t(s)$$

如果$$\lambda=0$$，则只有当前状态得到更新，即$$E_t(s)=1(S_t=s)$$，这实际上就和之前提到TD(n=0)算法是一致的了。

>David Silver的课件在这里存在表示混乱的问题，在之前的章节中，TD(X)表示的是n=X，而下文中TD(X)有的时候指的是$$\lambda=X$$。这里借用python表示参数的语法，更准确的描述公式。

如果$$\lambda=1$$，TD($$\lambda=1$$)粗略看与每次访问的MC算法等同；在线更新时，状态价值差每一步都会有积累；离线更新时，TD($$\lambda=1$$)等同于MC算法(即遍历整个Episode)。

参考：

https://mp.weixin.qq.com/s/X6bukOqZ2Eg7jZ6MydHwSg

伯克利提出时序差分模型TDM：让深度强化学习更像人类

# Model-Free Control

## 概述

之前提到的MC & TD都是Model-free prediction，下面讲讲Model-Free Control。

现实中有很多此类的例子，比如控制一个大厦内的多个电梯使得效率最高；控制直升机的特技飞行，机器人足球世界杯上控制机器人球员，围棋游戏等等。所有的这些问题要么我们对其模型运行机制未知，但是我们可以去经历、去试；要么是虽然问题模型是已知的，但问题的规模太大以至于计算机无法高效的计算，除非使用采样的办法。Model-Free Control的内容就专注于解决这些问题。

我们之前已经指出了MDP的五要素：

$$<\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma>$$

从形式定义的角度来说，**Model-Free Control实际上就是一种$$\mathcal{P},\mathcal{R},\gamma$$均为未知的MDP**。

根据优化控制过程中是否利用已有或他人的经验策略来改进我们自身的控制策略，我们可以将这种优化控制分为两类：

一类是On-policy Learning，其基本思想是个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。

另一类是Off-policy Learning: 其基本思想是，虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类的策略等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即在自己的策略形成的价值函数的基础上观察别的策略产生的行为，以此达到学习的目的。这种学习方式类似于“站在别人的肩膀上可以看得更远”。

**简单来说，On-policy Learning训练时，使用当前策略，而Off-policy Learning使用非当前策略。**

## On-Policy Monte-Carlo Control

Model-Free Control应用MC需要解决两个问题：

1.在模型未知的条件下无法知道当前状态的所有后续状态，进而无法确定在当前状态下采取怎样的行为更合适。

解决这一问题的方法是使用action-value function：$$q_{\pi}(s; a)$$替换state-value function：$$v_{\pi}(s)$$。即下图所示：

这样做的目的是可以改善策略而不用知道整个模型，只需要知道在某个状态下采取什么样的行为价值最大即可。

2.当我们每次都使用贪婪算法来改善策略的时候，将很有可能由于没有足够的采样经验而导致产生一个并不是最优的策略，我们需要不时的尝试一些新的行为，这就是探索（Exploration）。
