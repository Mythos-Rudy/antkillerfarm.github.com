---
layout: post
title:  强化学习（八）——Actor-Critic, 博弈论（1）
category: RL 
---

# 策略梯度（续）

## Policy Gradient Theorem

One-Step MDP是一类特殊的MDP：它从任意状态开始，只执行一次动作，然后就结束了。

$$J(\theta)=E_{\pi_\theta}[r]=\sum_{s\in S} d(s)\sum_{a\in A}\pi_\theta(s,a)R_{s,a}$$

$$\nabla_\theta J(\theta)=\sum_{s\in S} d(s)\sum_{a\in A}\nabla_\theta \log \pi_\theta(s,a)R_{s,a}=E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)r]$$

One-Step MDP的相关结论可以扩展到multi-step MDP，只需要将即时奖励r，替换为长期奖励（一般使用其估计函数$$Q^{\pi}(s,a)$$）即可。

>在监督学习里，当前状态、行为的好坏由监督信息告知；而在强化学习里，则需要通过价值函数来估计当前状态行为的好坏。

我们将Policy Gradient应用到MC算法上，则得到如下流程：

>Initialise $$\theta$$ arbitrarily   
>>for each episode $$\{s_1,a_1,r_2,\dots,s_{T−1},a_{T−1},r_T\}\sim \pi_\theta$$ {   
>>>for t = 1 to $$T−1$$ {   
>>>$$\theta \leftarrow \theta + \alpha\nabla_\theta \log \pi_\theta (s_t , a_t )v_t$$   
>>>}   
>>
>>}   
>
>return $$\theta$$

在基于策略的学习算法中，算法挑选策略的时候不需使用$$\epsilon$$-贪婪搜索，策略是直接根据参数$$\theta$$得到的。同时在对策略参数更新时有一个学习率$$\alpha$$，它体现了在梯度方向上更新参数θ的步长（step size），一般的我们在更新参数时是按梯度方向只更新由$$\alpha$$确定的一定量。

打个比方，当前策略在更新时提示梯度方向倾向于选择“向左”的行为，那么在更新策略参数时，可以朝着向左的方向更新一定的值，如果这个$$\alpha$$取值增大，则导致决策朝着更容易选择“向左”的行为倾斜，这其实就相当于没有探索的贪婪决策行为。而只要学习在持续，就有可能因为梯度变化而尝试更多的行为，这一过程中参数$$\alpha$$控制了策略更新的平滑度。

如果基于价值函数制定策略，则使用查表（table look-up)的方式可以保证收敛到全局最优解。即使使用的是直接基于策略的学习方法；但是,如果使用的是通用化的近似函数表示方法，比如神经网络等，则无论是基于价值函数，还是基于策略，都可能陷入局部最优解。

## 参考

https://zhuanlan.zhihu.com/p/54825295

基于值和策略的强化学习入坑

https://blog.csdn.net/gsww404/article/details/80705950

策略梯度（Policy Gradient）

# Actor-Critic

## 概述

MC策略梯度方法使用了收获作为状态价值的估计，它虽然是无偏的，但是噪声却比较大，也就是变异性（方差）较高。如果我们能够相对准确地估计状态价值，用它来指导策略更新，那么是不是会有更好的学习效果呢？这就是Actor-Critic策略梯度的主要思想。

![](/images/img3/Actor-critic.png)

Actor-Critic的字面意思是“演员-评论”，相当于演员在演戏的同时，有评论家指点，继而演员演得越来越好。即使用Critic来估计行为价值：

$$Q_w(s,a)\approx Q^{\pi_\theta}(s,a)$$

基于Actor-Critic策略梯度学习分为两部分内容：

1.Critic：更新action-value函数的参数w。

2.Actor：按照Critic得到的价值，引导策略函数参数$$\theta$$的更新。

$$\nabla_\theta J(\theta)\approx E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

$$\Delta \theta = \alpha \nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)$$

可以看出，Critic做的事情其实是我们已经见过的：策略评估，他要告诉个体，在由参数$$\theta$$确定的策略$$\pi_\theta$$到底表现得怎么样。

## Compatible Function Approximation

近似策略梯度的方法引入了Bias，从而得到的策略梯度不一定能最后找到较好的解决方案，例如当近似价值函数引起状态重名的特征时。

幸运的是，如果我们小心设计近似函数，是可以避免引入bias的。该近似函数需要满足下面两个条件:

- 近似价值函数的梯度完全等同于策略函数对数的梯度，即不存在重名情况：

$$\nabla_w Q_w(s,a)=\nabla_\theta \log \pi_\theta(s,a)$$

- 价值函数参数w使得均方差最小：

$$\epsilon = E_{\pi_\theta}[(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2]$$

符合这两个条件，则认为策略梯度是准确的，即：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$$

<a name="advantage_function"/>

## Reducing Variance Using Baseline

除了bias之外，variance也是需要考虑的方面。

我们从策略梯度里抽出一个基准函数B(s)，要求这一函数仅与状态有关，而与行为无关，因而不改变梯度本身。

$$E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)B(s)] = \sum_{s\in S} d^{\pi_\theta}(s)\sum_{a\in A}\nabla_\theta \pi_\theta(s,a)B(s)\\= \sum_{s\in S} d^{\pi_\theta}(s)B(s) \nabla_\theta \sum_{a\in A} \pi_\theta(s,a) = 0$$

由于B(s)与行为无关，可以将其从针对行为a的求和中提出来，同时我们也可以把梯度从求和符号中提出来，而最后一个求和项：策略函数针对所有行为的求和，根据策略函数的定义，这里的结果肯定是1。而常数的梯度是0，因此总的结果等于0。

原则上，和行为无关的函数都可以作为B(s)。一个很好的B(s)就是基于当前状态的状态价值函数：$$V^{\pi_\theta}(s)$$。

所以，我们可以用一个advantage function来改写策略梯度：

$$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$$

上式的现实意义在于评估当个体采取行为a离开s状态时，究竟比该状态s总体平均价值要好多少？

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]$$

advantage function可以明显减少状态价值的variance。（通俗的说法就是A的值有正有负，不像Q和V都是正值。）

因此，现在Critic的任务就改为估计advantage function。

在这种情况下，我们需要两个近似函数也就是两套参数，一套用来近似状态价值函数，一套用来近似行为价值函数。即：

$$V_v(s)\approx V^{\pi_\theta}(s)$$

$$Q_w(s,a)\approx Q^{\pi_\theta}(s,a)$$

$$A(s,a) = Q_w(s,a) - V_v(s)$$

不过实际操作时，我们并不需要计算两个近似函数。这里以TD学习为例说明一下。

根据定义，TD误差$$\delta^{\pi_\theta}$$可以根据真实的状态价值函数$$V^{\pi_\theta}(s)$$算出：

$$\delta^{\pi_\theta} = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)$$

因为：

$$\begin{align}
E_{\pi_\theta}[\delta^{\pi_\theta} | s,a] &= E_{\pi_\theta}[r + \gamma V^{\pi_\theta}(s') | s,a] - V^{\pi_\theta}(s)\\
&= Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s) \\
&= A^{\pi_\theta}(s,a)
\end{align}$$

可见$$\delta^{\pi_\theta}$$就是$$A^{\pi_\theta}(s,a)$$的一个无偏估计。

因此，我们就可以使用TD误差来计算策略梯度：

$$\nabla_\theta J(\theta)= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\delta^{\pi_\theta}]$$

实际运用时，我们使用一个近似的TD误差，即用状态函数的近似函数来代替实际的状态函数：

$$\delta_v = r + \gamma V_v(s') - V_v(s)$$

这也就是说，我们只需要一套参数描述状态价值函数，而不再需要行为价值函数了。

上述方法不仅可用于TD方法，还可用于MC方法等，以下不加讨论的给出如下结论：

$$\begin{align}
\nabla_\theta J(\theta) &= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{v_t}] & \text{REINFORCE}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{Q_w(s,a)}] & \text{Q Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{A_w(s,a)}] & \text{Advantage Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{\delta}] & \text{TD Actor-Critic}\\
&= E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)\color{red}{\delta e}] & \text{TD(}\lambda\text{) Actor-Critic}
\end{align}$$

## 参考

https://zhuanlan.zhihu.com/p/51652845

强化学习这都学不会的话，咳咳，你过来下！

https://mp.weixin.qq.com/s/ce9W3FbLdsqAEyvw6px_RA

Actor Critic——一个融合基于策略梯度和基于值优点的强化学习算法

# 博弈论

博弈论（game theory）是一门单独的学科，和RL并无统属关系。然而由于RL，特别是MARL大量应用到了相关的知识，所以这里也把它写在RL系列里了。

## 历史

博弈论最早可追溯到“齐威王田忌赛马”，但它真正的发展是在20世纪下半叶。

RL的历史相对比较晚，因此从渊源来看，RL=博弈论+控制论+ML。

参考：

https://blog.csdn.net/sobermineded/article/details/79601986

博弈论历史、发展与应用

## 教程

《Game Theory An Introduction》，Steven Tadelis著。

>Steven Tadelis，经济学家。Harvard博士（1997），UCB教授。

https://mp.weixin.qq.com/s/U_vDI_PJm-buubTwJgV9pw

数学博弈论与应用，431页pdf，Mathematical Game Theory and Applications

## 概述

要理解博弈论，可以通过博弈论和决策论的区别开始。

**决策论**是研究局中人在给定其他环境参数条件下的最优选择问题。

**博弈论**研究的是当局中人充分考虑到其他局中人对其战略选择的反应后（即局中人都具有同样充分的理性时）进行最优战略的选择。

博弈论的直接目标不是找到一个玩家的最佳策略，而是找到所有玩家的最理性策略组合。我们称最理性策略组合为**均衡（equilibrium）**。

从宏观上可以将博弈论研究的问题分为：**合作博弈**和**非合作博弈**。现代狭义的博弈论一般是指非合作博弈。

非合作博弈根据参与博弈的参与人做决策的先后顺序可以分为：静态博弈和动态博弈。

**静态博弈**：参与人同时做决策，常用标准型（normal form）表述其策略。如两人零和博弈等。

**动态博弈**：参与人有先后顺序做决策，且后者能观察到前者所做的决策，如围棋等。常用扩展型（extensive form）来表述其策略，常用的扩展型表述为博弈树。

非合作博弈根据参与人是否已知对方的信息，可以分为：完美信息博弈和不完美信息博弈。

**完美信息博弈**：参与人对相关信息完全已知，如棋类游戏。玩家知道对方棋子所在的位置。

**不完美信息博弈**：参与人对相关信息并不完全已知。如牌类游戏，玩家并不知道对手的牌是什么。

当局中人的个数n为有限数且每个局中人的战略空间中的元素只有限个时，称博弈为**有限博弈（finite game）**。
