---
layout: post
title:  强化学习（七）——策略梯度
category: RL 
---

# 价值函数的近似表示

## Incremental Prediction Algorithms（续）

下面给出各种算法的收敛特性。

**预测算法的收敛性**：

| On/Off-Policy | Algorithm | Table Lookup | Linear | Non-Linear |
|:--:|:--:|:--:|:--:|:--:|
| On-Policy | MC<br/>TD(0)<br/>TD($$\lambda$$)<br/>Gradient TD | Yes<br/>Yes<br/>Yes<br/>Yes | Yes<br/>Yes<br/>Yes<br/>Yes | Yes<br/>No<br/>No<br/>Yes |
| On-Policy | MC<br/>TD(0)<br/>TD($$\lambda$$)<br/>Gradient TD | Yes<br/>Yes<br/>Yes<br/>Yes | Yes<br/>No<br/>No<br/>Yes | Yes<br/>No<br/>No<br/>Yes |

Gradient TD中的Gradient指的是PBE（projected Bellman error）的梯度。

![](/images/img3/PBE.png)

**控制算法的收敛性**：

| Algorithm | Table Lookup | Linear | Non-Linear |
|:--:|:--:|:--:|:--:|
| Monte-Carlo Control<br/>Sarsa<br/>Q-learning<br/>Gradient Q-learning | Yes<br/>Yes<br/>Yes<br/>Yes | Yes'<br/>Yes'<br/>No<br/>Yes | No<br/>No<br/>No<br/>No |

Yes' = chatters around near-optimal value function

## Batch Methods

前面所说的Incremental算法都是基于数据流的，经历一步，更新算法后，我们就不再使用这步的数据了，这种算法简单，但有时候不够高效。与之相反，Batch Methods则是把一段时期内的数据集中起来，通过学习来使得参数能较好地符合这段时期内所有的数据。这里的训练数据集Batch相当于个体的一段经验。

### Least Squares Prediction

假设存在一个价值函数的近似：$$\hat v(s,w)\approx v_\pi(s)$$和一个经验集$$\mathcal{D}=\{<s_1,v_1^{\pi}>,<s_2,v_2^{\pi}>,\dots,<s_T,v_T^{\pi}>\}$$，则Least Squares Error为：

$$LS(w)=\sum_{t=1}^T(v_t^{\pi}-\hat v(s_t,w))^2$$

使用LSE的Prediction被称为Least Squares Prediction，针对不同方向衍生还出了LSMC、LSTD、LSTD($$\lambda$$)等算法。

LSE亦可推广到Q函数，即所谓的LS Q-Learning。

### Experience Replay

为了充分利用经验集，我们还可以使用Experience Replay技术，即：随机从$$\mathcal{D}中采样$$<s,v^{\pi}>$$，然后使用SGD最小化LSE。

这一点和普通的监督学习很类似，梯度下降从原理上来说，本就不可能一个epoch就达到最优，而需要多跑几个epoch，这里的Experience Replay也是类似的道理。

Experience Replay还可用来消除训练数据间的相关性。例如，1个agent在1个episode中产生的experience就是相关的，$$<s_t,a_t>$$和$$<s_{t+1},a_{t+1}>$$显然存在着一定的逻辑关系。训练数据间的相关性会影响算法收敛到最优解。

# 策略梯度

价值函数可以进行近似的参数化表达，策略本身也同样可以函数化、参数化：

$$\pi_\theta(s,a)=P[a | s, \theta]$$

所谓函数化是指，通过一个概率分布函数$$\pi_\theta(s,a)$$，来表示每一步的最优策略，在每一步根据该概率分布进行action采样，获得当前的最佳action取值。显然概率越高的a，就是该策略最倾向的a。

显然。生成action的过程，本质上是一个随机过程；最后学习到的策略，也是一个随机策略(stochastic policy).

而策略函数的参数化就和价值函数的参数化类似了，无非是用较少的参数，来表示巨大的状态空间而已。

基于策略学习的优点：

- 基于策略的学习可能会具有更好的收敛性，这是因为基于策略的学习虽然每次只改善一点点，但总是朝着好的方向在改善；但是上讲提到有些价值函数在后期会一直围绕最优价值函数持续小的震荡而不收敛。

- 在对于那些拥有高维度或连续状态空间来说，使用基于价值函数的学习在得到价值函数后，制定策略时，需要比较各种行为对应的价值大小，这样如果行为空间维度较高或者是连续的，则从中比较得出一个有最大价值函数的行为这个过程就比较难了，这时候使用基于策略的学习就高效的多。

- 能够学到一些随机策略。这一点后文会详述。

- 有时候计算价值函数非常复杂。比如当小球从从空中某个位置落下你需要左右移动接住时，计算小球在某一个位置时采取什么行为的价值是很难得；但是基于策略就简单许多，你只需要朝着小球落地的方向移动修改策略就行。

缺点：

- 容易收敛到局部最优，而非全局最优。

- 因为基于价值函数的策略决定每次都是推促个体去选择一个最大价值的行为；但是基于策略的，更多的时候策略的选择时仅会在策略某一参数梯度上移动一点点，使得整个的学习比较平滑，因此不够高效。

## 随机策略

随机策略有时是最优策略。对于石头剪刀布的游戏，只要一方有一个确定性的策略，就会被对手抓住进而整体上输掉。这个时候最好的策略就是随机选择每次出法，以得到最大可能的总体奖励。

![](/images/img3/Gridworld.png)

以上面的Gridworld图为例，如果我们用某一个格子的某个方向是否有墙挡住来描述格子状态的话，就会发生灰色格子的状态是一样的情况，这就是所谓的状态重名（Aliased）。

在这种情况下，如果采用**确定性**的策略话，在个体处于无论哪个灰色格子时，都只能选取相同的行为。假设个体现在学到了一个价值函数，在这个价值函数里状态就是基于上述特征的参数化表示，此时当个体处在灰色格子中，如果采取的是greedy执行的方式，价值函数给出的策略要么都是向东，要么都是向西。如果是向西，那么当个体处在左侧灰色格子时，它将一直（greedy）或很长时间（$$\epsilon$$-greedy）徘徊在长廊左侧两个格子之间而无法到达有钱袋子的格子，因而很长时间得不到奖励。

当发生状态重名情况时，随机策略将会优于确定性的策略。之前的理论告诉我们，对于任何MDP总有一个确定性的最优策略。不过那是针对状态可完美观测、或者使用的特征可以完美描述状态的情况下的。当发生状态重名无法区分，或者使用的近似函数无法对状态完美描述时，个体得到的状态信息等效于部分观测的环境信息，这时问题将不具备Markov性。而此时的最优策略，也不再是确定性的。

## 策略目标函数（Policy Objective Functions）

首先，我们看看如何评价一个策略的好坏。

- **Start value**：

$$J_1(\theta)=V^{\pi_\theta}(s_1)=E_{\pi_\theta}[v_1]$$

上式表示在能够产生完整Episode的环境下，从状态$$s_1$$开始，直到结束状态所能获得的累计奖励。在这种情况下，由于整个Episode的状态、路径都是确定的，因此奖励也是确定的。我们需要做的只是最大化start value，以找到最佳路径（也就是最佳策略）。

- **Average Value**：

$$J_{avV}(\theta)=\sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)$$

对于连续环境条件，不存在一个开始状态，这个时候可以使用average value。上式中的$$d^{\pi_\theta}(s)$$表示$$\pi_\theta$$对应的MDP过程的Markov chain的稳态分布。

- **Average reward per time-step**：

$$J_{avR}(\theta)=\sum_s d^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_{s,a}$$

上式表示每一个time-step在各种情况下所能得到的平均奖励。

当然了，在很多情况下，我们未必可以得到一个真实的奖励值，这时使用相关函数的估计值作为奖励值即可。

## 策略优化（Policy Optimisation）

主要分为两大类：

1.非gradient算法。

- Hill climbing
- Simplex / amoeba / Nelder Mead
- Genetic algorithms

2.gradient算法。

- Gradient descent
- Conjugate gradient
- Quasi-newton

一般来说，gradient算法的优化效率较高，因此本文只讨论gradient算法。

我们可以仿照ML中的梯度优化算法，定义策略的梯度函数：

$$\Delta\theta=\alpha\nabla_\theta J(\theta)$$

其中，

$$\nabla_\theta J(\theta)=\begin{pmatrix} \frac{\partial J(\theta)}{\partial \theta_1} \\ \vdots \\ \frac{\partial J(\theta)}{\partial \theta_n} \end{pmatrix}$$

然而，梯度函数有的时候并不简单，也许并不可微。这时可以使用有限差分法：

$$\frac{\partial J(\theta)}{\partial \theta_k}\approx \frac{J(\theta + \epsilon u_k)-J(\theta)}{\epsilon}$$

这里的$$u_k$$是第k个元素为1，其余元素为0的单位向量。

有限差分法简单，不要求策略函数可微分，适用于任意策略；但有噪声，且大多数时候不高效。

## Monte-Carlo Policy Gradient

$$\nabla_\theta \pi_\theta(s,a) = \pi_\theta(s,a)\frac{\nabla_\theta \pi_\theta(s,a)}{\pi_\theta(s,a)}=\pi_\theta(s,a) \nabla_\theta \log \pi_\theta(s,a)$$

上式一般被称作Likelihood ratios，这里使用到了一个公式：$$\mathrm{d}\log(y)=\mathrm{d}y/y$$

我们定义Score function为：$$\nabla_\theta \log \pi_\theta(s,a)$$

下面讨论几种常见的策略。

## Softmax Policy

Softmax Policy适用于离散行为空间，它把行为权重看成是多个特征在一定权重下的线性代数和：$$\phi(s,a)^T\theta$$，则采取某一行为的概率为：

$$\pi_\theta(s,a)\propto e^{\phi(s,a)^T\theta}$$

相应的Score function为:

$$\nabla_\theta \log \pi_\theta(s,a)=\phi(s,a)-E_{\pi_\theta}[\phi(s,\cdot)]$$

上式中，等号右边第一部分是采取某行为的Score，第二部分是当前状态的期望分值。

## Gaussian Policy

Gaussian Policy适用于连续行为空间，它用一个正态分布表示策略：$$a\sim N(\mu(s), \sigma^2)$$，其中$$\mu(s)=\phi(s)^T\theta$$。

相应的Score function为:

$$\nabla_\theta \log \pi_\theta(s,a)=\frac{(a-\mu(s))\phi(s)}{\sigma^2}$$

下面我们用一个图来演示一下相关步骤。

![](/images/img3/pg.png)

左图的蓝点表示采样X，箭头指示该点的梯度方向，既然是正态分布，这些箭头显然是远离中心的。中图表示，X对应的Score function。我们根据Score function更新参数（沿着绿色箭头方向，或者沿着红色箭头的反方向）。最终得到右图中的新的正态分布。

从贝叶斯学习的角度来看，左图相当于先验分布，而中图相当于后验分布。

参考：

https://karpathy.github.io/2016/05/31/rl/

Deep Reinforcement Learning: Pong from Pixels

## Policy Gradient Theorem

One-Step MDP是一类特殊的MDP：它从任意状态开始，只执行一次动作，然后就结束了。

$$J(\theta)=E_{\pi_\theta}[r]=\sum_{s\in S} d(s)\sum_{a\in A}\pi_\theta(s,a)R_{s,a}$$

$$\nabla_\theta J(\theta)=\sum_{s\in S} d(s)\sum_{a\in A}\nabla_\theta \log \pi_\theta(s,a)R_{s,a}=E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)r]$$

One-Step MDP的相关结论可以扩展到multi-step MDP，只需要将即时奖励r，替换为长期奖励（一般使用其估计函数$$Q^{\pi}(s,a)$$）即可。
