---
layout: post
title:  强化学习（六）——价值函数的近似表示
category: RL 
---

* toc
{:toc}

# Model-Free Control

## On-Policy Monte-Carlo Control（续）

一般使用《强化学习（二）》中提到的$$\epsilon$$-greedy算法，解决这个问题，这里不再赘述。

![](/images/img2/Model-Free_Control.png)

>这个图建议和《强化学习（三）》中Policy Iteration一节的类似图，对比着看。可以看出无论是Q函数迭代还是V函数迭代，都会收敛到最佳策略$$\pi$$。

上图中每一个向上或向下的箭头都对应着多个Episode。也就是说我们一般在经历了多个Episode之后才进行一次Ｑ函数更新或策略改善。实际上我们也可以在每经历一个Episode之后就更新Ｑ函数或改善策略。但不管使用那种方式，在Ɛ-贪婪探索算法下，我们始终只能得到基于某一策略下的近似Ｑ函数，且该算法没有一个终止条件，因为它一直在进行探索。

因此我们必须关注以下两个方面：一方面我们不想丢掉任何更好信息和状态，另一方面随着我们策略的改善我们最终希望能终止于某一个最优策略，因为事实上最优策略不应该包括一些随机行为选择。为此引入了另一个理论概念：**GLIE**。

**GLIE(Greedy in the Limit with Infinite Exploration)**，直白的说是在有限的时间内进行无限可能的探索。具体表现为：所有已经经历的状态行为对（state-action pair）会被无限次探索；另外随着探索的无限延伸，贪婪算法中$$\epsilon$$值趋向于０。例如，如果我们取$$\epsilon=1/k$$（k为探索的Episode数目），那么该$$\epsilon$$-greedy MC Control就具备GLIE特性。

基于GLIE的MC Control流程如下：

1.对于给定策略$$\pi$$，采样第k个Episode：$$\{S_1,A_1,R_2,\dots,S_T\}\sim \pi$$

2.对于该Episode里出现的每一个状态/行为对更新：

$$
\begin{array}\\
N(S_t,A_t)\leftarrow N(S_t,A_t)+1\\
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))
\end{array}
$$

3.基于新的Q函数改善：

$$\epsilon\leftarrow 1/k,\pi\leftarrow \epsilon-greedy(Q)$$

## On-Policy Temporal-Difference Learning

TD在On-Policy的应用主要是Sarsa算法。Sarsa是State–action–reward–state–action的缩写。

Sarsa算法的流程如下所示：

>随机初始化$$Q(s,a)$$，其中$$Q(\text{terminal-state},\cdot)=0$$。   
>每个Episode执行：   
>>初始化S   
>>根据Q选择当前S下的A   
>>对Episode中的每一步执行：   
>>>执行A，获得观察值R,S'   
>>>根据Q选择当前S'下的A'   
>>>$$Q(S,A)\leftarrow Q(S,A)+\alpha [R+\gamma Q(S',A')-Q(S,A)]$$   
>>>$$S\leftarrow S',A\leftarrow A'$$   
>>
>>直到S是terminal状态。

Sarsa算法的最优化收敛性（即$$Q(s,a)\to q_*(s,a)$$）的条件是：

1.策略产生的序列满足GLIE。

2.$$\alpha_t$$满足Robbins–Monro特性，即：

$$\sum_{t=1}^\infty \alpha_t=\infty, \sum_{t=1}^\infty \alpha_t^2<\infty$$

>Herbert Ellis Robbins，1915～2001，美国数学家，Harvard博士，University of North Carolina教授，Columbia University教授。美国科学院院士，美国艺术科学院院士。

>Sutton Monro，1920～1995，美国数学家，MIT博士，Lehigh University教授。作为海军参加过二战和韩战。

和TD类似，我们也可以定义n-step的Sarsa(n)算法

$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(q_t^{n}-Q(S_t,A_t))$$

Sarsa($$\lambda$$)：

$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(q_t^{\lambda}-Q(S_t,A_t))$$

Sarsa($$\lambda$$)+ET：

$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\delta_tE_t(s,a)$$

参考：

https://blog.csdn.net/zchang81/article/details/77746313

Sarsa与Q-learning对比

## 采样方法

在继续后面的内容之前，我们首先介绍几种采样方法。

### Inverse Sampling

如果某种分布函数不容易采样的话，可以使用它的反函数进行采样：

$$P(F^{-1}(a) \le x) = P(a \le F(x)) = H(F(x))$$

![](/images/img2/Inverse_Sampling.png)

上图是均匀分布到指数分布的采样图，其中y的采样间隔是：

$$F^{-1}_{exp}(a) = -\frac{1}{\lambda}*log(1-a)$$

### Rejective Sampling

我们想求一个空间里均匀分布的集合面积，可以尝试在更大范围内按照均匀分布随机采样，如果采样点在集合中，则接受，否则拒绝。

这种方法最经典的例子就是采样计算圆周率：我们在一个1x1的范围内随机采样一个点，如果它到原点的距离小于1,则说明它在1/4圆内，则接受它，最后通过接受的占比来计算1/4圆形的面积，从而根据公式反算出预估的$$\pi$$值，随着采样点的增多，最后的结果会越精准。

Rejective Sampling的形式化表述为：

$$acc(x_i) = \frac{\tilde{p}(x_i)}{cq(x_i)}$$

其中，$$\tilde{p}(x_i)$$是实际采样的样本，它应该正比于目标分布，$$cq(x_i)$$是比例系数，c为常数。在上例中，$$q(x_i)$$是均匀分布，所以就是常数1，如果是其它分布的话，则是一个函数。

### Importance Sampling

然而实际工作中，分布的正函数已经很复杂，更不用说反函数，$$cq(x_i)$$也不是那么容易得到的，这时就需要Importance Sampling了。

我们采样的目标是：

$$E_{x\sim p}[f(x)] = \int_x f(x)p(x) \mathrm{d}x$$

如果符合p(x)分布的样本不太好生成，我们可以引入另一个分布q(x)，可以很方便地生成样本。使得：

$$\begin{align}
\int_x f(x)p(x) \mathrm{d}x &= \int_x f(x)\frac{p(x)}{q(x)}q(x) \mathrm{d}x\\
&= \int_x g(x)q(x) \mathrm{d}x = E_{x\sim q}[g(x)]\\
where\ \ g(x) &= f(x)\frac{p(x)}{q(x)} = f(x)w(x)
\end{align}$$

我们将问题转化为了求g(x)在q(x)分布下的期望。我们称其中的$$w(x)=\frac{p(x)}{q(x)}$$为**Importance Weight**。

Importance Sampling还可以改善已有的采样方法：如果我们找到一个q分布，使得它能在f(x)∗p(x)较大的地方采集到样本，则能更好地逼近E[f(x)]。

参考：

http://blog.csdn.net/Dark_Scope/article/details/70992266

采样方法

## Off-Policy Learning

### Importance Sampling for Off-Policy Monte-Carlo

前面已经说过，Off-Policy Learning就是根据当前策略$$\mu(a\mid s)$$，评估目标策略$$\pi(a\mid s)$$。因此，对Off-Policy Learning进行Importance Sampling，实际上就是从$$\mu$$中，对$$\pi$$采样的过程。由于MC需要对整个Episode进行采样，因此相应的采样函数为：

$$G_t^{\pi/\mu}=\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}\frac{\pi(A_{t+1}\mid S_{t+1})}{\mu(A_{t+1}\mid S_{t+1})}\dots\frac{\pi(A_T\mid S_T)}{\mu(A_T\mid S_T)}G_t$$

更新公式为：

$$V(S_t)\leftarrow V(S_t)+\alpha(G_t^{\pi/\mu}-V(S_t))$$

由于这种方法的采样函数实在太过复杂，因此只有理论价值，而无实际意义。

### Importance Sampling for Off-Policy TD

类似的，TD算法的更新公式为：

$$V(S_t)\leftarrow V(S_t)+\alpha\left(\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}(R_{t+1}+\gamma V(S_{t+1}))-V(S_t)\right)$$

应用这种思想最好的方法是基于TD(0)的Q-learning。Q-learning的相关内容参见《强化学习（二）》。

### DP和TD的关系

|  | Full Backup (DP) | Sample Backup (TD) |
|:--:|:--:|:--:|
| Bellman Expectation<br/>Equation for $$v_\pi(s)$$ | Iterative Policy Evaluation<br/>$$V(s)\leftarrow E[R+\gamma V(S')\mid s]$$ | TD Learning<br/>$$V(S)\xleftarrow{\alpha} R+\gamma V(S')$$ |
| Bellman Expectation<br/>Equation for $$q_\pi(s,a)$$ | Q-Policy Iteration<br/>$$Q(s,a)\leftarrow E[R+\gamma Q(S',A')\mid s,a]$$ | Sarsa<br/>$$Q(S,A)\xleftarrow{\alpha} R+\gamma Q(S',A')$$ |
| Bellman Optimality<br/>Equation for $$q_*(s,a)$$ | Q-Value Iteration<br/>$$Q(s,a)\leftarrow E[R+\gamma \max_{a'\in A}Q(S',a')\mid s]$$ | Q-Learning<br/>$$Q(S,A)\xleftarrow{\alpha} R+\gamma \max_{a'\in A}Q(S',a')$$ |

上表中$$x\xleftarrow{\alpha}y\equiv x\leftarrow x+\alpha(y-x)$$。

<a name="Value"/>

# 价值函数的近似表示（Value Function Approximation）

之前的内容都是讲解一些强化学习的基础理论，这些知识只能解决一些中小规模的问题。很多价值函数需要用一张大表来存储。当获取某一状态或行为的价值的时候，通常需要一个查表操作（Table Lookup），这对于那些状态空间或行为空间很大的问题几乎无法求解。

在实际应用中，对于状态和行为空间都比较大的情况，精确获得各种v(s)和q(s,a)几乎是不可能的。这时候需要找到近似的函数。具体来说，可以使用线性组合、神经网络以及其他方法来近似价值函数：

$$
\begin{array}\\
\hat v(s,w)\approx v_\pi(s)\\
\hat q(s,a,w)\approx q_\pi(s,a)
\end{array}
$$

其中w是该近似函数的参数。

## 线性函数

这里仍然从最简单的线性函数说起：

$$\hat v(S,w)=x(S)^Tw=\sum_{j=1}^nx_j(S)w_j$$

目标函数为：

$$J(w)=E_\pi[(v_\pi(S)-x(S)^Tw)^2]$$

更新公式为：

$$\Delta w=\alpha (v_\pi(S)-\hat v(S,w))x(S)$$

上述公式都是基本的ML方法，这里不再赘述。

最简单的一类线性函数，被叫做Table Lookup Features：

$$x^{table}(S)=\begin{pmatrix} 1(S=s_1) \\ \vdots \\ 1(S=s_n) \end{pmatrix}$$

则：

$$\hat v(S,w)=\begin{pmatrix} 1(S=s_1) \\ \vdots \\ 1(S=s_n) \end{pmatrix}\begin{pmatrix} w_1 \\ \vdots \\ w_n \end{pmatrix}$$

## Incremental Prediction Algorithms

事实上，之前所列的公式都不能直接用于强化学习，因为公式里都有一个实际价值函数$$v_\pi(S)$$，或者是一个具体的数值，而强化学习没有监督数据，因此不能直接使用上述公式。

因此，我们需要找到一个替代$$v_\pi(S)$$的目标函数。

|:--:|:--:|:--:|:--:|
| MC | $$\Delta w=\alpha ({\color{red}{G_t}}-\hat v(S_t,w))\nabla_w \hat v(S_t,w)$$ | 有噪声、无偏采样 | 收敛至一个局部最优解 |
| TD(0) | $$\Delta w=\alpha ({\color{red}{R_{t+1}+\gamma\hat v(S_{t+1},w)}}-\hat v(S_t,w))\nabla_w \hat v(S_t,w)$$ | 有噪声、有偏采样 | 收敛至全局最优解 |
| TD($$\lambda$$) | $$\Delta w=\alpha ({\color{red}{G_t^\lambda}}-\hat v(S_t,w))\nabla_w \hat v(S_t,w)$$ | 有噪声、有偏采样 |  |

上面公式中，红色的部分就是目标函数。

对于$$\hat q(S,A,w)$$，我们也有类似的结论：

|:--:|:--:|:--:|:--:|
| MC | $$\Delta w=\alpha ({\color{red}{G_t}}-\hat q(S_t,A_t,w))\nabla_w \hat q(S_t,A_t,w)$$ | 有噪声、无偏采样 | 收敛至一个局部最优解 |
| TD(0) | $$\Delta w=\alpha ({\color{red}{R_{t+1}+\gamma\hat q(S_{t+1},A_{t+1},w)}}-\hat q(S_t,A_t,w))\nabla_w \hat q(S_t,A_t,w)$$ | 有噪声、有偏采样 | 收敛至全局最优解 |
| TD($$\lambda$$) | $$\Delta w=\alpha ({\color{red}{q_t^\lambda}}-\hat q(S_t,A_t,w))\nabla_w \hat q(S_t,A_t,w)$$ | 有噪声、有偏采样 |  |
