---
layout: post
title:  Hadoop, Hbase（1）
category: AI 
---

# Hadoop

最近（2016.4），参加公司组织的内部培训，对Hadoop有了一些认识，特记录如下。

## IOE

![](/images/img2/IOE.jpg)

在Google开发GFS之前，业界主流的数据库方案是IOE，即：**IBM大型机+Oracle数据库+EMC存储设备**。

这套方案的主要缺陷在于：

1.服务上限有限。单个机器再强也有上限，对于PB级数据有时会力不从心。

2.IOE方案是商用的专有方案，价格高昂。

## 概述

Hadoop项目由Doug Cutting创建。Doug Cutting也是Lucene项目的创建者。

PS：Lucene是我2007年学习搜索引擎技术时，所接触到的开源项目。回想起来，简直恍如隔世啊。

官网：

http://hadoop.apache.org/

官方文档：

http://hadoop.apache.org/docs/current/

广义的Hadoop包含一个庞大的生态圈：

http://cqwjfck.blog.chinaunix.net/uid-22312037-id-3969789.html

Hadoop初探之Hadoop生态圈

http://www.360doc.com/content/14/0113/17/15109633_345010019.shtml

Hadoop的“生态圈”

狭义的Hadoop包含如下组件：

>Hadoop Common   
>Hadoop Distributed File System(HDFS)   
>Hadoop YARN   
>Hadoop MapReduce

![](/images/article/big_data.png)

## 编译

Hadoop目前不在Ubuntu的官方软件仓库中，无法使用apt-get安装。使用源代码编译Hadoop的相关步骤，可在源码包的BUILDING.txt中找到。这里仅作为补遗之用。总的来说，如无必要还是直接下载bin包比较好，编译还是很麻烦的。

### 安装FindBugs

Hadoop的大部分软件依赖，都可以使用apt-get安装。BUILDING.txt里已经写的很详细了。

FindBugs是一个Java代码的静态分析检查工具。它的官网：

http://findbugs.sourceforge.net/index.html

它的安装方法有3种：

1.源代码安装。下载源代码之后，运行`ant build`，然后设定相关路径，以供Hadoop使用。

2.apt-get安装。FindBugs目前不在Ubuntu 14.04的软件仓库中，而在Ubuntu 15.10的软件仓库中，需要设置源方可安装。这种方法也需要设定相关路径，以供Hadoop使用。

2.maven安装。

`mvn compile findbugs:findbugs`

这种方法最简单。安装完成之后的FindBugs位于maven repository中，而后者通常在~/.m2/repository/下。

这种方法的好处是：由于Hadoop使用maven编译，maven安装之后，可以免去设置路径的步骤。但坏处是：其他不用maven的程序，无法使用该软件。

这一步只要不出下载不成功之类的错误，就算成功。错误留给下一步来解决。

PS：maven下载的文件，大约有180MB，且多为小文件，初次运行相当费时。

### 编译Hadoop

`mvn package -Pdist -DskipTests -Dtar`

这里一定要`-DskipTests`，原因是test不仅速度非常慢，会导致系统响应缓慢，而且即使是官方代码，也不一定能通过所有的test case。

编译的结果在hadoop-dist/target下

## 安装

Hadoop有Single Node和Cluster两种安装模式。一般的部署，当然采用后者。得益于Java的可移植性，Hadoop甚至可以部署到由Raspberry Pi组成的集群中。

前者主要用于开发和学习之用。这里只讨论前者。

Single Node又可分为两种模式：Standalone和Pseudo-Distributed。前者一般仅用于检验程序逻辑的正确性，因为这种模式下，并没有Hadoop常见的各种节点和HFS的概念，所有的程序都运行在一个Java进程中。而后者在配置和运行方面，与Cluster已经相差无几。

http://www.cnblogs.com/serendipity/archive/2011/08/23/2151031.html

## CDH

Cloudera’s Distribution Including Apache Hadoop，简称CDH，是目前用的比较多的Hadoop版本，相比于Apache官方的Hadoop来说，有以下优点：

1.CDH基于稳定版Apache Hadoop，并应用了最新Bug修复或者Feature的Patch。Cloudera常年坚持季度发行Update版本，年度发行Release版本，更新速度比Apache官方快，而且在实际使用过程中，CDH表现无比稳定，并没有引入新的问题。

2.CDH支持Yum/Apt包，Tar包，RPM包，Cloudera Manager四种方式安装，可自动处理软件包之间的依赖和版本匹配的问题。

官网：

www.cloudera.com/downloads/cdh.html

## 何时使用hadoop fs、hadoop dfs与hdfs dfs命令

hadoop fs：使用面最广，可以操作任何文件系统。

hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。

这些命令的选项大部分与linux shell相同，差异点主要在于：

1.HDFS->Local FS。

`hadoop fs -get`

1.Local FS->HDFS。

`hadoop fs -put`

## 服务端口

Hadoop自带了一些web服务端口，如下表所示：

| 默认端口 | 设置位置 | 描述信息 |
|:--|:--|:--|
| 8020 |  | namenode RPC交互端口 |
| 8021 |  | JT RPC交互端口 |
| 8080 |  | Storm UI |
| 50030 | mapred.job.tracker.http.address | JOBTRACKER的HTTP服务器和端口 |
| 50070 | dfs.http.address | NAMENODE的HTTP服务器和端口 |
| 50010 | dfs.datanode.address | DATANODE控制端口，主要用于DATANODE初始化时，向NAMENODE提出注册和应答请求 |
| 50020 | dfs.datanode.ipc.address | DATANODE的RPC服务器地址和端口 |
| 50060 | mapred.task.tracker.http.address | TASKTRACKER的HTTP服务器和端口 |
| 50075 | dfs.datanode.http.address | DATANODE的HTTP服务器和端口 |
| 50090 | dfs.secondary.http.address | 辅助DATANODE的HTTP服务器和端口 |

## jps

jps是Java提供的虚拟机进程查看工具。

使用方法：

`jps -ml`

查到的进程，可用如下方法kill：

`kill -9 <进程号>`

和hadoop有关的进程包括：

| 名称 | 说明 |
|:--|:--|
| QuorumPeerMain | ZooKeeper Daemon |
| DataNode | HDFS Data Node |
| HRegionServer | Hbase Region Server |
| HMaster | Hbase Master |
| NodeManager | YARN Node Manager |
| ResourceManager | YARN Resource Manager |
| nimbus | Storm nimbus |
| supervisor | Storm supervisor |

## MapReduce编程

教程：

http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html

## 参考

https://mp.weixin.qq.com/s/Koz2xOnmVgKFeCr2Xqe9VQ

YARN，母系社会的运行架构

https://mp.weixin.qq.com/s/goXmXMc8JyAgU_GKyWXDzw

白话Hadoop架构原理

# Hbase

官网：

http://hbase.apache.org/

快速开始：

http://hbase.apache.org/book.html#quickstart

启动脚本：

1.HDFS

`sbin/start-dfs.sh`

2.Hbase

`bin/start-hbase.sh`

参考：

http://www.cnblogs.com/cenyuhai/

这个blog专注于hadoop、hbase、spark。

http://www.cnblogs.com/nexiyi/p/hbase_shell.html

HBase的常用Shell命令

http://www.cnblogs.com/Dreama/articles/2219190.html

Hadoop+HBase伪分布式安装配置

https://sourceforge.net/projects/haredbhbaseclie/

HareDB HBase Client是一个Hbase的Web GUI客户端，比较好用。

## 基本概念

HBase以表的形式存储数据。表有行和列组成。列划分为若干个列族/列簇(column family)。

<table>
<tr>
<td rowspan="2"><strong>Row Key</strong></td>
<td colspan="2"><strong>column-family1</strong></td>
<td colspan="3"><strong>column-family2</strong></td>
<td><strong>column-family3</strong></td></tr>
<tr>
<td><strong>column1</strong></td>
<td><strong>column2</strong></td>
<td><strong>column1</strong></td>
<td><strong>column2</strong></td>
<td><strong>column3</strong></td>
<td><strong>column1</strong></td></tr>
<tr>
<td><strong>key1</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><strong>key2</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><strong>key3</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr></table>

如上图所示，key1,key2,key3是三条记录的唯一的row key值，column-family1,column-family2,column-family3是三个列族，每个列族下又包括几列。比如，column-family1这个列族下包括两列，名字是column1和column2。

HBase中，将一个Column Family中的列存在一起，而不同Column Family的数据则分开。

不要在一张表里定义太多的column family。目前Hbase并不能很好的处理超过2~3个column family的表。因为某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O。

吐槽一下。既然2~3个column family的表都支持的不好，那么似乎表明column family在现阶段只是一个设计的概念，离实用尚有距离。这样做的用意，大概是期待有一天Hbase把这个特性做好之后，上层应用可以无须修改吧。

## HBase架构

![](/images/article/hbase.jpg)

HBase在物理上是HDFS上的文件，因此它也是主从结构的。

## 常用的HBase Shell命令

这些命令可在hbase的控制台输入。进入控制台：

`bin/hbase shell`

| 名称 | 命令表达式 |
|:--|:--|
| 查看当前用户 | whoami |
| 创建表 | create '表名称', '列名称1','列名称2','列名称N' |
| 添加记录 | put '表名称', '行名称', '列名称:', '值' |
| 查看记录 | get '表名称', '行名称' |
| 查看表中的记录总数 | count  '表名称' |
| 删除记录 | delete  '表名' ,'行名称' , '列名称' |
| 删除一张表 | 先要禁用该表，才能对该表进行删除，第一步 disable '表名称' 第二步  drop '表名称' |
| 查看所有记录 | scan "表名称" |
| 查看某个表某个列中所有数据 | scan "表名称" , ['列名称:'] |
| 更新记录 | 就是重写一遍进行覆盖 |

## 二级索引

原生态的HBase由于是按列存储的key-value对，原则上只能通过key查询value，因此无法创建主键之外的索引。然而，二级索引在当前的数据应用中，已经相当普遍了。如何解决这一问题呢？

**方法一**：将需要创建键的若干列的内容放到key中。比如，某数据表需要对日期和地点建索引，则key的值可以写成“2017-北京”这样的形式。

这个方法的缺点在于，放入key中的列相当于是行存储结构，会降低查询效率。设想一下，如果所有列都放到key中，那么实际上这个数据表就退化成了普通的按行存储的关系数据库了。

一般来说，采用方法一的数据表其key中包含的列数不应超过10个。

**方法二**：外挂二级索引库。目前主要实现的方案有ITHBase，IHBase，CCIndex，华为二级索引和360二级索引等。这些方案的实现细节虽有差异，但原理上都差不多，即：在数据本身之外，创建专门的数据结构用于存放二级索引。

细节可参考：

http://blog.csdn.net/dhtx_wzgl/article/details/49423979

HBase学习之二级索引
