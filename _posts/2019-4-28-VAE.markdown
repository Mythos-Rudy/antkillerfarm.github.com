---
layout: post
title:  VAE（一）——前置知识, Vanilla VAE（1）
category: GAN & VAE 
---

# 前置知识

## 最速降线

约翰·伯努利在1696年提出最速降线的问题（problem of brachistochrone），向全欧洲数学家征求解答。这个问题最早由伽利略在1630年提出：“一个质点在只受重力的作用下，从一个给定点A到不在它垂直下方的另一点B，问沿着什么曲线下滑所需时间最短？”

然而伽利略自己给出的答案是错误的：他认为这条曲线是过AB的圆弧。这条曲线也不是连接AB两点的直线，尽管AB间线段最短，但小球滚下来的时间不是最短。

伯努利把此问题发布在Acta Eruditorum上，他还这么说：

“我，约翰·伯努利，想找到世界上最出色的数学家。对聪明人而言，没有什么能比一道诚实而富有挑战性的难题更有吸引力，其可能的解决方案将会成为一个永恒的纪念碑。按照帕斯卡，费马等人设定的例子，请允许我代表整个数学界将这个尤其能在今天考验大家的数学技巧和思维耐力的问题展示在最优秀的数学家面前。如果有人能把答案递交与我，我会将其公开，并授予其应得的奖赏。”

伯努利原定的截止期限是1696年年底，可是他只受到了一份来自他的老师莱布尼兹的解答。莱布尼兹要求伯努利将截止期限延长到来年复活节（大致在3月下旬到4月下旬之间），以便让欧洲数学家们有更多时间来充分解决此道难题。约翰·伯努利亲自把最速降线问题抄了一份，装进信封寄给在英国的牛顿。

1697年1月29日，牛顿正在造币局里忙着改铸新币的工作。下午4点回到家里，他看到了邮箱里伯努利寄来的问题。尽管牛顿非常疲惫，他立即彻夜未眠的投入研究，在凌晨4点时得到问题的解答。他将他的解答寄给好友兼皇家协会主席查尔斯，随后皇家协会以匿名的形式发表在Philosophical Transactions上。

要知道，此时的牛顿已经56岁，工作重点是皇家铸币厂监管。即使如此，在忙了一天的本职工作后，牛顿还是用几个小时就解决了许多欧洲数学家都无法解出的难题。约翰·伯努利本人也花了两个星期的时间才完成解答。

1697年复活节的截止期限，伯努利共收到了5份答案，他自己和其老师莱布尼兹，第三份是他的哥哥雅可布·伯努利，洛必达是第四个，最后是一份匿名答案。伯努利在阅读最后一份解答时立即认出它的作者，他惊叹道：

**“从利爪上认出了这头狮子（recognizes a lion from his claw mark）”**

在给查尔斯的信里，牛顿还写道：**我不喜欢在数学上被外国人糊弄（I do not love to be dunned and teased by foreigners about mathematical things）。**

## 泛函 & 变分

泛函数 (Functionals)、Calculus of Variations

最速降线（The most fast line problem）

教程：

《变分法基础》，老大中著。

参考：

https://zhuanlan.zhihu.com/yueaptx

一个变分法方面的专栏

https://blog.csdn.net/shenziheng1/article/details/54808173

泛函与变分初步（Euler-lagrange条件）

https://www.cnblogs.com/MagicXYoung/p/4906606.html

泛函与变分基础

https://blog.csdn.net/theonegis/article/details/86217916

变分法入门介绍

https://zhuanlan.zhihu.com/p/41573146

变分法理解1——泛函简介

https://zhuanlan.zhihu.com/p/41810184

变分法理解2——基本方法

## Variational Inference

论文：

《Variational Inference: A Review for Statisticians》

《A Tutorial on Variational Bayesian Inference》

《Stochastic Variational Inference》

《An Introduction to Variational Methods for Graphical Models》

参考：

https://zhuanlan.zhihu.com/p/49401976

变分推断

https://www.zhihu.com/question/41765860

如何简单易懂地理解变分推断(variational inference)？

http://nooverfit.com/wp/当变分推断（variational-inference）遇上神经网络，贝叶斯深度/

当变分推断（variational inference）遇上神经网络，贝叶斯深度学习以及Pytorch开源代码

https://blog.csdn.net/aws3217150/article/details/57072827

变分贝叶斯推断(Variational Bayes Inference)简介

https://www.cnblogs.com/yifdu25/p/8181185.html

变分推断（Variational Inference）

# Vanilla VAE

变分自编码器（Variational Auto-Encoder，VAE）是Autoencoder的一种扩展。

论文：

《Auto-Encoding Variational Bayes》

>Diederik P. Kingma，荷兰人，Univ. of Amsterdam博士（2017）。现为OpenAI科学家。VAE和Adam optimizer的发明者。   
>个人主页：   
>http://dpkingma.com

除了原始论文之外，以下综述也很有名：

《Tutorial on Variational Autoencoders》

代码：

https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py

Keras官方提供的代码示例

以下部分主要摘自：

https://kexue.fm/archives/5253

变分自编码器（一）：原来是这么一回事

## 分布变换

通常我们会拿VAE跟GAN比较，的确，它们两个的目标基本是一致的——希望构建一个从隐变量Z生成目标数据X的模型，但是实现上有所不同。更准确地讲，它们是假设了Z服从某些常见的分布（比如正态分布或均匀分布），然后希望训练一个模型$$X=g(Z)$$，这个模型能够将原来的概率分布映射到训练集的概率分布，也就是说，它们的目的都是进行分布之间的映射。

现在假设Z服从标准的正态分布，那么我就可以从中采样得到若干个$$Z_1, Z_2, \dots, Z_n$$，然后对它做变换得到$$\hat{X}_1 = g(Z_1),\hat{X}_2 = g(Z_2),\dots,\hat{X}_n = g(Z_n)$$，我们怎么判断这个通过f构造出来的数据集，它的分布跟我们目标数据集的分布是不是一样的呢？

![](/images/img2/VAE.png)

**生成模型的难题就是判断生成分布与真实分布的相似度，因为我们只知道两者的采样结果，不知道它们的分布表达式。**

有读者说不是有KL散度吗？当然不行，因为KL散度是根据两个概率分布的表达式来算它们的相似度的，然而目前我们并不知道它们的概率分布的表达式，我们只有一批从构造的分布采样而来的数据$$\{\hat{X}_1,\hat{X}_2,\dots,\hat{X}_n\}$$，还有一批从真实的分布采样而来的数据$$\{X_1,X_2,\dots,X_n\}$$（也就是我们希望生成的训练集）。我们只有样本本身，没有分布表达式，当然也就没有方法算KL散度。

虽然遇到困难，但还是要想办法解决的。GAN的思路很直接粗犷：既然没有合适的度量，那我干脆把这个度量也用神经网络训练出来吧。而VAE则使用了一个精致迂回的技巧。

## VAE的传统理解

首先我们有一批数据样本$$\{X_1,\dots,X_n\}$$，其整体用X来描述，我们本想根据$$\{X_1,\dots,X_n\}$$得到X的分布p(X)，如果能得到的话，那我直接根据p(X)来采样，就可以得到所有可能的X了（包括$$\{X_1,\dots,X_n\}$$以外的），这是一个终极理想的生成模型了。当然，这个理想很难实现，于是我们将分布改一改：

$$p(X)=\sum_Z p(X|Z)p(Z)$$

此时$$p(X\mid Z)$$就描述了一个由Z来生成X的模型，而我们假设Z服从标准正态分布，也就是$$p(Z)=\mathcal{N}(0,I)$$。如果这个理想能实现，那么我们就可以先从标准正态分布中采样一个Z，然后根据Z来算一个X，也是一个很棒的生成模型。接下来就是结合自编码器来实现重构，保证有效信息没有丢失，再加上一系列的推导，最后把模型实现。框架的示意图如下：

![](/images/img2/VAE_2.png)

看出了什么问题了吗？如果像这个图的话，我们其实完全不清楚：究竟经过重新采样出来的$$Z_k$$，是不是还对应着原来的$$X_k$$，所以我们如果直接最小化$$\mathcal{D}(\hat{X}_k,X_k)^2$$（这里D代表某种距离函数）是很不科学的，而事实上你看代码也会发现根本不是这样实现的。

## VAE初现

其实，在整个VAE模型中，我们并没有去使用p(Z)（先验分布）是正态分布的假设，我们用的是假设$$p(Z\mid X)$$（后验分布）是正态分布！！

具体来说，给定一个真实样本$$X_k$$，我们假设存在一个专属于$$X_k$$的分布$$p(Z\mid X_k)$$（学名叫后验分布），并进一步假设这个分布是（独立的、多元的）正态分布。为什么要强调“专属”呢？因为我们后面要训练一个生成器$$X=g(Z)$$，希望能够把从分布$$p(Z\mid X_k)$$采样出来的一个$$Z_k$$还原为$$X_k$$。如果假设p(Z)是正态分布，然后从p(Z)中采样一个Z，那么我们怎么知道这个Z对应于哪个真实的X呢？现在$$p(Z\mid X_k)$$专属于$$X_k$$，我们有理由说从这个分布采样出来的Z应该要还原到$$X_k$$中去。

这样有多少个X就有多少个正态分布了。我们知道正态分布有两组参数：均值$$\mu$$和方差$$\sigma^2$$（多元的话，它们都是向量），那我怎么找出专属于$$X_k$$的正态分布$$p(Z\mid X_k)$$的均值和方差呢？好像并没有什么直接的思路。那好吧，就用神经网络拟合出来吧！

![](/images/img2/VAE_3.png)

于是我们构建两个神经网络$$\mu_k = f_1(X_k),\log \sigma^2 = f_2(X_k)$$来算它们了。我们选择拟合$$\log \sigma^2$$而不是直接拟合$$\sigma^2$$，是因为$$\sigma^2$$总是非负的，需要加激活函数处理，而拟合$$\log \sigma^2$$不需要加激活函数，因为它可正可负。到这里，知道专属于$$X_k$$的均值和方差，也就知道它的正态分布长什么样了，然后从这个专属分布中采样一个$$Z_k$$出来，经过一个生成器得到$$\hat{X}_k=g(Z_k)$$，现在我们可以放心地最小化$$\mathcal{D}(\hat{X}_k,X_k)^2$$，因为$$Z_k$$是从专属$$X_k$$的分布中采样出来的，这个生成器应该要把开始的$$X_k$$还原回来。
